{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMC 2024 submission (notebook version)\n",
    "\n",
    "## Flat Agent and Flat Attention Agent (with policy shaping) - SPARSE reward environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 15:11:52.493009: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-24 15:11:52.521059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 15:11:53.000499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "### this is where we convert the problem into compatible mode for gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register, registry, EnvSpec\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "from enum import Enum\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM assistant setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "\n",
    "def get_file_type(document_path):\n",
    "    # Split the path and get the extension\n",
    "    _, file_extension = os.path.splitext(document_path)\n",
    "    # Return the file extension without the period\n",
    "    return file_extension[1:] if file_extension else None\n",
    "\n",
    "### class that processes verbal inputs handle disaster-related verbal inputs, analyze them using RAG architecture, and generate a \n",
    "# response in a specified format. It leverages models like ChatOllama and techniques like vector storage and retrieval for its operations.\n",
    "class DisasterResponseAssistant:\n",
    "    def __init__(self, data_path, data_type, model_name=\"mistral\", embedding_model='nomic-embed-text', collection_name=\"rag-chroma\"):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = embedding_model\n",
    "        self.collection_name = collection_name\n",
    "        self.data_path = data_path\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        self.llm = None\n",
    "        self.loader = None\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "        self._load_model()            # Initializes an instance of the ChatOllama model    \n",
    "        self._load_documents()        # Loads and splits the PDF document into chunks\n",
    "        self._create_vectorstore()    # Creates a vector store using Chroma from the document splits\n",
    "        self._create_retriever()      # Creates a retriever from the vector store\n",
    "        \n",
    "        self.hazard_coordinates = []  # To store hazard coordinates\n",
    "        self.poi_coordinates = []     # To store points of interest coordinates\n",
    "    \n",
    "    def _load_model(self):\n",
    "        self.llm = ChatOllama(model=self.model_name)\n",
    "        \n",
    "\n",
    "    def _load_documents(self): ## for json documents\n",
    "        print(f\"document {self.data_type} will be infused\")\n",
    "        if self.data_type == 'pdf':\n",
    "            self.loader = PyPDFLoader(self.data_path)\n",
    "            self.data = self.loader.load_and_split()\n",
    "        elif self.data_type == 'json':\n",
    "            self.loader = JSONLoader(\n",
    "                file_path=self.data_path,\n",
    "                jq_schema='.',\n",
    "                text_content=False)\n",
    "            self.data = self.loader.load()\n",
    "            #pprint(self.data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported document type. Please choose either 'pdf' or 'json'.\")\n",
    "\n",
    "\n",
    "    def _create_vectorstore(self): ## for json documents\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.data,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=embeddings.OllamaEmbeddings(model=self.embedding_model),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def _create_retriever(self):\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "\n",
    "    ### generate a response based on a verbal input\n",
    "    ### construct a template for the response using RAG architecture\n",
    "    def generate_response(self, verbal_input):\n",
    "        prompt_template = \"\"\"You are an assistant, who carefully listens to verbal inputs: {verbal_input} and specialized in analyzing disaster-related inputs. Your task is \n",
    "to identify physical locations mentioned in the text and classify them as either points of interest (POI) or as hazards/dangers (HAZARD) for rescue operations. Use the\n",
    "information provided in the documents: {context}, such as KEYWORDS, descriptions and context when locations are mentioned, to make your classification.\n",
    "Output the classification in the form of a JSON array dictionary with keys 'location', 'coordinates', and 'category'. Here are some rules you always follow:\n",
    "- Focus strictly on physical locations. Avoid including entities that do not represent physical, geographical places (such as individuals, conditions, or \n",
    "  abstract concepts).\n",
    "- Generate human-readable output in the specified dictionary format.\n",
    "- Generate only the requested output, strictly following the dictionary structure.\n",
    "- Within the dictionary, the value of the `category` key must be either 'POI' or 'HAZARD'. \n",
    "- Never generate offensive or foul language.\n",
    "- Never give explanations over your output.\n",
    "Input: {verbal_input}\n",
    "\"\"\"\n",
    "        system_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "        output_parser = StrOutputParser()\n",
    "        after_rag_chain = (\n",
    "            {\"context\": self.retriever, \"verbal_input\": RunnablePassthrough()}\n",
    "            | system_template\n",
    "            | self.llm  # Assuming model_local is defined elsewhere and accessible\n",
    "            | output_parser\n",
    "        )\n",
    "        response = after_rag_chain.invoke(verbal_input)\n",
    "        return response\n",
    "    \n",
    "    def refine_response(self, output):\n",
    "        cleaned_output_str = output.strip().replace('\\n', '').replace('(', '[').replace(')', ']')\n",
    "        output_dict = json.loads(cleaned_output_str)\n",
    "\n",
    "        for item in output_dict:\n",
    "            coord = tuple(item['coordinates'])\n",
    "            if item['category'] == 'HAZARD':\n",
    "                self.hazard_coordinates.append(coord)\n",
    "            else:\n",
    "                self.poi_coordinates.append(coord)\n",
    "                    \n",
    "        print(\"Hazardous Coordinates:\", self.hazard_coordinates)\n",
    "        print(\"Point of Interest Coordinates:\", self.poi_coordinates)\n",
    "        return self.hazard_coordinates, self.poi_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot and Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotAction(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    COLLECT_X = 4\n",
    "    COLLECT_Y = 5\n",
    "    COLLECT_Z = 6\n",
    "    COLLECT_A = 7\n",
    "    COLLECT_B = 8\n",
    "    COLLECT_C = 9\n",
    "    SAVE = 10\n",
    "    USE = 11\n",
    "    REMOVE = 12\n",
    "    CARRY = 13\n",
    "\n",
    "class GridTile(Enum):\n",
    "    _FLOOR = 0\n",
    "    ROBOT = 1\n",
    "    TARGET = 2\n",
    "    X_INFO = 3\n",
    "    Y_INFO = 4\n",
    "    Z_INFO = 5\n",
    "    DITCH = 6\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, test_episodes=1):\n",
    "    total_rewards = []\n",
    "    for episode in range(test_episodes):\n",
    "        obs, _ = env.reset(seed=episode)\n",
    "        state = agent.get_state(obs)\n",
    "        terminated = False\n",
    "        total_return, step, cnt = 0, 0, 0\n",
    "        collisions = []\n",
    "        while not terminated:\n",
    "            action = np.argmax(agent.Q_table[state]) # Worker chooses action greedily\n",
    "            next_obs, reward, terminated, _, _ = env.step(action) # Take action in environment\n",
    "            next_state = agent.get_state(next_obs)\n",
    "\n",
    "            print(f\"Step {step+1}: || State={state} || Action={RobotAction(action).name}|| Reward={reward} || Next State={next_state} || Done={terminated}\")\n",
    "            # Optionally, print logs or store them\n",
    "            total_return += reward\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "            if tuple([state[0], state[1]]) in env.sar_robot.fires:\n",
    "                print(colored(\"Robot is in fire!\", \"red\"))\n",
    "                cnt += 1\n",
    "                collisions.append(tuple([state[0], state[1]]))\n",
    "        total_rewards.append(total_return)\n",
    "        print(f\"Test {episode}: Finished after {step} steps with total reward {total_return} and {cnt} collisions at {collisions}.\")\n",
    "    avg_reward = sum(total_rewards) / test_episodes\n",
    "    print(f\"Average reward over {test_episodes} testing episodes: {avg_reward}\")\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document json will be infused\n"
     ]
    }
   ],
   "source": [
    "class searchANDrescueRobot:\n",
    "    def __init__(self, grid_rows=7, grid_cols=7, info_number_needed=3):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.info_number_needed = info_number_needed\n",
    "        self.reset()\n",
    "        # for LLM integration\n",
    "        self.ask_action_counter = 0\n",
    "        self.visited_information_state = False\n",
    "        self.input_received = False\n",
    "        self.POIs, self.fires, self.hazards, self.pois = [], [], [], []\n",
    "        document_path = \"/home/dimi/HRL-LLM/data/sar_data.json\"\n",
    "        document_type = get_file_type(document_path)\n",
    "        self.assistant = DisasterResponseAssistant(document_path, document_type)\n",
    "        self.sensor_readings = {}\n",
    "         \n",
    "    def reset(self, seed=None):\n",
    "        self.init_positions = [[4, 1]]\n",
    "        self.robot_pos = random.choice(self.init_positions)\n",
    "        self.has_info = 0\n",
    "        self.has_saved = 0\n",
    "        random.seed(seed)\n",
    "        self.target_pos = [0, 3]\n",
    "        self.info_pos1 = [4, 4]\n",
    "        self.info_pos2 = [6, 2]\n",
    "        self.info_pos3 = [5, 5]\n",
    "        self.ditches = [(1, 6), (2, 2), (2, 4), (3, 2), (3, 3), (3, 4), (4, 5), \\\n",
    "                        (5, 0), (5, 1), (5, 2), (6, 0), (0, 2), (0, 4)]\n",
    "        \n",
    "        self.POIs, self.fires = [], []\n",
    "        self.visited_information_state = False\n",
    "\n",
    "\n",
    "    # Update the robot's position\n",
    "    def next_state_vision(self, target, robot_action:RobotAction) -> bool:\n",
    "        robot_pos = target\n",
    "        self.last_action = robot_action\n",
    "        if robot_action == RobotAction.UP:\n",
    "            if robot_pos[0] > 0:\n",
    "                robot_pos[0] -= 1  \n",
    "        elif robot_action == RobotAction.DOWN:\n",
    "            if robot_pos[0] < self.grid_rows-1:\n",
    "                robot_pos[0] += 1\n",
    "        elif robot_action == RobotAction.LEFT:\n",
    "            if robot_pos[1] > 0:\n",
    "                robot_pos[1] -= 1\n",
    "        elif robot_action == RobotAction.RIGHT:\n",
    "            if robot_pos[1] < self.grid_cols-1:\n",
    "                robot_pos[1] += 1\n",
    "        if robot_action in [RobotAction.COLLECT_A, RobotAction.COLLECT_B, RobotAction.COLLECT_C, \\\n",
    "                            RobotAction.COLLECT_X, RobotAction.COLLECT_Y, RobotAction.COLLECT_Z, \\\n",
    "                            RobotAction.SAVE, RobotAction.USE, RobotAction.REMOVE, RobotAction.CARRY]: \n",
    "            robot_pos = robot_pos\n",
    "        return robot_pos\n",
    "\n",
    "\n",
    "    def perform_action(self, robot_action:RobotAction):\n",
    "        self.last_action = robot_action\n",
    "        info_collected_X, info_collected_Y, info_collected_Z = False, False, False\n",
    "        total_info_collected = False\n",
    "        illegal_action = False\n",
    "        if robot_action in [RobotAction.UP, RobotAction.DOWN, RobotAction.LEFT, RobotAction.RIGHT]:\n",
    "            if robot_action == RobotAction.UP:\n",
    "                if self.robot_pos[0] > 0:\n",
    "                    self.robot_pos[0] -= 1  \n",
    "            elif robot_action == RobotAction.DOWN:\n",
    "                if self.robot_pos[0] < self.grid_rows-1:\n",
    "                    self.robot_pos[0] += 1\n",
    "            elif robot_action == RobotAction.LEFT:\n",
    "                if self.robot_pos[1] > 0:\n",
    "                    self.robot_pos[1] -= 1\n",
    "            elif robot_action == RobotAction.RIGHT:\n",
    "                if self.robot_pos[1] < self.grid_cols-1:\n",
    "                    self.robot_pos[1] += 1\n",
    "        \n",
    "        elif robot_action in [RobotAction.COLLECT_A, RobotAction.COLLECT_B, RobotAction.COLLECT_C,\n",
    "                              RobotAction.COLLECT_X, RobotAction.COLLECT_Y, RobotAction.COLLECT_Z,\n",
    "                              RobotAction.SAVE, RobotAction.USE, RobotAction.REMOVE, RobotAction.CARRY]:\n",
    "            if self.robot_pos == self.info_pos1 and self.has_info < 1:\n",
    "                if robot_action == RobotAction.COLLECT_X:\n",
    "                    self.has_info += 1\n",
    "                    info_collected_X = True\n",
    "                else:\n",
    "                    illegal_action = True\n",
    "            elif self.robot_pos == self.info_pos2 and self.has_info == 1:\n",
    "                if robot_action == RobotAction.COLLECT_Y:\n",
    "                    self.has_info += 1\n",
    "                    info_collected_Y = True\n",
    "                else:\n",
    "                    illegal_action = True\n",
    "            elif self.robot_pos == self.info_pos3 and self.has_info == 2:\n",
    "                if robot_action == RobotAction.COLLECT_Z:\n",
    "                    self.perform_collect_action()\n",
    "                    self.has_info += 1\n",
    "                    info_collected_Z = True\n",
    "                    total_info_collected = True\n",
    "                else:\n",
    "                    illegal_action = True\n",
    "            elif self.robot_pos == self.target_pos and self.has_info == self.info_number_needed:\n",
    "                if robot_action == RobotAction.SAVE:\n",
    "                    self.has_saved = 1\n",
    "                else:\n",
    "                    illegal_action = True\n",
    "                \n",
    "        mission_complete = self.robot_pos == self.target_pos and self.has_info == self.info_number_needed and self.has_saved\n",
    "        return mission_complete, info_collected_X, info_collected_Y, total_info_collected, illegal_action\n",
    "\n",
    "\n",
    "    def perform_collect_action(self):\n",
    "        self.ask_action_counter += 1\n",
    "        x, y = self.robot_pos\n",
    "        verbal_inputs = []\n",
    "        if self.has_info == 2:  ## should be 2 if total number of infos are 3 \n",
    "            verbal_input = (\"Hey, there's a victim at the hospital. A fire was reported at the train station. There is a fire at the bank. A safe area is the mall. You must go to the access route in the school. Another access route at the restaurant. And there is a shelter in the shop. There are also reports of significant instances of heat at the bakery. Police told us that no access allowed around the petrol station.\")\n",
    "            # print(f\"real LLM is about to start handling the input {verbal_input}\")\n",
    "            verbal_inputs.append(verbal_input)\n",
    "            \n",
    "            if self.ask_action_counter <= 1:\n",
    "                print(f\"real LLM is about to start handling the input {verbal_input}\")\n",
    "                for input_text in verbal_inputs:\n",
    "                    response = self.assistant.generate_response(input_text)\n",
    "                    if response:\n",
    "                        self.visited_information_state = True\n",
    "                    self.hazards, self.pois = self.assistant.refine_response(response)\n",
    "                    print(f\"real LLM is about to end handling the input {verbal_input}\")\n",
    "                    self.update_environment_REAL(self.hazards, self.pois)\n",
    "            else:\n",
    "                # #print(f\"input will be handled hereby by pseudoLLM\")\n",
    "                # print(self.hazards, self.pois)\n",
    "                self.visited_information_state = True\n",
    "                self.update_environment_REAL(self.hazards, self.pois)\n",
    "            \n",
    "    def update_environment_REAL(self, haz, poi):\n",
    "        for hazardous_location in haz:\n",
    "            self.sensor_readings[(hazardous_location[0], hazardous_location[1], 3, 0)] = -10.0\n",
    "            self.fires.append(hazardous_location)\n",
    "        for safe_location in poi:\n",
    "            self.sensor_readings[(safe_location[0], safe_location[1], 3, 0)] = 10.0\n",
    "            self.POIs.append(safe_location)\n",
    "            \n",
    "    \n",
    "    def is_in_ditch(self):\n",
    "        return tuple(self.robot_pos) in self.ditches\n",
    "\n",
    "    def render(self):\n",
    "        for x in range(self.grid_rows):\n",
    "            for y in range(self.grid_cols):\n",
    "                if [x, y] == self.robot_pos:\n",
    "                    print(GridTile.ROBOT, end=' ')\n",
    "                elif [x, y] == self.target_pos:\n",
    "                    print(GridTile.TARGET, end=' ')\n",
    "                elif [x, y] == self.info_pos1:\n",
    "                    print(GridTile.X_INFO, end=' ')\n",
    "                elif [x, y] == self.info_pos2:\n",
    "                    print(GridTile.Y_INFO, end=' ')\n",
    "                elif [x, y] == self.info_pos3:\n",
    "                    print(GridTile.Z_INFO, end=' ')\n",
    "                elif tuple([x, y]) in self.ditches:\n",
    "                    print(GridTile.DITCH, end=' ')\n",
    "                else:\n",
    "                    print(GridTile._FLOOR, end=' ')\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "\n",
    "class SARrobotEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], 'render_fps': 1}\n",
    "    def __init__(self, grid_rows=7, grid_cols=7, render_mode=None, info_number_needed=3):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        self.sar_robot = searchANDrescueRobot(grid_rows, grid_cols, info_number_needed)\n",
    "        self.action_space = spaces.Discrete(len(RobotAction))\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low = 0,\n",
    "            high = np.array([self.grid_rows-1, self.grid_cols-1, info_number_needed, 1]),\n",
    "            shape = (4,),\n",
    "            dtype = np.int32\n",
    "        )\n",
    "        \n",
    "        self.max_steps = 50\n",
    "        self.current_step = 0\n",
    "        self.turnPenalty = -1\n",
    "        self.stepsPenalty = -5\n",
    "        self.ditchPenalty = -30\n",
    "        self.illegalActionPenalty = -5  # Penalty for illegal actions\n",
    "        self.winReward = 100\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.sar_robot.reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        obs = np.concatenate((self.sar_robot.robot_pos, [self.sar_robot.has_info], [self.sar_robot.has_saved])).astype(np.int32)\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        self.current_step += 1\n",
    "        target_reached, info_collected_X, info_collected_Y, total_info_collected, illegal_action = self.sar_robot.perform_action(RobotAction(action))\n",
    "        terminated = False\n",
    "        \n",
    "        if self.sar_robot.is_in_ditch():\n",
    "            reward = self.ditchPenalty\n",
    "            terminated = True\n",
    "        \n",
    "        if self.is_max_steps_exceeded():\n",
    "            reward = self.stepsPenalty\n",
    "            terminated = True\n",
    "        \n",
    "        ## the reward for getting the information is now removed --> this makes the environment more challenging (sparse reward)\n",
    "        # if info_collected_X or info_collected_Y or total_info_collected:\n",
    "        #     reward = 10  # Reward for collecting info\n",
    "        \n",
    "        if target_reached:\n",
    "            reward = self.winReward\n",
    "            terminated = True\n",
    "\n",
    "        if illegal_action:\n",
    "            reward = self.illegalActionPenalty\n",
    "        \n",
    "        reward += self.turnPenalty\n",
    "        \n",
    "        obs = np.concatenate((self.sar_robot.robot_pos, [self.sar_robot.has_info], [self.sar_robot.has_saved])).astype(np.int32)\n",
    "        info = {}\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"Action: {RobotAction(action)}, Reward: {reward}, Terminated: {terminated}\")\n",
    "            self.render()\n",
    "        \n",
    "        return obs, reward, terminated, False, info\n",
    "\n",
    "    def is_max_steps_exceeded(self):\n",
    "        return self.current_step >= self.max_steps\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            self.sar_robot.render()\n",
    "\n",
    "env = SARrobotEnv(grid_rows=7, grid_cols=7, render_mode='None',  info_number_needed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat learning agent (Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentFlat:\n",
    "    def __init__(self, env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, log_dir=\"curve-SMC-HRL_sparse/flatQ\"):\n",
    "        self.env = env \n",
    "        self.ALPHA = ALPHA\n",
    "        self.GAMMA = GAMMA \n",
    "        self.EPSILON_MAX = EPSILON_MAX\n",
    "        self.EPSILON = EPSILON_MAX\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.EPSILON_MIN = EPSILON_MIN\n",
    "        self.num_states = (self.env.observation_space.high[0] + 1, \n",
    "                           self.env.observation_space.high[1] + 1, \n",
    "                           self.env.observation_space.high[2] + 1,\n",
    "                           self.env.observation_space.high[3] + 1)  # 7*7*4*2\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.Q_table = np.zeros((*self.num_states, self.num_actions))\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.EPSILON:\n",
    "            return self.env.action_space.sample() # Explore: choose a random action\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state]) # Exploit: choose the action with max Q-value\n",
    "    \n",
    "    def get_state(self, observation):\n",
    "        return tuple(observation)\n",
    "    \n",
    "    def decay_epsilon(self, episodes):\n",
    "        if self.EPSILON > 0.1:\n",
    "            self.EPSILON -= self.DECAY_RATE/episodes\n",
    "        else:\n",
    "            self.EPSILON = self.EPSILON_MIN\n",
    "        return self.EPSILON\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.Q_table[next_state])\n",
    "        td_target = reward + self.GAMMA * self.Q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.Q_table[state][action]\n",
    "        self.Q_table[state][action] += self.ALPHA * td_error\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        return_list_Q = []\n",
    "        total_rewards_per_episode = np.zeros(num_episodes)\n",
    "        total_steps_per_episode = np.zeros(num_episodes)\n",
    "        Rewards, steps_cnt, episode_return_Q = 0, 0, 0\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"episode: {episode} | reward: {Rewards} | epsilon: {self.EPSILON}\")\n",
    "            \n",
    "            obs, _ = self.env.reset(seed=episode)\n",
    "            s = self.get_state(obs)\n",
    "            \n",
    "            terminated = False\n",
    "            Rewards, steps_cnt, episode_return_Q = 0, 0, 0\n",
    "            while not terminated:\n",
    "                a = self.epsilon_greedy_policy(s)\n",
    "                obs_, r, terminated, _, _ = self.env.step(a)\n",
    "                s_ = self.get_state(obs_)\n",
    "                Rewards += r\n",
    "                episode_return_Q += r\n",
    "                self.update(s, a, r, s_)\n",
    "                s = s_\n",
    "                steps_cnt += 1\n",
    "            \n",
    "            # Log the rewards and steps to Tensorboard\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar('Episode Return', Rewards, step=episode)\n",
    "                tf.summary.scalar('Steps per Episode', steps_cnt, step=episode)\n",
    "                \n",
    "            self.EPSILON = self.decay_epsilon(num_episodes)\n",
    "            total_rewards_per_episode[episode] = Rewards\n",
    "            total_steps_per_episode[episode] = steps_cnt\n",
    "            return_list_Q.append(episode_return_Q)\n",
    "            \n",
    "        return total_rewards_per_episode, total_steps_per_episode, return_list_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run (flat learning agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 15:13:09.908064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.911801: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.911924: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.913107: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.913197: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.913267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.958255: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.958427: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.958510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:13:09.958566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2669 MB memory:  -> device: 0, name: NVIDIA RTX A2000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fb728c7f7047ef93422daffbfbfedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 | reward: 0 | epsilon: 1.0\n",
      "episode: 100 | reward: -55 | epsilon: 0.7999999999999998\n",
      "episode: 200 | reward: -55 | epsilon: 0.5999999999999996\n",
      "episode: 300 | reward: -55 | epsilon: 0.39999999999999947\n",
      "episode: 400 | reward: -59 | epsilon: 0.1999999999999993\n",
      "episode: 500 | reward: -55 | epsilon: 0.01\n",
      "episode: 600 | reward: -55 | epsilon: 0.01\n",
      "episode: 700 | reward: -55 | epsilon: 0.01\n",
      "episode: 800 | reward: -55 | epsilon: 0.01\n",
      "episode: 900 | reward: -55 | epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "all_total_rewards_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_flat = []  # List to store total rewards from each run\n",
    "for _ in range(1):\n",
    "    EPISODES = 1000\n",
    "    ALPHA = 0.1\n",
    "    GAMMA = 0.98\n",
    "    EPSILON_MAX = 1.0\n",
    "    EPSILON_MIN = 0.01\n",
    "    DECAY_RATE = 2\n",
    "    agent_flat = QLearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN)\n",
    "    rewards_flat, steps_flat, returns_flat = agent_flat.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_flat.append(rewards_flat)\n",
    "    all_total_steps_AGENT_flat.append(steps_flat)\n",
    "    \n",
    "avg_total_rewards_AGENT_flat = np.mean(all_total_rewards_AGENT_flat, axis=0)\n",
    "avg_total_steps_AGENT_flat = np.mean(all_total_steps_AGENT_flat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate (flat learning agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: || State=(4, 1, 0, 0) || Action=RIGHT|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 2: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 3: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 4: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 5: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 6: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 7: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 8: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 9: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 10: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 11: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 12: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 13: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 14: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 15: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 16: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 17: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 18: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 19: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 20: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 21: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 22: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 23: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 24: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 25: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 26: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 27: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 28: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 29: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 30: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 31: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 32: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 33: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 34: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 35: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 36: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 37: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 38: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 39: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 40: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 41: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 42: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 43: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 44: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 45: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 46: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 47: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 48: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 49: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 50: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-6 || Next State=(4, 2, 0, 0) || Done=True\n",
      "Test 0: Finished after 50 steps with total reward -55 and 0 collisions at [].\n",
      "Average reward over 1 testing episodes: -55.0\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_agent(env, agent_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat learning agent (Q-learning) + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSpace:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.num_states = (self.env.observation_space.high[0] + 1, \n",
    "                           self.env.observation_space.high[1] + 1, \n",
    "                           self.env.observation_space.high[2] + 1,\n",
    "                           self.env.observation_space.high[3] + 1)  # 7*7*4*2\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.attention_space_low = np.zeros((*self.num_states, self.num_actions))\n",
    "    \n",
    "    def identify_changed_states(self, readings):\n",
    "        changed_states = [i for i, value in readings.items() if value != 1]\n",
    "        return changed_states\n",
    "    \n",
    "    def get_connected_states(self, target_state):\n",
    "        inverse_actions = {\n",
    "            RobotAction.UP.value: RobotAction.DOWN.value, \n",
    "            RobotAction.DOWN.value: RobotAction.UP.value, \n",
    "            RobotAction.LEFT.value: RobotAction.RIGHT.value, \n",
    "            RobotAction.RIGHT.value: RobotAction.LEFT.value\n",
    "        }\n",
    "        connected_states_pairs = []\n",
    "        for action in range(self.num_actions-10):  # Movement actions only\n",
    "            possible_prev_state = self.env.sar_robot.next_state_vision(\n",
    "                list(target_state[:2]), \n",
    "                RobotAction(inverse_actions[action])\n",
    "            )\n",
    "            if tuple(possible_prev_state) != tuple(target_state[:2]) and tuple(possible_prev_state) not in self.env.sar_robot.ditches:\n",
    "                connected_states_pairs.append((tuple(possible_prev_state), action))\n",
    "        return connected_states_pairs\n",
    "\n",
    "    def update_attention_space(self, connection, readings):\n",
    "        connected_states = self.get_connected_states(connection)\n",
    "        value_to_add = 2.0 if readings[connection] > 0 else -100.0\n",
    "        for connected_state, action in connected_states:\n",
    "            full_state = tuple([*connected_state, connection[2], connection[3]])\n",
    "            self.attention_space_low[full_state][action] = value_to_add\n",
    "        # Special handling for victim state\n",
    "        if list((connection[0], connection[1])) == self.env.sar_robot.target_pos:\n",
    "            self.attention_space_low[connection][10] = 100  # Action SAVE\n",
    "\n",
    "    def apply_attention_to_q_table(self, Q_table):\n",
    "        for index, value in np.ndenumerate(self.attention_space_low):\n",
    "            *state_indices, action = index\n",
    "            if value != 0:\n",
    "                Q_table[tuple(state_indices)][action] = value\n",
    "                print(f\"Updated Q-table at {tuple(state_indices)}, action {action} with value {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentFlatAttention:\n",
    "    def __init__(self, env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, log_dir=\"curve-SMC-HRL_sparse/flatQ-Att\"):\n",
    "        self.env = env \n",
    "        self.ALPHA = ALPHA\n",
    "        self.GAMMA = GAMMA \n",
    "        self.EPSILON_MAX = EPSILON_MAX\n",
    "        self.EPSILON = EPSILON_MAX\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.EPSILON_MIN = EPSILON_MIN\n",
    "        self.num_states = (self.env.observation_space.high[0] + 1, \n",
    "                           self.env.observation_space.high[1] + 1, \n",
    "                           self.env.observation_space.high[2] + 1,\n",
    "                           self.env.observation_space.high[3] + 1)  # 7*7*4*2\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.Q_table = np.zeros((*self.num_states, self.num_actions))\n",
    "        self.attention_space = AttentionSpace(self.env)  # Instantiate the new AttentionSpace class\n",
    "        self.input_received = False\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if not self.env.sar_robot.visited_information_state:\n",
    "            if np.random.rand() < self.EPSILON:\n",
    "                return self.env.action_space.sample()  # Explore: choose a random action\n",
    "            else:\n",
    "                return np.argmax(self.Q_table[state])  # Exploit: choose the action with max Q-value\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "\n",
    "    def get_state(self, observation):\n",
    "        return tuple(observation)\n",
    "    \n",
    "    def decay_epsilon(self, episodes):\n",
    "        if self.EPSILON > 0.1:\n",
    "            self.EPSILON -= self.DECAY_RATE/episodes\n",
    "        else:\n",
    "            self.EPSILON = self.EPSILON_MIN\n",
    "        return self.EPSILON\n",
    "\n",
    "    def decay_epsilon_exploit(self):\n",
    "        self.EPSILON = 0.01\n",
    "        return self.EPSILON\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.Q_table[next_state])\n",
    "        td_target = reward + self.GAMMA * self.Q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.Q_table[state][action]\n",
    "        self.Q_table[state][action] += self.ALPHA * td_error\n",
    "\n",
    "    def update_attention(self, sensor_readings):\n",
    "        changed_states = self.attention_space.identify_changed_states(sensor_readings)\n",
    "        if changed_states:\n",
    "            for state in changed_states:\n",
    "                self.attention_space.update_attention_space(state, sensor_readings)\n",
    "            self.attention_space.apply_attention_to_q_table(self.Q_table)\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        total_rewards_per_episode = np.zeros(num_episodes)\n",
    "        total_steps_per_episode = np.zeros(num_episodes)\n",
    "        Rewards = 0\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"episode: {episode} | reward: {Rewards} | epsilon: {self.EPSILON}\")\n",
    "\n",
    "            obs, _ = self.env.reset(seed=episode)\n",
    "            s = self.get_state(obs)\n",
    "            terminated = False\n",
    "            Rewards, steps_cnt = 0, 0\n",
    "            \n",
    "            while not terminated:\n",
    "                # Check if we have new sensor readings and update attention\n",
    "                if self.env.sar_robot.visited_information_state and not self.input_received:\n",
    "                    self.update_attention(self.env.sar_robot.sensor_readings)\n",
    "                    self.input_received = True\n",
    "                    print(f\"Updated attention space with new information at episode {episode}\")\n",
    "\n",
    "                # Choose action using epsilon-greedy policy\n",
    "                a = self.epsilon_greedy_policy(s)\n",
    "                \n",
    "                # Step the environment\n",
    "                obs_, r, terminated, _, _ = self.env.step(a)\n",
    "                s_ = self.get_state(obs_)\n",
    "                \n",
    "                # Update Q-table\n",
    "                self.update(s, a, r, s_)\n",
    "                s = s_\n",
    "                \n",
    "                Rewards += r\n",
    "                steps_cnt += 1\n",
    "            \n",
    "            # Log rewards and steps to Tensorboard\n",
    "            if self.writer:\n",
    "                with self.writer.as_default():\n",
    "                    tf.summary.scalar('Episode Return', Rewards, step=episode)\n",
    "                    tf.summary.scalar('Steps per Episode', steps_cnt, step=episode)\n",
    "\n",
    "            # Adjust epsilon for exploration\n",
    "            if not self.env.sar_robot.visited_information_state:\n",
    "                self.EPSILON = self.decay_epsilon(num_episodes)\n",
    "            else:\n",
    "                self.EPSILON = self.decay_epsilon_exploit()\n",
    "            \n",
    "            total_rewards_per_episode[episode] = Rewards\n",
    "            total_steps_per_episode[episode] = steps_cnt\n",
    "        \n",
    "        return total_rewards_per_episode, total_steps_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run (flat learning agent + attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407af5327a9f4278be5c8b7edf29fbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 | reward: 0 | epsilon: 1.0\n",
      "episode: 100 | reward: -115 | epsilon: 0.7999999999999998\n",
      "episode: 200 | reward: -33 | epsilon: 0.5999999999999996\n",
      "episode: 300 | reward: -39 | epsilon: 0.39999999999999947\n",
      "episode: 400 | reward: -52 | epsilon: 0.1999999999999993\n",
      "episode: 500 | reward: -60 | epsilon: 0.01\n",
      "episode: 600 | reward: -55 | epsilon: 0.01\n",
      "episode: 700 | reward: -70 | epsilon: 0.01\n",
      "episode: 800 | reward: -55 | epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 900 | reward: -55 | epsilon: 0.01\n",
      "real LLM is about to start handling the input Hey, there's a victim at the hospital. A fire was reported at the train station. There is a fire at the bank. A safe area is the mall. You must go to the access route in the school. Another access route at the restaurant. And there is a shelter in the shop. There are also reports of significant instances of heat at the bakery. Police told us that no access allowed around the petrol station.\n",
      "Hazardous Coordinates: [(5, 6), (6, 5), (3, 6), (2, 5)]\n",
      "Point of Interest Coordinates: [(0, 3), (4, 1), (3, 0), (2, 0), (1, 2)]\n",
      "real LLM is about to end handling the input Hey, there's a victim at the hospital. A fire was reported at the train station. There is a fire at the bank. A safe area is the mall. You must go to the access route in the school. Another access route at the restaurant. And there is a shelter in the shop. There are also reports of significant instances of heat at the bakery. Police told us that no access allowed around the petrol station.\n",
      "Updated Q-table at (0, 3, 3, 0), action 10 with value 100.0\n",
      "Updated Q-table at (1, 0, 3, 0), action 1 with value 2.0\n",
      "Updated Q-table at (1, 1, 3, 0), action 3 with value 2.0\n",
      "Updated Q-table at (1, 3, 3, 0), action 0 with value 2.0\n",
      "Updated Q-table at (1, 3, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (1, 5, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (2, 0, 3, 0), action 1 with value 2.0\n",
      "Updated Q-table at (2, 1, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (2, 6, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (2, 6, 3, 0), action 2 with value -100.0\n",
      "Updated Q-table at (3, 0, 3, 0), action 0 with value 2.0\n",
      "Updated Q-table at (3, 1, 3, 0), action 1 with value 2.0\n",
      "Updated Q-table at (3, 1, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (3, 5, 3, 0), action 0 with value -100.0\n",
      "Updated Q-table at (3, 5, 3, 0), action 3 with value -100.0\n",
      "Updated Q-table at (4, 0, 3, 0), action 0 with value 2.0\n",
      "Updated Q-table at (4, 0, 3, 0), action 3 with value 2.0\n",
      "Updated Q-table at (4, 2, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (4, 6, 3, 0), action 0 with value -100.0\n",
      "Updated Q-table at (4, 6, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (5, 5, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (5, 5, 3, 0), action 3 with value -100.0\n",
      "Updated Q-table at (6, 4, 3, 0), action 3 with value -100.0\n",
      "Updated Q-table at (6, 6, 3, 0), action 0 with value -100.0\n",
      "Updated Q-table at (6, 6, 3, 0), action 2 with value -100.0\n",
      "Updated attention space with new information at episode 958\n"
     ]
    }
   ],
   "source": [
    "### flat agent (5 runs -- 1500 episodes) \n",
    "all_total_rewards_AGENT_att = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_att = []  # List to store total rewards from each run\n",
    "for _ in range(1):\n",
    "    EPISODES = 1000\n",
    "    ALPHA = 0.1\n",
    "    GAMMA = 0.98\n",
    "    EPSILON_MAX = 1.0\n",
    "    EPSILON_MIN = 0.01\n",
    "    DECAY_RATE = 2\n",
    "    agent_att = QLearningAgentFlatAttention(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN)\n",
    "    returns_att, steps_att = agent_att.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_att.append(returns_att)\n",
    "    all_total_steps_AGENT_att.append(steps_att)\n",
    "    \n",
    "avg_total_rewards_AGENT_att = np.mean(all_total_rewards_AGENT_att, axis=0)\n",
    "avg_total_steps_AGENT_att = np.mean(all_total_steps_AGENT_att, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate (flat learning agent + attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: || State=(4, 1, 0, 0) || Action=RIGHT|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 2: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 3: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 4: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 5: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 6: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 7: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 8: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 9: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 10: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 11: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 12: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 13: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 14: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 15: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 16: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 17: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 18: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 19: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 20: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 21: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 22: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 23: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 24: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 25: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 26: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 27: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 28: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 29: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 30: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 31: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 32: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 33: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 34: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 35: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 36: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 37: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 38: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 39: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 40: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 41: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 42: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 43: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 44: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 45: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 46: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 47: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 48: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 49: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 50: || State=(4, 2, 0, 0) || Action=REMOVE|| Reward=-6 || Next State=(4, 2, 0, 0) || Done=True\n",
      "Test 0: Finished after 50 steps with total reward -55 and 0 collisions at [].\n",
      "Average reward over 1 testing episodes: -55.0\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_agent(env, agent_att)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
