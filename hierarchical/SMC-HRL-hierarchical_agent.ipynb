{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:22:09.407821: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-19 16:22:09.435493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 16:22:09.917408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "### this is where we convert the problem into compatible mode for gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register, registry, EnvSpec\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "from enum import Enum\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actions for Each Option\n",
    "class NavigationActions(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "class InformationCollectionActions(Enum):\n",
    "    COLLECT_X = 0\n",
    "    COLLECT_Y = 1\n",
    "    COLLECT_Z = 2\n",
    "    COLLECT_A = 3\n",
    "    COLLECT_B = 4\n",
    "    COLLECT_C = 5\n",
    "\n",
    "class OperationTriageActions(Enum):\n",
    "    SAVE = 0\n",
    "    USE = 1\n",
    "    REMOVE = 2\n",
    "    CARRY = 3\n",
    "\n",
    "# Define Robot Options (Subtasks)\n",
    "class RobotOption(Enum):\n",
    "    NAVIGATION = 0\n",
    "    INFORMATION_COLLECTION = 1\n",
    "    OPERATION_TRIAGE = 2\n",
    "\n",
    "class GridTile(Enum):\n",
    "    _FLOOR = 0\n",
    "    ROBOT = 1\n",
    "    TARGET = 2\n",
    "    X_INFO = 3\n",
    "    Y_INFO = 4\n",
    "    Z_INFO = 5\n",
    "    DITCH = 6\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "\n",
    "def get_file_type(document_path):\n",
    "    # Split the path and get the extension\n",
    "    _, file_extension = os.path.splitext(document_path)\n",
    "    # Return the file extension without the period\n",
    "    return file_extension[1:] if file_extension else None\n",
    "\n",
    "### class that processes verbal inputs handle disaster-related verbal inputs, analyze them using RAG architecture, and generate a \n",
    "# response in a specified format. It leverages models like ChatOllama and techniques like vector storage and retrieval for its operations.\n",
    "class DisasterResponseAssistant:\n",
    "    def __init__(self, data_path, data_type, model_name=\"mistral\", embedding_model='nomic-embed-text', collection_name=\"rag-chroma\"):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = embedding_model\n",
    "        self.collection_name = collection_name\n",
    "        self.data_path = data_path\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        self.llm = None\n",
    "        self.loader = None\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "        self._load_model()            # Initializes an instance of the ChatOllama model    \n",
    "        self._load_documents()        # Loads and splits the PDF document into chunks\n",
    "        self._create_vectorstore()    # Creates a vector store using Chroma from the document splits\n",
    "        self._create_retriever()      # Creates a retriever from the vector store\n",
    "        \n",
    "        self.hazard_coordinates = []  # To store hazard coordinates\n",
    "        self.poi_coordinates = []     # To store points of interest coordinates\n",
    "    \n",
    "    def _load_model(self):\n",
    "        self.llm = ChatOllama(model=self.model_name)\n",
    "        \n",
    "\n",
    "    def _load_documents(self): ## for json documents\n",
    "        print(f\"document {self.data_type} will be infused\")\n",
    "        if self.data_type == 'pdf':\n",
    "            self.loader = PyPDFLoader(self.data_path)\n",
    "            self.data = self.loader.load_and_split()\n",
    "        elif self.data_type == 'json':\n",
    "            self.loader = JSONLoader(\n",
    "                file_path=self.data_path,\n",
    "                jq_schema='.',\n",
    "                text_content=False)\n",
    "            self.data = self.loader.load()\n",
    "            #pprint(self.data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported document type. Please choose either 'pdf' or 'json'.\")\n",
    "\n",
    "\n",
    "    def _create_vectorstore(self): ## for json documents\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.data,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=embeddings.OllamaEmbeddings(model=self.embedding_model),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def _create_retriever(self):\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "\n",
    "    ### generate a response based on a verbal input\n",
    "    ### construct a template for the response using RAG architecture\n",
    "    def generate_response(self, verbal_input):\n",
    "        prompt_template = \"\"\"You are an assistant, who carefully listens to verbal inputs: {verbal_input} and specialized in analyzing disaster-related inputs. Your task is \n",
    "to identify physical locations mentioned in the text and classify them as either points of interest (POI) or as hazards/dangers (HAZARD) for rescue operations. Use the\n",
    "information provided in the documents: {context}, such as KEYWORDS, descriptions and context when locations are mentioned, to make your classification.\n",
    "Output the classification in the form of a JSON array dictionary with keys 'location', 'coordinates', and 'category'. Here are some rules you always follow:\n",
    "- Focus strictly on physical locations. Avoid including entities that do not represent physical, geographical places (such as individuals, conditions, or \n",
    "  abstract concepts).\n",
    "- Generate human-readable output in the specified dictionary format.\n",
    "- Generate only the requested output, strictly following the dictionary structure.\n",
    "- Within the dictionary, the value of the `category` key must be either 'POI' or 'HAZARD'. \n",
    "- Never generate offensive or foul language.\n",
    "- Never give explanations over your output.\n",
    "Input: {verbal_input}\n",
    "\"\"\"\n",
    "        system_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "        output_parser = StrOutputParser()\n",
    "        after_rag_chain = (\n",
    "            {\"context\": self.retriever, \"verbal_input\": RunnablePassthrough()}\n",
    "            | system_template\n",
    "            | self.llm  # Assuming model_local is defined elsewhere and accessible\n",
    "            | output_parser\n",
    "        )\n",
    "        response = after_rag_chain.invoke(verbal_input)\n",
    "        return response\n",
    "    \n",
    "    def refine_response(self, output):\n",
    "        cleaned_output_str = output.strip().replace('\\n', '').replace('(', '[').replace(')', ']')\n",
    "        output_dict = json.loads(cleaned_output_str)\n",
    "\n",
    "        for item in output_dict:\n",
    "            coord = tuple(item['coordinates'])\n",
    "            if item['category'] == 'HAZARD':\n",
    "                self.hazard_coordinates.append(coord)\n",
    "            else:\n",
    "                self.poi_coordinates.append(coord)\n",
    "                    \n",
    "        print(\"Hazardous Coordinates:\", self.hazard_coordinates)\n",
    "        print(\"Point of Interest Coordinates:\", self.poi_coordinates)\n",
    "        return self.hazard_coordinates, self.poi_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_HRL_agent(env, manager, workers, test_episodes=1):\n",
    "    total_rewards = []\n",
    "    for episode in range(test_episodes):\n",
    "        obs, _ = env.reset(seed=episode)\n",
    "        state = manager.get_state(obs)\n",
    "        terminated = False\n",
    "        total_return, step, cnt = 0, 0, 0\n",
    "        collisions = []\n",
    "        while not terminated:\n",
    "            current_option = np.argmax(manager.Q_table[state]) # Manager chooses option greedily\n",
    "            worker = workers[current_option] # Get the worker for this option\n",
    "            action = np.argmax(worker.Q_table[state]) # Worker chooses action greedily\n",
    "            next_obs, reward, terminated, _, _ = env.step(action) # Take action in environment\n",
    "            next_state = manager.get_state(next_obs)\n",
    "\n",
    "            option_name = RobotOption(current_option).name # Map the option index to the correct option Enum\n",
    "            if current_option == RobotOption.NAVIGATION.value:\n",
    "                action_name = NavigationActions(action).name\n",
    "            elif current_option == RobotOption.INFORMATION_COLLECTION.value:\n",
    "                action_name = InformationCollectionActions(action).name\n",
    "            elif current_option == RobotOption.OPERATION_TRIAGE.value:\n",
    "                action_name = OperationTriageActions(action).name\n",
    "            else:\n",
    "                action_name = f\"Unknown Action ({action})\"\n",
    "\n",
    "            print(f\"Step {step+1}: || State={state} || Option={option_name} || Action={action_name} || Reward={reward} || Next State={next_state} || Done={terminated}\")\n",
    "            # Optionally, print logs or store them\n",
    "            total_return += reward\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "            if tuple([state[0], state[1]]) in env.sar_robot.fires:\n",
    "                print(colored(\"Robot is in fire!\", \"red\"))\n",
    "                cnt += 1\n",
    "                collisions.append(tuple([state[0], state[1]]))\n",
    "        total_rewards.append(total_return)\n",
    "        print(f\"Test {episode}: Finished after {step} steps with total reward {total_return} and {cnt} collisions at {collisions}.\")\n",
    "    avg_reward = sum(total_rewards) / test_episodes\n",
    "    print(f\"Average reward over {test_episodes} testing episodes: {avg_reward}\")\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document json will be infused\n"
     ]
    }
   ],
   "source": [
    "#hier + attention \n",
    "class searchANDrescueRobot_HRL:\n",
    "    def __init__(self, grid_rows=7, grid_cols=7, info_number_needed=3):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.info_number_needed = info_number_needed\n",
    "        self.reset()\n",
    "        # for LLM integration\n",
    "        self.ask_action_counter = 0\n",
    "        self.visited_information_state = False\n",
    "        self.input_received = False\n",
    "        self.POIs, self.fires, self.hazards, self.pois = [], [], [], []\n",
    "        document_path = \"/home/dimi/HRL-LLM/data/sar_data.json\"\n",
    "        document_type = get_file_type(document_path)\n",
    "        self.assistant = DisasterResponseAssistant(document_path, document_type)\n",
    "        self.sensor_readings = {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.init_positions = [[4, 1]]\n",
    "        self.robot_pos = random.choice(self.init_positions)\n",
    "        self.has_info = 0\n",
    "        self.has_saved = 0\n",
    "        self.current_option = RobotOption.NAVIGATION.value\n",
    "        random.seed(seed)\n",
    "        self.target_pos = [0, 3]\n",
    "        self.info_pos1 = [4, 4]\n",
    "        self.info_pos2 = [6, 2]\n",
    "        self.info_pos3 = [5, 5]\n",
    "        self.ditches = [(1, 6), (2, 2), (2, 4), (3, 2), (3, 3), (3, 4), (4, 5), \\\n",
    "                        (5, 0), (5, 1), (5, 2), (6, 0), (0, 2), (0, 4)]\n",
    "        \n",
    "        self.POIs, self.fires = [], []\n",
    "        self.visited_information_state = False\n",
    "\n",
    "\n",
    "    # Update the robot's position\n",
    "    def next_state_vision(self, target, robot_action):\n",
    "        robot_pos = target\n",
    "        self.last_action = robot_action\n",
    "        if robot_action in [NavigationActions.UP, NavigationActions.DOWN, NavigationActions.LEFT, NavigationActions.RIGHT]:\n",
    "            if robot_action == NavigationActions.UP:\n",
    "                if robot_pos[0] > 0:\n",
    "                    robot_pos[0] -= 1  \n",
    "            elif robot_action == NavigationActions.DOWN:\n",
    "                if robot_pos[0] < self.grid_rows-1:\n",
    "                    robot_pos[0] += 1\n",
    "            elif robot_action == NavigationActions.LEFT:\n",
    "                if robot_pos[1] > 0:\n",
    "                    robot_pos[1] -= 1\n",
    "            elif robot_action == NavigationActions.RIGHT:\n",
    "                if robot_pos[1] < self.grid_cols-1:\n",
    "                    robot_pos[1] += 1\n",
    "        if robot_action in [OperationTriageActions.SAVE, OperationTriageActions.USE, OperationTriageActions.REMOVE, OperationTriageActions.CARRY, \\\n",
    "                            InformationCollectionActions.COLLECT_X, InformationCollectionActions.COLLECT_Y, InformationCollectionActions.COLLECT_Z]:\n",
    "            robot_pos = robot_pos\n",
    "        return robot_pos\n",
    "\n",
    "\n",
    "    def perform_action(self, robot_action):\n",
    "        robot_option = self.current_option # hrl\n",
    "        self.last_action = robot_action\n",
    "        info_collected_X, info_collected_Y, info_collected_Z = False, False, False\n",
    "        total_info_collected = False\n",
    "        illegal_action = False\n",
    "        \n",
    "        if robot_option == RobotOption.NAVIGATION.value: ### option 0 (subtask 0)\n",
    "            if robot_action in [NavigationActions.UP.value, NavigationActions.DOWN.value, NavigationActions.LEFT.value, NavigationActions.RIGHT.value]:\n",
    "                if (self.has_info < 1 and self.robot_pos != self.info_pos1) or \\\n",
    "                    (self.has_info == 1 and self.robot_pos != self.info_pos2) or \\\n",
    "                    (self.has_info == 2 and self.robot_pos != self.info_pos3) or \\\n",
    "                    (self.has_info == self.info_number_needed and self.robot_pos != self.target_pos):\n",
    "                    if robot_action == NavigationActions.UP.value:\n",
    "                        if self.robot_pos[0] > 0:\n",
    "                            self.robot_pos[0] -= 1  \n",
    "                    elif robot_action == NavigationActions.DOWN.value:\n",
    "                        if self.robot_pos[0] < self.grid_rows-1:\n",
    "                            self.robot_pos[0] += 1\n",
    "                    elif robot_action == NavigationActions.LEFT.value:\n",
    "                        if self.robot_pos[1] > 0:\n",
    "                            self.robot_pos[1] -= 1\n",
    "                    elif robot_action == NavigationActions.RIGHT.value:\n",
    "                        if self.robot_pos[1] < self.grid_cols-1:\n",
    "                            self.robot_pos[1] += 1\n",
    "                if (self.has_info < 1 and self.robot_pos == self.info_pos1) or \\\n",
    "                    (self.has_info == 1 and self.robot_pos == self.info_pos2) or \\\n",
    "                    (self.has_info == 2 and self.robot_pos == self.info_pos3):\n",
    "                    robot_option = RobotOption.INFORMATION_COLLECTION.value\n",
    "                if self.has_info == self.info_number_needed and self.robot_pos == self.target_pos:\n",
    "                    robot_option = RobotOption.OPERATION_TRIAGE.value\n",
    "            else:\n",
    "                illegal_action = True\n",
    "        \n",
    "        elif robot_option == RobotOption.INFORMATION_COLLECTION.value:   ### option 1 (subtask 1)\n",
    "            if robot_action in [InformationCollectionActions.COLLECT_X.value, InformationCollectionActions.COLLECT_Y.value, InformationCollectionActions.COLLECT_Z.value]:\n",
    "                if robot_action == InformationCollectionActions.COLLECT_X.value:\n",
    "                    if self.robot_pos == self.info_pos1 and self.has_info < 1:\n",
    "                        self.has_info += 1\n",
    "                        info_collected_X = True\n",
    "                        robot_option = RobotOption.NAVIGATION.value\n",
    "                if robot_action == InformationCollectionActions.COLLECT_Y.value:\n",
    "                    if self.robot_pos == self.info_pos2 and self.has_info == 1:\n",
    "                        self.has_info += 1\n",
    "                        info_collected_Y = True\n",
    "                        robot_option = RobotOption.NAVIGATION.value\n",
    "                if robot_action == InformationCollectionActions.COLLECT_Z.value:\n",
    "                    if self.robot_pos == self.info_pos3 and self.has_info == 2:\n",
    "                        self.perform_collect_action() # Collect the third info and exploit the knowledge from this moment on\n",
    "                        self.has_info += 1\n",
    "                        info_collected_Z = True\n",
    "                        total_info_collected = True\n",
    "                        robot_option = RobotOption.NAVIGATION.value\n",
    "            else:\n",
    "                illegal_action = True\n",
    "\n",
    "        elif robot_option == RobotOption.OPERATION_TRIAGE.value:  ### option 2 (subtask 2)\n",
    "            if robot_action in [OperationTriageActions.SAVE.value, OperationTriageActions.USE.value, OperationTriageActions.REMOVE.value, OperationTriageActions.CARRY.value]:\n",
    "                if robot_action == OperationTriageActions.SAVE.value:\n",
    "                    if self.robot_pos == self.target_pos and self.has_info == self.info_number_needed:\n",
    "                        self.has_saved = 1\n",
    "                else:\n",
    "                    illegal_action = True\n",
    "            else:\n",
    "                illegal_action = True\n",
    "        \n",
    "        self.current_option = robot_option\n",
    "                \n",
    "        mission_complete = self.has_saved\n",
    "        return mission_complete, info_collected_X, info_collected_Y, total_info_collected, illegal_action\n",
    "    \n",
    "\n",
    "    def perform_collect_action(self):\n",
    "        self.ask_action_counter += 1\n",
    "        x, y = self.robot_pos\n",
    "        verbal_inputs = []\n",
    "        if self.has_info == 2:  ## should be 2 if total number of infos are 3 \n",
    "            verbal_input = (\"Hey, there's a victim at the hospital. A fire was reported at the train station. There is a fire at the bank. A safe area is the mall. You must go to the access route in the school. Another access route at the restaurant. And there is a shelter in the shop. There are also reports of significant instances of heat at the bakery. Police told us that no access allowed around the petrol station.\")\n",
    "            # print(f\"real LLM is about to start handling the input {verbal_input}\")\n",
    "            verbal_inputs.append(verbal_input)\n",
    "            \n",
    "            if self.ask_action_counter <= 1:\n",
    "                print(f\"real LLM is about to start handling the input {verbal_input}\")\n",
    "                for input_text in verbal_inputs:\n",
    "                    response = self.assistant.generate_response(input_text)\n",
    "                    if response:\n",
    "                        self.visited_information_state = True\n",
    "                    self.hazards, self.pois = self.assistant.refine_response(response)\n",
    "                    print(f\"real LLM is about to end handling the input {verbal_input}\")\n",
    "                    self.update_environment_REAL(self.hazards, self.pois)\n",
    "            else:\n",
    "                # #print(f\"input will be handled hereby by pseudoLLM\")\n",
    "                # print(self.hazards, self.pois)\n",
    "                self.visited_information_state = True\n",
    "                self.update_environment_REAL(self.hazards, self.pois)\n",
    "            \n",
    "    def update_environment_REAL(self, haz, poi):\n",
    "        for hazardous_location in haz:\n",
    "            self.sensor_readings[(hazardous_location[0], hazardous_location[1], 3, 0)] = -10.0\n",
    "            self.fires.append(hazardous_location)\n",
    "        for safe_location in poi:\n",
    "            self.sensor_readings[(safe_location[0], safe_location[1], 3, 0)] = 10.0\n",
    "            self.POIs.append(safe_location)\n",
    "\n",
    "    \n",
    "    def is_in_ditch(self):\n",
    "        return tuple(self.robot_pos) in self.ditches\n",
    "\n",
    "    def render(self):\n",
    "        for x in range(self.grid_rows):\n",
    "            for y in range(self.grid_cols):\n",
    "                if [x, y] == self.robot_pos:\n",
    "                    print(GridTile.ROBOT, end=' ')\n",
    "                elif [x, y] == self.target_pos:\n",
    "                    print(GridTile.TARGET, end=' ')\n",
    "                elif [x, y] == self.info_pos1:\n",
    "                    print(GridTile.X_INFO, end=' ')\n",
    "                elif [x, y] == self.info_pos2:\n",
    "                    print(GridTile.Y_INFO, end=' ')\n",
    "                elif [x, y] == self.info_pos3:\n",
    "                    print(GridTile.Z_INFO, end=' ')\n",
    "                elif tuple([x, y]) in self.ditches:\n",
    "                    print(GridTile.DITCH, end=' ')\n",
    "                else:\n",
    "                    print(GridTile._FLOOR, end=' ')\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "\n",
    "class SARrobotEnv_HRL(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], 'render_fps': 1}\n",
    "    def __init__(self, grid_rows=7, grid_cols=7, render_mode=None, info_number_needed=3):\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        self.sar_robot = searchANDrescueRobot_HRL(grid_rows, grid_cols, info_number_needed)\n",
    "        self.option_space = spaces.Discrete(len(RobotOption))\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low = 0,\n",
    "            high = np.array([self.grid_rows-1, self.grid_cols-1, info_number_needed, 1]),\n",
    "            shape = (4,),\n",
    "            dtype = np.int32\n",
    "        )\n",
    "        \n",
    "        self.max_steps = 50\n",
    "        self.current_step = 0\n",
    "        self.turnPenalty = -1\n",
    "        self.stepsPenalty = -5\n",
    "        self.ditchPenalty = -30\n",
    "        self.illegalActionPenalty = -5  # Penalty for illegal actions\n",
    "        self.winReward = 100\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.sar_robot.reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        obs = np.concatenate((self.sar_robot.robot_pos, [self.sar_robot.has_info], [self.sar_robot.has_saved])).astype(np.int32)\n",
    "        info = {'option': self.sar_robot.current_option}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        self.current_step += 1\n",
    "        target_reached, info_collected_X, info_collected_Y, total_info_collected, illegal_action = self.sar_robot.perform_action(action)\n",
    "        terminated = False\n",
    "        \n",
    "        if self.sar_robot.is_in_ditch():\n",
    "            reward = self.ditchPenalty\n",
    "            terminated = True\n",
    "            self.sar_robot.current_option = RobotOption.NAVIGATION\n",
    "        \n",
    "        if self.is_max_steps_exceeded():\n",
    "            reward = self.stepsPenalty\n",
    "            terminated = True\n",
    "            self.sar_robot.current_option = RobotOption.NAVIGATION\n",
    "        \n",
    "        if info_collected_X or info_collected_Y or total_info_collected:\n",
    "            reward = 6  # Reward for collecting info\n",
    "        \n",
    "        if target_reached:\n",
    "            reward = self.winReward\n",
    "            terminated = True\n",
    "\n",
    "        if illegal_action:\n",
    "            reward = self.illegalActionPenalty\n",
    "        \n",
    "        reward += self.turnPenalty\n",
    "        \n",
    "        obs = np.concatenate((self.sar_robot.robot_pos, [self.sar_robot.has_info], [self.sar_robot.has_saved])).astype(np.int32)\n",
    "        info = {'option': self.sar_robot.current_option}\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"Option: {self.sar_robot.current_option}, Action: {action}, Reward: {reward}, Terminated: {terminated}\")\n",
    "            self.render()\n",
    "        \n",
    "        return obs, reward, terminated, False, info\n",
    "\n",
    "    def is_max_steps_exceeded(self):\n",
    "        return self.current_step >= self.max_steps\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            self.sar_robot.render()\n",
    "\n",
    "\n",
    "env = SARrobotEnv_HRL(grid_rows=7, grid_cols=7, render_mode='None',  info_number_needed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentHierarchical:\n",
    "    def __init__(self, env, action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay, log_dir=\"testing_HRLgym/HierQ\"):\n",
    "        self.env = env \n",
    "        self.ALPHA = ALPHA\n",
    "        self.GAMMA = GAMMA \n",
    "        self.EPSILON_MAX = EPSILON_MAX\n",
    "        self.EPSILON = EPSILON_MAX\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.EPSILON_MIN = EPSILON_MIN\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.num_states = (self.env.observation_space.high[0] + 1, \n",
    "                           self.env.observation_space.high[1] + 1, \n",
    "                           self.env.observation_space.high[2] + 1,\n",
    "                           self.env.observation_space.high[3] + 1)  # 7*7*4*2\n",
    "        self.action_space_size = action_space_size\n",
    "        self.Q_table = np.zeros((*self.num_states, self.action_space_size))\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.EPSILON:\n",
    "            return np.random.randint(self.action_space_size)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])  # Exploit\n",
    "\n",
    "    def get_state(self, observation):\n",
    "        return tuple(observation)\n",
    "\n",
    "    def decay_epsilon(self, episodes):\n",
    "        if self.EPSILON > 0.1:\n",
    "            self.EPSILON -= self.DECAY_RATE / episodes\n",
    "        else:\n",
    "            self.EPSILON = self.EPSILON_MIN\n",
    "        return self.EPSILON\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.Q_table[next_state])\n",
    "        td_target = reward + self.GAMMA * self.Q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.Q_table[state][action]\n",
    "        self.Q_table[state][action] += self.ALPHA * td_error\n",
    "\n",
    "\n",
    "    def train(self, manager, workers, num_episodes):\n",
    "        return_list_Q = []\n",
    "        total_rewards_per_episode = np.zeros(num_episodes)\n",
    "        total_steps_per_episode = np.zeros(num_episodes)\n",
    "        Rewards = 0\n",
    "\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode: {episode} | Reward: {Rewards} | epsilon: {self.EPSILON}\")\n",
    "\n",
    "            obs, _ = self.env.reset(seed=episode)\n",
    "            s = self.get_state(obs)\n",
    "            terminated = False\n",
    "            Rewards, steps_cnt, episode_return_Q = 0, 0, 0\n",
    "\n",
    "            while not terminated:\n",
    "                option = self.env.sar_robot.current_option\n",
    "                worker = workers[option]\n",
    "                a = worker.epsilon_greedy_policy(s)\n",
    "                obs_, r, terminated, _, _ = self.env.step(a)\n",
    "                s_ = self.get_state(obs_)\n",
    "                \n",
    "                Rewards += r\n",
    "                episode_return_Q += r\n",
    "\n",
    "                worker.update(s, a, r, s_)\n",
    "                manager.update(s, option, r, s_)\n",
    "\n",
    "                s = s_\n",
    "                steps_cnt += 1\n",
    "\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar('Episode Return', Rewards, step=episode)\n",
    "                tf.summary.scalar('Steps per Episode', steps_cnt, step=episode)\n",
    "\n",
    "            manager.EPSILON = self.decay_epsilon(num_episodes)\n",
    "            for w in workers.values():\n",
    "                w.decay_epsilon(num_episodes)\n",
    "\n",
    "            total_rewards_per_episode[episode] = Rewards\n",
    "            return_list_Q.append(episode_return_Q)\n",
    "\n",
    "        return total_rewards_per_episode, workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier\n",
    "# Define action space sizes for each worker\n",
    "# Manager action space size (Options)\n",
    "manager_action_space_size = len(RobotOption)\n",
    "\n",
    "# Workers for each option\n",
    "explore_action_space_size = len(NavigationActions)\n",
    "collect_action_space_size = len(InformationCollectionActions)\n",
    "operate_action_space_size = len(OperationTriageActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:23:07.188318: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.191879: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.192012: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.193385: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.193503: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.193567: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.234068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.234201: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.234274: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 16:23:07.234327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1227 MB memory:  -> device: 0, name: NVIDIA RTX A2000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a7b3aa2d16412bbcd5a66146a37a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Reward: 0 | epsilon: 1.0\n",
      "Episode: 100 | Reward: -52 | epsilon: 0.8666666666666702\n",
      "Episode: 200 | Reward: -32 | epsilon: 0.7333333333333405\n",
      "Episode: 300 | Reward: -34 | epsilon: 0.6000000000000107\n",
      "Episode: 400 | Reward: -31 | epsilon: 0.4666666666666796\n",
      "Episode: 500 | Reward: -43 | epsilon: 0.3333333333333443\n",
      "Episode: 600 | Reward: -49 | epsilon: 0.20000000000001006\n",
      "Episode: 700 | Reward: -37 | epsilon: 0.01\n",
      "Episode: 800 | Reward: -37 | epsilon: 0.01\n",
      "Episode: 900 | Reward: 85 | epsilon: 0.01\n",
      "Episode: 1000 | Reward: 94 | epsilon: 0.01\n",
      "Episode: 1100 | Reward: 94 | epsilon: 0.01\n",
      "Episode: 1200 | Reward: 93 | epsilon: 0.01\n",
      "Episode: 1300 | Reward: 94 | epsilon: 0.01\n",
      "Episode: 1400 | Reward: 94 | epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "#hier\n",
    "all_total_rewards_AGENT_hier = []  # List to store total rewards from each run\n",
    "EPISODES = 1500\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.98\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "epsilon_decay = 0.95\n",
    "DECAY_RATE = 2\n",
    "for _ in range(1):\n",
    "    # Manager for choosing options\n",
    "    manager_hier = QLearningAgentHierarchical(env, manager_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)\n",
    "    explore_worker_hier = QLearningAgentHierarchical(env, explore_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)  # [Up, down, left, right]\n",
    "    collect_worker_hier = QLearningAgentHierarchical(env, collect_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)  # [A, B, C, X, Y, Z]\n",
    "    operate_worker_hier = QLearningAgentHierarchical(env, operate_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)  # [save, use, remove, carry]\n",
    "\n",
    "    workers_hier = {\n",
    "        0: explore_worker_hier,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier,  # Worker for COLLECT\n",
    "        2: operate_worker_hier   # Worker for OPERATE\n",
    "    }\n",
    "    hier_returns, workers_simple = manager_hier.train(manager_hier, workers_hier, EPISODES)\n",
    "    all_total_rewards_AGENT_hier.append(hier_returns)\n",
    "    \n",
    "avg_total_rewards_AGENT_hier = np.mean(all_total_rewards_AGENT_hier, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: || State=(4, 1, 0, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 2: || State=(4, 2, 0, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(4, 3, 0, 0) || Done=False\n",
      "Step 3: || State=(4, 3, 0, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(4, 4, 0, 0) || Done=False\n",
      "Step 4: || State=(4, 4, 0, 0) || Option=INFORMATION_COLLECTION || Action=COLLECT_X || Reward=5 || Next State=(4, 4, 1, 0) || Done=False\n",
      "Step 5: || State=(4, 4, 1, 0) || Option=NAVIGATION || Action=DOWN || Reward=-1 || Next State=(5, 4, 1, 0) || Done=False\n",
      "Step 6: || State=(5, 4, 1, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(5, 3, 1, 0) || Done=False\n",
      "Step 7: || State=(5, 3, 1, 0) || Option=NAVIGATION || Action=DOWN || Reward=-1 || Next State=(6, 3, 1, 0) || Done=False\n",
      "Step 8: || State=(6, 3, 1, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(6, 2, 1, 0) || Done=False\n",
      "Step 9: || State=(6, 2, 1, 0) || Option=INFORMATION_COLLECTION || Action=COLLECT_Y || Reward=5 || Next State=(6, 2, 2, 0) || Done=False\n",
      "Step 10: || State=(6, 2, 2, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(6, 3, 2, 0) || Done=False\n",
      "Step 11: || State=(6, 3, 2, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(6, 4, 2, 0) || Done=False\n",
      "Step 12: || State=(6, 4, 2, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(6, 5, 2, 0) || Done=False\n",
      "Step 13: || State=(6, 5, 2, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(5, 5, 2, 0) || Done=False\n",
      "Step 14: || State=(5, 5, 2, 0) || Option=INFORMATION_COLLECTION || Action=COLLECT_Z || Reward=5 || Next State=(5, 5, 3, 0) || Done=False\n",
      "Step 15: || State=(5, 5, 3, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(5, 6, 3, 0) || Done=False\n",
      "\u001b[31mRobot is in fire!\u001b[0m\n",
      "Step 16: || State=(5, 6, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(4, 6, 3, 0) || Done=False\n",
      "Step 17: || State=(4, 6, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(3, 6, 3, 0) || Done=False\n",
      "\u001b[31mRobot is in fire!\u001b[0m\n",
      "Step 18: || State=(3, 6, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(2, 6, 3, 0) || Done=False\n",
      "Step 19: || State=(2, 6, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(2, 5, 3, 0) || Done=False\n",
      "\u001b[31mRobot is in fire!\u001b[0m\n",
      "Step 20: || State=(2, 5, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(1, 5, 3, 0) || Done=False\n",
      "Step 21: || State=(1, 5, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(1, 4, 3, 0) || Done=False\n",
      "Step 22: || State=(1, 4, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(1, 3, 3, 0) || Done=False\n",
      "Step 23: || State=(1, 3, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(0, 3, 3, 0) || Done=False\n",
      "Step 24: || State=(0, 3, 3, 0) || Option=OPERATION_TRIAGE || Action=SAVE || Reward=99 || Next State=(0, 3, 3, 1) || Done=True\n",
      "Test 0: Finished after 24 steps with total reward 94 and 3 collisions at [(5, 6), (3, 6), (2, 5)].\n",
      "Average reward over 1 testing episodes: 94.0\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_HRL_agent(env, manager_hier, workers_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_policy(env, manager, workers, num_episodes=1):\n",
    "#     for episode in range(num_episodes):\n",
    "#         obs, _ = env.reset()\n",
    "#         s = manager.get_state(obs)\n",
    "#         terminated = False\n",
    "#         total_reward = 0\n",
    "#         steps = 0\n",
    "#         path = []\n",
    "#         while not terminated:\n",
    "#             # Manager chooses option greedily\n",
    "#             option = np.argmax(manager.Q_table[s])\n",
    "#             # Get the worker for this option\n",
    "#             worker = workers[option]\n",
    "#             # Worker chooses action greedily\n",
    "#             action = np.argmax(worker.Q_table[s])\n",
    "#             # Take action in environment\n",
    "#             obs_, reward, terminated, truncated, info = env.step(action)\n",
    "#             s_ = manager.get_state(obs_)\n",
    "#             # Log the transition\n",
    "#             path.append({\n",
    "#                 'state': s,\n",
    "#                 'option': option,\n",
    "#                 'action': action,\n",
    "#                 'next_state': s_,\n",
    "#                 'reward': reward,\n",
    "#                 'terminated': terminated\n",
    "#             })\n",
    "#             s = s_\n",
    "#             total_reward += reward\n",
    "#             steps += 1\n",
    "#         # After episode ends, log the path\n",
    "#         print(f\"Episode {episode}: Total reward: {total_reward}, Steps: {steps}\")\n",
    "#         for idx, t in enumerate(path):\n",
    "#             print(f\"Step {idx + 1}:\")\n",
    "#             print(f\"  State: {t['state']}\")\n",
    "#             print(f\"  Option chosen by Manager: {t['option']}\")\n",
    "#             print(f\"  Action taken by Worker: {t['action']}\")\n",
    "#             print(f\"  Next State: {t['next_state']}\")\n",
    "#             print(f\"  Reward: {t['reward']}\")\n",
    "#             print(f\"  Terminated: {t['terminated']}\\n\")\n",
    "#     return\n",
    "# evaluate_policy(env, manager_hier, workers_hier, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSpace:\n",
    "    def __init__(self, env, action_space_size):\n",
    "        self.env = env\n",
    "        self.num_states = (self.env.observation_space.high[0] + 1, \n",
    "                           self.env.observation_space.high[1] + 1, \n",
    "                           self.env.observation_space.high[2] + 1,\n",
    "                           self.env.observation_space.high[3] + 1)  # 7*7*4*2\n",
    "        self.action_space_size = action_space_size\n",
    "        self.attention_space_low = np.zeros((*self.num_states, self.action_space_size))\n",
    "\n",
    "    def identify_changed_states(self, readings):\n",
    "        return [i for i, value in readings.items() if value != 1]\n",
    "\n",
    "    def get_connected_states(self, target_state):\n",
    "        inverse_actions = {\n",
    "            NavigationActions.UP.value: NavigationActions.DOWN.value,\n",
    "            NavigationActions.DOWN.value: NavigationActions.UP.value,\n",
    "            NavigationActions.LEFT.value: NavigationActions.RIGHT.value,\n",
    "            NavigationActions.RIGHT.value: NavigationActions.LEFT.value\n",
    "        }\n",
    "        connected_states_pairs = []\n",
    "        for action in range(len(NavigationActions)):\n",
    "            possible_prev_state = self.env.sar_robot.next_state_vision(list(target_state[:2]), NavigationActions(inverse_actions[action]))\n",
    "            if tuple(possible_prev_state) != tuple(target_state[:2]) and tuple(possible_prev_state) not in self.env.sar_robot.ditches:\n",
    "                connected_states_pairs.append((tuple(possible_prev_state), action))\n",
    "        return connected_states_pairs\n",
    "\n",
    "    def update_attention_space(self, connection, readings):\n",
    "        connected_states = self.get_connected_states(connection)\n",
    "        value_to_add = 2.0 if readings[connection] > 0 else -100.0\n",
    "        for connected_state, action in connected_states:\n",
    "            full_state = tuple([*connected_state, connection[2], connection[3]])\n",
    "            if self.attention_space_low[full_state][action] == 0:  # Avoid overwriting\n",
    "                self.attention_space_low[full_state][action] = value_to_add\n",
    "        if list((connection[0], connection[1])) == self.env.sar_robot.target_pos:\n",
    "            self.attention_space_low[connection][0] = 100  # 'save' is highly favored\n",
    "\n",
    "    def apply_attention_to_q_table(self, Q_table):\n",
    "        for index, value in np.ndenumerate(self.attention_space_low):\n",
    "            *state_indices, action = index\n",
    "            if value != 0:\n",
    "                Q_table[tuple(state_indices)][action] = value\n",
    "                print(f\"Updated Q-table at {tuple(state_indices)}, action {action} with value {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentHierarchicalAttention:\n",
    "    def __init__(self, env, action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay, log_dir=\"testing_HRLgym/HierQ-Att\"):\n",
    "        self.env = env \n",
    "        self.ALPHA = ALPHA\n",
    "        self.GAMMA = GAMMA \n",
    "        self.EPSILON_MAX = EPSILON_MAX\n",
    "        self.EPSILON = EPSILON_MAX\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.EPSILON_MIN = EPSILON_MIN\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.num_states = (self.env.observation_space.high[0] + 1, \n",
    "                           self.env.observation_space.high[1] + 1, \n",
    "                           self.env.observation_space.high[2] + 1,\n",
    "                           self.env.observation_space.high[3] + 1)  # 7*7*4*2\n",
    "        self.action_space_size = action_space_size\n",
    "        self.Q_table = np.zeros((*self.num_states, self.action_space_size))\n",
    "        self.attention_space = AttentionSpace(self.env, self.action_space_size)  # Refactor: use AttentionSpace class\n",
    "        self.input_received = False\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if not self.env.sar_robot.visited_information_state:\n",
    "            if np.random.rand() < self.EPSILON:\n",
    "                return np.random.randint(self.action_space_size)  # Explore\n",
    "            else:\n",
    "                return np.argmax(self.Q_table[state])  # Exploit\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])  # Exploit\n",
    "\n",
    "    def get_state(self, observation):\n",
    "        return tuple(observation)\n",
    "\n",
    "    def decay_epsilon(self, episodes):\n",
    "        if self.EPSILON > 0.1:\n",
    "            self.EPSILON -= self.DECAY_RATE / episodes\n",
    "        else:\n",
    "            self.EPSILON = self.EPSILON_MIN\n",
    "        return self.EPSILON\n",
    "\n",
    "    def decay_epsilon_exploit(self):\n",
    "        self.EPSILON = 0.01\n",
    "        return self.EPSILON\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.Q_table[next_state])\n",
    "        td_target = reward + self.GAMMA * self.Q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.Q_table[state][action]\n",
    "        self.Q_table[state][action] += self.ALPHA * td_error\n",
    "\n",
    "    def update_attention(self, manager, workers, sensor_readings):\n",
    "        changed_states = self.attention_space.identify_changed_states(sensor_readings)\n",
    "        if changed_states:\n",
    "            for state in changed_states:\n",
    "                if list((state[0], state[1])) != self.env.sar_robot.target_pos:\n",
    "                    workers[0].attention_space.update_attention_space(state, sensor_readings)\n",
    "                else:\n",
    "                    workers[2].attention_space.update_attention_space(state, sensor_readings)\n",
    "        workers[0].attention_space.apply_attention_to_q_table(workers[0].Q_table)\n",
    "        workers[2].attention_space.apply_attention_to_q_table(workers[2].Q_table)\n",
    "\n",
    "\n",
    "    def train(self, manager, workers, num_episodes):\n",
    "        return_list_Q = []\n",
    "        total_rewards_per_episode = np.zeros(num_episodes)\n",
    "        total_steps_per_episode = np.zeros(num_episodes)\n",
    "        attention_space = {}\n",
    "        Rewards = 0\n",
    "\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode: {episode} | Reward: {Rewards} | epsilon: {self.EPSILON}\")\n",
    "\n",
    "            obs, _ = self.env.reset(seed=episode)\n",
    "            s = self.get_state(obs)\n",
    "            terminated = False\n",
    "            Rewards, steps_cnt, episode_return_Q = 0, 0, 0\n",
    "\n",
    "            while not terminated:\n",
    "                if self.env.sar_robot.visited_information_state and not manager.input_received:\n",
    "                    self.update_attention(manager, workers, self.env.sar_robot.sensor_readings)\n",
    "                    manager.input_received = True\n",
    "                    print(f\"Updated attention space with new information at episode {episode}\")\n",
    "\n",
    "                option = self.env.sar_robot.current_option\n",
    "                worker = workers[option]\n",
    "                a = worker.epsilon_greedy_policy(s)\n",
    "                obs_, r, terminated, _, _ = self.env.step(a)\n",
    "                s_ = self.get_state(obs_)\n",
    "                \n",
    "                Rewards += r\n",
    "                episode_return_Q += r\n",
    "\n",
    "                worker.update(s, a, r, s_)\n",
    "                manager.update(s, option, r, s_)\n",
    "\n",
    "                s = s_\n",
    "                steps_cnt += 1\n",
    "\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar('Episode Return', Rewards, step=episode)\n",
    "                tf.summary.scalar('Steps per Episode', steps_cnt, step=episode)\n",
    "\n",
    "            manager.EPSILON = self.decay_epsilon(num_episodes) if not self.env.sar_robot.visited_information_state else self.decay_epsilon_exploit()\n",
    "            for w in workers.values():\n",
    "                w.decay_epsilon(num_episodes) if not self.env.sar_robot.visited_information_state else w.decay_epsilon_exploit()\n",
    "\n",
    "            total_rewards_per_episode[episode] = Rewards\n",
    "            return_list_Q.append(episode_return_Q)\n",
    "\n",
    "        return total_rewards_per_episode, attention_space, workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da0aff43a364005a0a354d96b1a31cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Reward: 0 | epsilon: 1.0\n",
      "Episode: 100 | Reward: -34 | epsilon: 0.8666666666666702\n",
      "Episode: 200 | Reward: -31 | epsilon: 0.7333333333333405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300 | Reward: -41 | epsilon: 0.6000000000000107\n",
      "real LLM is about to start handling the input Hey, there's a victim at the hospital. A fire was reported at the train station. There is a fire at the bank. A safe area is the mall. You must go to the access route in the school. Another access route at the restaurant. And there is a shelter in the shop. There are also reports of significant instances of heat at the bakery. Police told us that no access allowed around the petrol station.\n",
      "Hazardous Coordinates: [(5, 6), (6, 5), (3, 6), (2, 5)]\n",
      "Point of Interest Coordinates: [(0, 3), (4, 1), (3, 0), (2, 0), (1, 2)]\n",
      "real LLM is about to end handling the input Hey, there's a victim at the hospital. A fire was reported at the train station. There is a fire at the bank. A safe area is the mall. You must go to the access route in the school. Another access route at the restaurant. And there is a shelter in the shop. There are also reports of significant instances of heat at the bakery. Police told us that no access allowed around the petrol station.\n",
      "Updated Q-table at (1, 0, 3, 0), action 1 with value 2.0\n",
      "Updated Q-table at (1, 1, 3, 0), action 3 with value 2.0\n",
      "Updated Q-table at (1, 3, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (1, 5, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (2, 0, 3, 0), action 1 with value 2.0\n",
      "Updated Q-table at (2, 1, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (2, 6, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (2, 6, 3, 0), action 2 with value -100.0\n",
      "Updated Q-table at (3, 0, 3, 0), action 0 with value 2.0\n",
      "Updated Q-table at (3, 1, 3, 0), action 1 with value 2.0\n",
      "Updated Q-table at (3, 1, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (3, 5, 3, 0), action 0 with value -100.0\n",
      "Updated Q-table at (3, 5, 3, 0), action 3 with value -100.0\n",
      "Updated Q-table at (4, 0, 3, 0), action 0 with value 2.0\n",
      "Updated Q-table at (4, 0, 3, 0), action 3 with value 2.0\n",
      "Updated Q-table at (4, 2, 3, 0), action 2 with value 2.0\n",
      "Updated Q-table at (4, 6, 3, 0), action 0 with value -100.0\n",
      "Updated Q-table at (4, 6, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (5, 5, 3, 0), action 1 with value -100.0\n",
      "Updated Q-table at (5, 5, 3, 0), action 3 with value -100.0\n",
      "Updated Q-table at (6, 4, 3, 0), action 3 with value -100.0\n",
      "Updated Q-table at (6, 6, 3, 0), action 0 with value -100.0\n",
      "Updated Q-table at (6, 6, 3, 0), action 2 with value -100.0\n",
      "Updated Q-table at (0, 3, 3, 0), action 0 with value 100.0\n",
      "Updated Q-table at (1, 3, 3, 0), action 0 with value 2.0\n",
      "Updated attention space with new information at episode 317\n",
      "Episode: 400 | Reward: -43 | epsilon: 0.01\n",
      "Episode: 500 | Reward: -37 | epsilon: 0.01\n",
      "Episode: 600 | Reward: 89 | epsilon: 0.01\n",
      "Episode: 700 | Reward: 92 | epsilon: 0.01\n",
      "Episode: 800 | Reward: 92 | epsilon: 0.01\n",
      "Episode: 900 | Reward: 92 | epsilon: 0.01\n",
      "Episode: 1000 | Reward: 86 | epsilon: 0.01\n",
      "Episode: 1100 | Reward: 92 | epsilon: 0.01\n",
      "Episode: 1200 | Reward: 92 | epsilon: 0.01\n",
      "Episode: 1300 | Reward: 90 | epsilon: 0.01\n",
      "Episode: 1400 | Reward: 92 | epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "#hier\n",
    "all_total_rewards_AGENT_hier_att = []  # List to store total rewards from each run\n",
    "EPISODES = 1500\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.98\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "epsilon_decay = 0.95\n",
    "DECAY_RATE = 2\n",
    "for _ in range(1):\n",
    "    # Manager for choosing options\n",
    "    manager_hier_att = QLearningAgentHierarchicalAttention(env, manager_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)\n",
    "    explore_worker_hier_att = QLearningAgentHierarchicalAttention(env, explore_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)  # [Up, down, left, right]\n",
    "    collect_worker_hier_att = QLearningAgentHierarchicalAttention(env, collect_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)  # [A, B, C, X, Y, Z]\n",
    "    operate_worker_hier_att = QLearningAgentHierarchicalAttention(env, operate_action_space_size, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, epsilon_decay)  # [save, use, remove, carry]\n",
    "\n",
    "    workers_hier_att = {\n",
    "        0: explore_worker_hier_att,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier_att,  # Worker for COLLECT\n",
    "        2: operate_worker_hier_att   # Worker for OPERATE\n",
    "    }\n",
    "    hier_returns_att, attention, workers_simple_att = manager_hier_att.train(manager_hier_att, workers_hier_att, EPISODES)\n",
    "    all_total_rewards_AGENT_hier_att.append(hier_returns_att)\n",
    "    \n",
    "avg_total_rewards_AGENT_hier_att = np.mean(all_total_rewards_AGENT_hier_att, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: || State=(4, 1, 0, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(4, 2, 0, 0) || Done=False\n",
      "Step 2: || State=(4, 2, 0, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(4, 3, 0, 0) || Done=False\n",
      "Step 3: || State=(4, 3, 0, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(4, 4, 0, 0) || Done=False\n",
      "Step 4: || State=(4, 4, 0, 0) || Option=INFORMATION_COLLECTION || Action=COLLECT_X || Reward=5 || Next State=(4, 4, 1, 0) || Done=False\n",
      "Step 5: || State=(4, 4, 1, 0) || Option=NAVIGATION || Action=DOWN || Reward=-1 || Next State=(5, 4, 1, 0) || Done=False\n",
      "Step 6: || State=(5, 4, 1, 0) || Option=NAVIGATION || Action=DOWN || Reward=-1 || Next State=(6, 4, 1, 0) || Done=False\n",
      "Step 7: || State=(6, 4, 1, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(6, 3, 1, 0) || Done=False\n",
      "Step 8: || State=(6, 3, 1, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(6, 2, 1, 0) || Done=False\n",
      "Step 9: || State=(6, 2, 1, 0) || Option=INFORMATION_COLLECTION || Action=COLLECT_Y || Reward=5 || Next State=(6, 2, 2, 0) || Done=False\n",
      "Step 10: || State=(6, 2, 2, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(6, 3, 2, 0) || Done=False\n",
      "Step 11: || State=(6, 3, 2, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(6, 4, 2, 0) || Done=False\n",
      "Step 12: || State=(6, 4, 2, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(6, 5, 2, 0) || Done=False\n",
      "Step 13: || State=(6, 5, 2, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(5, 5, 2, 0) || Done=False\n",
      "Step 14: || State=(5, 5, 2, 0) || Option=INFORMATION_COLLECTION || Action=COLLECT_Z || Reward=5 || Next State=(5, 5, 3, 0) || Done=False\n",
      "Step 15: || State=(5, 5, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(5, 4, 3, 0) || Done=False\n",
      "Step 16: || State=(5, 4, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(4, 4, 3, 0) || Done=False\n",
      "Step 17: || State=(4, 4, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(4, 3, 3, 0) || Done=False\n",
      "Step 18: || State=(4, 3, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(4, 2, 3, 0) || Done=False\n",
      "Step 19: || State=(4, 2, 3, 0) || Option=NAVIGATION || Action=LEFT || Reward=-1 || Next State=(4, 1, 3, 0) || Done=False\n",
      "Step 20: || State=(4, 1, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(3, 1, 3, 0) || Done=False\n",
      "Step 21: || State=(3, 1, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(2, 1, 3, 0) || Done=False\n",
      "Step 22: || State=(2, 1, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(1, 1, 3, 0) || Done=False\n",
      "Step 23: || State=(1, 1, 3, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(1, 2, 3, 0) || Done=False\n",
      "Step 24: || State=(1, 2, 3, 0) || Option=NAVIGATION || Action=RIGHT || Reward=-1 || Next State=(1, 3, 3, 0) || Done=False\n",
      "Step 25: || State=(1, 3, 3, 0) || Option=NAVIGATION || Action=UP || Reward=-1 || Next State=(0, 3, 3, 0) || Done=False\n",
      "Step 26: || State=(0, 3, 3, 0) || Option=OPERATION_TRIAGE || Action=SAVE || Reward=99 || Next State=(0, 3, 3, 1) || Done=True\n",
      "Test 0: Finished after 26 steps with total reward 92 and 0 collisions at [].\n",
      "Average reward over 1 testing episodes: 92.0\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_HRL_agent(env, manager_hier_att, workers_hier_att)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
