{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this one should be used as a playground before anything changes at the HRL-LLM repository\n",
    "\n",
    "this notebook should include the following:\n",
    "- the LMM (class DisasterResponseAssistant)\n",
    "- the SARenv class for the flat RL\n",
    "- the SARenv class for the hierarchical RL\n",
    "- the Q learning agent class\n",
    "- the Q learning attention class\n",
    "- the hierarchical Q learning class\n",
    "- the hierarchical Q learning attention class\n",
    "- rewards plot (4 plots in the same figure)\n",
    "- policy evaluation\n",
    "- visitation heatmap\n",
    "\n",
    "The reason that we have two environments is because we have included the hierarchical process within the environment class and not \n",
    "within the agent class. But the functionality and parameters remain the same.\n",
    "\n",
    "TODO:\n",
    "1) each agent must evakuated across 10-20 runs and then average for rewards, policy, visitation \n",
    "2) include everything in one env class\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from termcolor import colored\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right for EXPLORE\n",
    "        self.observation_space = spaces.MultiDiscrete([7, 7, 2, 2, 2, 2])  # x, y, Info collected X, info collected Y, info_collectedZ, Victim saved\n",
    "        self.state = [4, 1, 0, 0, 0, 0]  # (x, y, info_collectedX, info_collectedY, info_collectedZ, victim_saved)\n",
    "        self.info_location = [(4, 4), (6, 2), (5, 5)]\n",
    "        self.final_location = (0, 3)\n",
    "        self.current_option = 0  # Starting option will be decided by the manager\n",
    "        self.grid_size = 7\n",
    "        self.max_steps = self.grid_size * self.grid_size\n",
    "        self.total_turns = 0\n",
    "\n",
    "        self.winReward = 100  ## 118\n",
    "        self.askingReward = 20\n",
    "        self.exceedStepPenalty = -5\n",
    "        self.turnPenalty = -1\n",
    "        self.wrongAskPenalty = -5\n",
    "        self.ditchPenalty = -30\n",
    "        self.savePenalty = -5\n",
    "\n",
    "        self.POIs, self.fires = [], []\n",
    "        self.visited_information_state = False\n",
    "\n",
    "        self.ditches = [(1, 6), (2, 2), (2, 4), (3, 2), (3, 3), (3, 4), (4, 5), \\\n",
    "                        (5, 0), (5, 1), (5, 2), (6, 0), (0, 2), (0, 4)]\n",
    "        \n",
    "        self.sensor_readings = {}\n",
    "        \n",
    "        self.keywords_for_POIs = [\"victim\", \"trail\", \"potential sighting\", \"Screams\", \"shelter\", \"high ground\", \"water source\",\n",
    "                                  \"access route\", \"last known position\", \"high probability area\", \"safe\"]\n",
    "        self.keywords_for_danger = [\"fire\", \"heat\", \"smoke\", \"restricted\", \"no access allowed\", \"flames\", \"dangerous\", \"steep terrain\",\n",
    "                                    \"dense vegetation\", \"unstable structures\", \"unstable buildings\", \"hazardous material\", \"unsafe\"]\n",
    "\n",
    "        self.locationsDict = {\n",
    "            'hospital': (0, 3),\n",
    "            'train station': (5, 6), ## (6, 5)\n",
    "            'school': (3, 0),\n",
    "            'mall': (4, 1),  # (1, 1)\n",
    "            'bank': (6, 5),\n",
    "            'restaurant': (2, 0),\n",
    "            'shop': (1, 2)\n",
    "        }\n",
    "    \n",
    "\n",
    "    ### Handles the 'ASK' action, providing the agent with environmental information\n",
    "    def ask_action(self, state):\n",
    "        x, y, info_collectedX, info_collectedY, info_collectedZ, victim_saved = state\n",
    "        verbal_inputs = []\n",
    "        if not info_collectedZ:\n",
    "            # self.last_reward_state = (position, True) # Update the last_reward_state here upon successful information retrieval\n",
    "            #print(f\"last reward state {self.last_reward_state}\")\n",
    "            VERBAL_INPUT1 = \"Hey, there's a victim at the hospital.\"\n",
    "            VERBAL_INPUT2 = \"Also, fire was reported at the train station.\"\n",
    "            VERBAL_INPUT3 = \"There is a fire at the bank.\"\n",
    "            VERBAL_INPUT4 = \"A safe area is the mall.\"\n",
    "            VERBAL_INPUT5 = \"Keep an eye on the access route in the school.\"\n",
    "            VERBAL_INPUT6 = \"Keep an eye on the access route in the restaurant.\"\n",
    "            VERBAL_INPUT7 = \"Keep an eye on the access route in the shop.\"\n",
    "            verbal_inputs.append(VERBAL_INPUT1)\n",
    "            verbal_inputs.append(VERBAL_INPUT2)\n",
    "            verbal_inputs.append(VERBAL_INPUT3)\n",
    "            verbal_inputs.append(VERBAL_INPUT4)\n",
    "            verbal_inputs.append(VERBAL_INPUT5)\n",
    "            verbal_inputs.append(VERBAL_INPUT6)\n",
    "            verbal_inputs.append(VERBAL_INPUT7)\n",
    "            for input_text in verbal_inputs:\n",
    "                    self.simulate_LLM_process_alternative(input_text)\n",
    "    \n",
    "    ### Simulates the process of obtaining information from a language model (multiple locations)\n",
    "    ### problem when two locations associated to different class are present in the smae sentence \n",
    "    def simulate_LLM_process_alternative(self, input):\n",
    "        sum_embedding = torch.tensor([0, 0], dtype=torch.float32)\n",
    "        locations_in_input = []\n",
    "        for location in self.locationsDict:\n",
    "            # Check if the location keyword is in the information string\n",
    "            if location in input:\n",
    "                location_embedding = torch.tensor(self.locationsDict[location], dtype=torch.float32)\n",
    "                sum_embedding += location_embedding\n",
    "                locations_in_input.append(tuple(int(x) for x in location_embedding.tolist()))\n",
    "        #return locations_in_input\n",
    "        if locations_in_input:\n",
    "            #print(f\"response is {locations_in_input} -- when input is: {input}\")\n",
    "            self.visited_information_state = True\n",
    "            #print(f\"response is {response}\")\n",
    "            sentences = input.split(\". \") if \". \" in input else [input]\n",
    "            #print(f\"the sentences are: {sentences}\")\n",
    "            for sentence in sentences:\n",
    "                is_poi = any(keyword in sentence for keyword in self.keywords_for_POIs)\n",
    "                is_fire = any(keyword in sentence for keyword in self.keywords_for_danger)\n",
    "                #print(f\"In sentence '{sentence}' we have POI: {is_poi} and fire: {is_fire}\")\n",
    "                for location, location_coords in self.locationsDict.items():\n",
    "                    if location in sentence:\n",
    "                        info = tuple(int(x) for x in torch.tensor(location_coords, dtype=torch.float32).tolist())\n",
    "                        #print(f\"info now is {info} and poi {is_poi} and fire {is_fire}\")\n",
    "                        # If the location is already categorized, skip it\n",
    "                        if info in self.POIs or info in self.fires:\n",
    "                            continue\n",
    "                        # Add location to POIs or fires based on the context of the sentence\n",
    "                        self.update_environment(info, is_poi, is_fire)\n",
    "    \n",
    "    def update_environment(self, info, is_poi, is_fire):\n",
    "        if is_poi and not is_fire:\n",
    "            self.sensor_readings[(info[0], info[1], 1, 1, 1, 0)] = 10.0\n",
    "            self.POIs.append(info)\n",
    "        elif is_fire and not is_poi:\n",
    "            self.sensor_readings[(info[0], info[1], 1, 1, 1, 0)] = -10.0\n",
    "            self.fires.append(info)\n",
    "    \n",
    "    def next_state_vision(self, target, action):\n",
    "        x, y, _, _, _, _ = target\n",
    "        if action == 0:  # Up\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(x + 1, self.grid_size - 1)\n",
    "        elif action == 2: # Left\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(y + 1, self.grid_size - 1)\n",
    "        next_position_x, next_position_y = x, y\n",
    "        return (next_position_x, next_position_y)\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y, info_collectedX, info_collectedY, info_collectedZ, victim_saved = self.state\n",
    "        reward = self.turnPenalty  # Small penalty for each step\n",
    "        done = False\n",
    "        self.total_turns += 1\n",
    "        if self.total_turns >= self.max_steps:\n",
    "            reward += self.exceedStepPenalty  ## (-5)\n",
    "            done = True\n",
    "            self.current_option = 0\n",
    "\n",
    "        if self.current_option == 0: ## EXPLORE\n",
    "            if (tuple([x, y]) != self.info_location[0] and not info_collectedX) or (tuple([x, y]) != self.info_location[1] and not info_collectedY and info_collectedX) or (tuple([x, y]) != self.info_location[2] and not info_collectedZ and info_collectedX and info_collectedY):  # EXPLORE (heading to info location X)\n",
    "                if action == 0:  # Up\n",
    "                    x = max(x - 1, 0)\n",
    "                elif action == 1:  # Down\n",
    "                    x = min(x + 1, self.grid_size - 1)\n",
    "                elif action == 2: # Left\n",
    "                    y = max(y - 1, 0)\n",
    "                elif action == 3:  # Right\n",
    "                    y = min(y + 1, self.grid_size - 1)\n",
    "            if (tuple([x, y]) == self.info_location[0] and not info_collectedX) or (tuple([x, y]) == self.info_location[1] and not info_collectedY and info_collectedX) or (tuple([x, y]) == self.info_location[2] and not info_collectedZ and info_collectedX and info_collectedY): ## switch to COLLECT\n",
    "                self.current_option = 1\n",
    "                \n",
    "            if tuple([x, y]) != self.final_location and info_collectedX and info_collectedY and info_collectedZ: ## EXPLORE (heading to final location)\n",
    "                if action == 0:  # Up\n",
    "                    x = max(x - 1, 0)\n",
    "                elif action == 1:  # Down\n",
    "                    x = min(x + 1, self.grid_size - 1)\n",
    "                elif action == 2: # Left\n",
    "                    y = max(y - 1, 0)\n",
    "                elif action == 3:  # Right\n",
    "                    y = min(y + 1, self.grid_size - 1)\n",
    "            if tuple([x, y]) == self.final_location and info_collectedX and info_collectedY and info_collectedZ: ## switch to SAVE\n",
    "                self.current_option = 2\n",
    "                \n",
    "            if tuple([x, y]) in self.ditches:\n",
    "                reward += self.ditchPenalty  ## (-20)\n",
    "                done = True\n",
    "                self.current_option = 0\n",
    "            \n",
    "        \n",
    "        elif self.current_option == 1:\n",
    "            if tuple([x, y]) == self.info_location[0]:\n",
    "                if action == 0:  # Assume action 0 is \"getX\"\n",
    "                    if not info_collectedX:\n",
    "                        info_collectedX = 1\n",
    "                        # reward += self.askingReward  # Reward for collecting the info for the first time\n",
    "                    else:\n",
    "                        pass\n",
    "                        # reward += self.wrongAskPenalty  # Penalty for attempting to collect again\n",
    "                    self.current_option = 0  # Switch back to EXPLORE after collection attempt\n",
    "\n",
    "            if tuple([x, y]) == self.info_location[1]:\n",
    "                if action == 1:  # Assume action 1 is \"getY\"\n",
    "                    if not info_collectedY and info_collectedX:\n",
    "                        # self.ask_action(self.state)\n",
    "                        info_collectedY = 1\n",
    "                        # reward += self.askingReward  # Reward for collecting the info for the first time\n",
    "                    else:\n",
    "                        pass\n",
    "                        # reward += self.wrongAskPenalty  # Penalty for attempting to collect again\n",
    "                    self.current_option = 0  # Switch back to EXPLORE after collection attempt\n",
    "            \n",
    "            if tuple([x, y]) == self.info_location[2]:\n",
    "                if action == 2:  # Assume action 2 is \"getZ\"\n",
    "                    if not info_collectedZ and info_collectedX and info_collectedY:\n",
    "                        self.ask_action(self.state)\n",
    "                        info_collectedZ = 1\n",
    "                        # reward += self.askingReward  # Reward for collecting the info for the first time\n",
    "                    else:\n",
    "                        pass\n",
    "                        # reward += self.wrongAskPenalty  # Penalty for attempting to collect again\n",
    "                    self.current_option = 0  # Switch back to EXPLORE after collection attempt\n",
    "            \n",
    "\n",
    "        elif self.current_option == 2:\n",
    "            if tuple([x, y]) == self.final_location:\n",
    "                if action == 0 and info_collectedX and info_collectedY and info_collectedZ:  # Assume action 0 is \"save\"\n",
    "                    if not victim_saved:\n",
    "                        victim_saved = 1\n",
    "                        reward += self.winReward  # High reward for saving the victim\n",
    "                        done = True\n",
    "                    else:\n",
    "                        reward += self.savePenalty # Penalty for redundant saving\n",
    "                else:\n",
    "                    reward += self.savePenalty  # Penalty if save action is taken without collected info or at wrong location\n",
    "\n",
    "\n",
    "        self.state = [x, y, info_collectedX, info_collectedY,info_collectedZ, victim_saved]\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_turns = 0\n",
    "        self.POIs = []\n",
    "        self.fires = []\n",
    "        self.visited_information_state = False\n",
    "        self.state = [4, 1, 0, 0, 0, 0]\n",
    "        self.current_option = 0  # Start with EXPLORE\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros((7, 7), dtype=int)\n",
    "        x, y, _, _ = self.state\n",
    "        grid[y, x] = 1\n",
    "        print(\"Current Grid:\\n\", grid)\n",
    "\n",
    "\n",
    "env = GridWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.998, epsilon=0.01, epsilon_decay=0.995, min_epsilon=0.01, decay_rate=2):\n",
    "        self.q_table = np.zeros((state_space, action_space))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def choose_action(self, state, evaluation=False):\n",
    "        state_index = self.state_to_index(state)\n",
    "        if evaluation or np.random.rand() > self.epsilon:\n",
    "            return np.argmax(self.q_table[state_index])\n",
    "        else:\n",
    "            return np.random.randint(0, self.q_table.shape[1])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state)\n",
    "        best_next_action = np.argmax(self.q_table[next_state_index])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state_index][best_next_action]\n",
    "        td_error = td_target - self.q_table[state_index][action]\n",
    "        self.q_table[state_index][action] += self.learning_rate * td_error\n",
    "        # self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        # Convert state to index for the Q-table\n",
    "        # return np.dot(state, [1, 17, 289, 578, 1156])  # Example for a hierarchical indexing (17x17)\n",
    "        return np.dot(state, [1, 7, 49, 98, 196, 392])  # Example for a hierarchical indexing\n",
    "\n",
    "    def decay_epsilon(self, episodes):\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon -= self.decay_rate / episodes\n",
    "        else:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        return self.epsilon\n",
    "\n",
    "\n",
    "def train(env, workers, num_episodes=1500):\n",
    "    total_rewards = np.zeros(num_episodes)\n",
    "    Rewards = 0\n",
    "    for episode in range(num_episodes):\n",
    "        # print(f\"Starting Episode {episode + 1}\")\n",
    "        if episode % 250 == 0:    \n",
    "            print(f\"Episode: {episode}, Total Reward: {Rewards}, Exploration Rate: {explore_worker.epsilon}\")\n",
    "\n",
    "        total_reward, Rewards = 0, 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        while not done:\n",
    "            option = env.current_option  # Get current option from the environment\n",
    "            worker = workers[option]  # Choose the correct worker for the option\n",
    "            action = worker.choose_action(state)  # Worker decides the action\n",
    "            next_state, reward, done, _ = env.step(action)  # Execute the action in the environment\n",
    "            Rewards += reward\n",
    "            # Update the worker's Q-table\n",
    "            worker.update(state, action, reward, next_state)\n",
    "            manager.update(state, option, reward, next_state)\n",
    "\n",
    "            # Print the details of the current step\n",
    "            # print(f\"  Step {step_count + 1}: State {state}, Option {option}, Action {action}, Reward {reward}, Next State {next_state}\")\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "        total_rewards[episode] = Rewards\n",
    "        manager.epsilon = manager.decay_epsilon(num_episodes)\n",
    "        explore_worker.epsilon = explore_worker.decay_epsilon(num_episodes)\n",
    "        collect_worker.epsilon = collect_worker.decay_epsilon(num_episodes)\n",
    "        save_worker.epsilon = save_worker.decay_epsilon(num_episodes)\n",
    "        # print(f\"Episode {episode + 1} finished. Total steps = {step_count}, Total Reward = {total_reward}\")\n",
    "    \n",
    "    return total_rewards, workers\n",
    "\n",
    "# Manager for choosing options\n",
    "manager = QLearningAgent(1527, 3)  # State space size 5*5*2*2, 3 options\n",
    "# Assuming QLearningAgent is defined elsewhere with methods for choose_action and update\n",
    "explore_worker = QLearningAgent(1527, 4)  # Up, down, left, right\n",
    "collect_worker = QLearningAgent(1527, 6)  # getX, getY, \n",
    "save_worker = QLearningAgent(1527, 4)  # Only save\n",
    "\n",
    "workers = {\n",
    "    0: explore_worker,  # Worker for EXPLORE\n",
    "    1: collect_worker,  # Worker for COLLECT\n",
    "    2: save_worker       # Worker for SAVE\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 0, Exploration Rate: 0.01\n",
      "Episode: 250, Total Reward: 67, Exploration Rate: 0.01\n",
      "Episode: 500, Total Reward: 75, Exploration Rate: 0.01\n",
      "Episode: 750, Total Reward: -50, Exploration Rate: 0.01\n",
      "Episode: 1000, Total Reward: 76, Exploration Rate: 0.01\n",
      "Episode: 1250, Total Reward: 76, Exploration Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "all_total_rewards_AGENT_hier = []  # List to store total rewards from each run\n",
    "for _ in range(1):\n",
    "    env = GridWorldEnv()\n",
    "    # Manager for choosing options\n",
    "    # Manager for choosing options\n",
    "    manager = QLearningAgent(1527, 3)  # State space size 5*5*2*2, 3 options\n",
    "    # Assuming QLearningAgent is defined elsewhere with methods for choose_action and update\n",
    "    explore_worker = QLearningAgent(1527, 4)  # Up, down, left, right\n",
    "    collect_worker = QLearningAgent(1527, 6)  # getX, getY, \n",
    "    save_worker = QLearningAgent(1527, 4)  # Only save\n",
    "\n",
    "    workers = {\n",
    "        0: explore_worker,  # Worker for EXPLORE\n",
    "        1: collect_worker,  # Worker for COLLECT\n",
    "        2: save_worker       # Worker for SAVE\n",
    "    }\n",
    "    hier_returns, workers_simple = train(env, workers, num_episodes=1500)\n",
    "    all_total_rewards_AGENT_hier.append(hier_returns)\n",
    "    \n",
    "avg_total_rewards_AGENT_hier = np.mean(all_total_rewards_AGENT_hier, axis=0)\n",
    "\n",
    "# hier_returns, workers_simple = train(env, workers, num_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent_attention:\n",
    "    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.998, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01, decay_rate=2):\n",
    "        self.q_table = np.zeros((state_space, action_space))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.attention_space_low = np.zeros((state_space, action_space))\n",
    "        self.received_input = False\n",
    "    \n",
    "    ### Identifies states that have changed based on sensor readings.\n",
    "    def identify_changed_states(self, readings):\n",
    "        changed_states = [i for i, value in readings.items() if value != 1]\n",
    "        return changed_states\n",
    "\n",
    "    ### Finds states connected to a given target state, considering possible actions and their inverse\n",
    "    def get_connected_states(self, target_state):\n",
    "        # Inverse action mapping for movement actions\n",
    "        inverse_actions = {0: 1, 1: 0, 2: 3, 3: 2}\n",
    "        # Generate state-action pairs\n",
    "        connected_states_pairs = []\n",
    "        for action in range(self.q_table.shape[1]):\n",
    "            inverse_action = inverse_actions[action]\n",
    "            possible_prev_state = env.next_state_vision(target_state, inverse_action)\n",
    "            # print(possible_prev_state)\n",
    "            # Check if the resulting state is valid and leads to the target state\n",
    "            if possible_prev_state != tuple([target_state[0], target_state[1]]) and possible_prev_state not in env.ditches:\n",
    "                connected_states_pairs.append((possible_prev_state, action))\n",
    "        return connected_states_pairs\n",
    "    \n",
    "    ### Updates the attention space, which influences the agent's decision-making based on environmental changes.\n",
    "    def update_attention_space(self, connection, readings):\n",
    "        connected_states = self.get_connected_states(connection)\n",
    "        # Determine the value to add based on sensor reading\n",
    "        value_to_add = 2.0 if readings[connection] > 0 else -100.0\n",
    "        for connected_state, action in connected_states:\n",
    "            # print(connected_state)\n",
    "        # Update the attention space value for the state-action pair\n",
    "            updated_state = [connected_state[0], connected_state[1], 1, 1, 1, 0]\n",
    "            updated_state_index = self.state_to_index(updated_state)\n",
    "            self.attention_space_low[updated_state_index][action] = value_to_add\n",
    "        # # Check if the new information refers to a victim state and update exclusively for SAVE action\n",
    "        if tuple([connection[0], connection[1]]) == env.final_location:\n",
    "            updated_victim_state = [connection[0], connection[1], 1, 1, 1, 0]\n",
    "            updated_victim_state_index = self.state_to_index(updated_victim_state)\n",
    "            self.attention_space_low[updated_victim_state_index][0] = 100\n",
    "            ##  action 'save' is highly favored in the Q-table when the agent is at the final location, guiding the save_worker to prioritize this action.\n",
    "        return self.attention_space_low\n",
    "    \n",
    "    def apply_attention_to_q_table(self):\n",
    "        for index, action_values in np.ndenumerate(self.attention_space_low):\n",
    "            state_index, action = index\n",
    "            if action_values != 0:\n",
    "                self.q_table[state_index][action] = action_values\n",
    "                \n",
    "    \n",
    "    def choose_action(self, state, evaluation=False):\n",
    "        state_index = self.state_to_index(state)\n",
    "        if not env.visited_information_state:\n",
    "            if evaluation or np.random.rand() > self.epsilon:\n",
    "                return np.argmax(self.q_table[state_index])\n",
    "            else:\n",
    "                return np.random.randint(0, self.q_table.shape[1])\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])\n",
    "            \n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state)\n",
    "        best_next_action = np.argmax(self.q_table[next_state_index])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state_index][best_next_action]\n",
    "        td_error = td_target - self.q_table[state_index][action]\n",
    "        self.q_table[state_index][action] += self.learning_rate * td_error\n",
    "        # self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        # Convert state to index for the Q-table\n",
    "        return np.dot(state, [1, 7, 49, 98, 196, 392])  # Example for a hierarchical indexing\n",
    "\n",
    "    def decay_epsilon(self, episodes):\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon -= self.decay_rate / episodes\n",
    "        else:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        return self.epsilon\n",
    "    \n",
    "    def decay_epsilon_exploit(self, episodes):\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon -= (8*self.decay_rate) / episodes\n",
    "        else:\n",
    "            self.epsilon = self.min_epsilon\n",
    "        return self.epsilon\n",
    "\n",
    "\n",
    "def train(env, workers, num_episodes=1500):\n",
    "    total_rewards = np.zeros(num_episodes)\n",
    "    Rewards = 0\n",
    "    attention_space = {}\n",
    "    stop_for_inspection = False  \n",
    "    for episode in range(num_episodes):\n",
    "        # print(f\"Starting Episode {episode + 1}\")\n",
    "        if episode % 250 == 0:    \n",
    "            print(f\"Episode: {episode}, Total Reward: {Rewards}, Exploration Rate: {explore_worker.epsilon}\")\n",
    "\n",
    "        total_reward, Rewards = 0, 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        while not done:\n",
    "            if env.visited_information_state and not manager.received_input:\n",
    "                identified_states = manager.identify_changed_states(env.sensor_readings)\n",
    "                manager.received_input = True\n",
    "                print(f\"Got the info needed at ep {episode} and location {tuple([state[0], state[1]])}\")\n",
    "                if identified_states:\n",
    "                    # print(f\"identified states: {identified_states}\")\n",
    "                    for each_state in identified_states:\n",
    "                        if tuple([each_state[0], each_state[1]]) != env.final_location: \n",
    "                            attention_space = explore_worker.update_attention_space(each_state, env.sensor_readings)\n",
    "                            explore_worker.apply_attention_to_q_table()  # Update the Q-table of the explorer worker\n",
    "                        else:\n",
    "                            attention_space = save_worker.update_attention_space(each_state, env.sensor_readings)\n",
    "                            save_worker.apply_attention_to_q_table()  # Update the Q-table of the save worker\n",
    "                        stop_for_inspection = True\n",
    "            # if stop_for_inspection:\n",
    "            #     print(\"Stopping the simulation to check the Q tables\")\n",
    "            #     return workers, attention_space\n",
    "                \n",
    "            option = env.current_option  # Get current option from the environment\n",
    "            worker = workers[option]  # Choose the correct worker for the option\n",
    "            action = worker.choose_action(state)  # Worker decides the action\n",
    "            next_state, reward, done, _ = env.step(action)  # Execute the action in the environment\n",
    "            Rewards += reward\n",
    "            # Update the worker's Q-table\n",
    "            worker.update(state, action, reward, next_state)\n",
    "            manager.update(state, option, reward, next_state)\n",
    "\n",
    "            # Print the details of the current step\n",
    "            # print(f\"  Step {step_count + 1}: State {state}, Option {option}, Action {action}, Reward {reward}, Next State {next_state}\")\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "        total_rewards[episode] = Rewards\n",
    "        if not env.visited_information_state:\n",
    "            for w in workers.values():\n",
    "                w.decay_epsilon(num_episodes)\n",
    "        else:\n",
    "            for w in workers.values():\n",
    "                w.decay_epsilon_exploit(num_episodes)\n",
    "        # manager.epsilon = manager.decay_epsilon(num_episodes)\n",
    "        # explore_worker.epsilon = explore_worker.decay_epsilon(num_episodes)\n",
    "        # collect_worker.epsilon = collect_worker.decay_epsilon(num_episodes)\n",
    "        # save_worker.epsilon = save_worker.decay_epsilon(num_episodes)\n",
    "        # print(f\"Episode {episode + 1} finished. Total steps = {step_count}, Total Reward = {total_reward}\")\n",
    "    \n",
    "    return total_rewards, attention_space, workers\n",
    "\n",
    "# Manager for choosing options\n",
    "manager = QLearningAgent_attention(1527, 3)  # State space size 5*5*2*2, 3 options\n",
    "# Assuming QLearningAgent is defined elsewhere with methods for choose_action and update\n",
    "explore_worker = QLearningAgent_attention(1527, 4)  # Up, down, left, right\n",
    "collect_worker = QLearningAgent_attention(1527, 6)  # getX, getY, \n",
    "save_worker = QLearningAgent_attention(1527, 4)  # Only save\n",
    "\n",
    "workers = {\n",
    "    0: explore_worker,  # Worker for EXPLORE\n",
    "    1: collect_worker,  # Worker for COLLECT\n",
    "    2: save_worker       # Worker for SAVE\n",
    "}\n",
    "\n",
    "\"\"\"This setup ensures that the attention mechanism's outputs \n",
    "are integrated into the Q-learning process, allowing the agents to adaptively focus \n",
    "on areas of interest based on new environmental information. \n",
    "Each agent's Q-table is dynamically updated based on these attention adjustments, \n",
    "and the training process reflects these changes in real-time to enhance decision-making.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_total_rewards_AGENT_hier_att = []  # List to store total rewards from each run\n",
    "for _ in range(20):\n",
    "    env = GridWorldEnv()\n",
    "    # Manager for choosing options\n",
    "    manager = QLearningAgent_attention(1527, 3)  # State space size 5*5*2*2, 3 options\n",
    "    # Assuming QLearningAgent is defined elsewhere with methods for choose_action and update\n",
    "    explore_worker = QLearningAgent_attention(1527, 4)  # Up, down, left, right\n",
    "    collect_worker = QLearningAgent_attention(1527, 6)  # getX, getY, \n",
    "    save_worker = QLearningAgent_attention(1527, 4)  # Only save\n",
    "    workers = {\n",
    "    0: explore_worker,  # Worker for EXPLORE\n",
    "    1: collect_worker,  # Worker for COLLECT\n",
    "    2: save_worker       # Worker for SAVE\n",
    "}\n",
    "    hier_att_returns, attention, workers = train(env, workers, num_episodes=1500)\n",
    "    all_total_rewards_AGENT_hier_att.append(hier_att_returns)\n",
    "    \n",
    "avg_total_rewards_AGENT_hier_att = np.mean(all_total_rewards_AGENT_hier_att, axis=0)\n",
    "\n",
    "# hier_att_returns, attention, workers = train(env, workers, num_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    for i, total_rewards in enumerate(total_rewards_list):\n",
    "        mean_rewards_1, mean_rewards_50 = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        for t in range(EPISODES):\n",
    "            mean_rewards_1[t] = np.mean(total_rewards[max(0, t-5):(t+1)])\n",
    "            mean_rewards_50[t] = np.mean(total_rewards[max(0, t-100):(t+1)])\n",
    "        ax.plot(mean_rewards_50, label=f'{labels[i]}', alpha=0.9, color=colors[i])\n",
    "        ax.fill_between(range(EPISODES), mean_rewards_1, mean_rewards_50, color=colors[i], alpha=0.15)\n",
    "        # Check for 20 consecutive iterations with reward >= optimal reward\n",
    "        for t in range(EPISODES - 10):\n",
    "            if all(total_rewards[t:t+10] >= optimal_reward):\n",
    "                # ax.axvline(x=t+10, color=colors[i], linestyle='dotted')\n",
    "                print(f\"Line appears at episode: {t+10} for agent {labels[i]}\")\n",
    "                break\n",
    "    # ax.axhline(y=optimal_reward, color='green', linestyle='--', label='Optimal Reward')\n",
    "    ax.axhline(y=optimal_reward, color='black', linestyle='--', label='Optimal Reward', linewidth=0.8)\n",
    "    ax.legend(fontsize=8, loc='best')\n",
    "    ax.grid(True, color=\"white\", linestyle='-', alpha=0.9)\n",
    "    # Define a fainter lavender by adding an alpha value\n",
    "    faint_lavender = mcolors.to_rgba('slategrey', alpha=0.2)  # Adjust alpha as needed\n",
    "    ax.set_facecolor(faint_lavender)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.xticks(np.arange(0, EPISODES, step=250))\n",
    "    # plt.yscale('symlog')  # 'symlog' handles negative values as well as positive values\n",
    "\n",
    "    plt.ylabel(\"Avg. Total Rewards\")\n",
    "    plt.title(\"SAR GridWorld (7x7) - sparse reward\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.show()\n",
    "labels = [\"Flat\", \"Flat-Attention\", \"HRL\", \"HRL-Attention\"]\n",
    "colors = [\"blue\", \"chocolate\", \"magenta\", \"green\"]\n",
    "total_rewards_list = [avg_total_rewards_AGENT_flat, avg_total_rewards_AGENT_flat_att, avg_total_rewards_AGENT_hier, avg_total_rewards_AGENT_hier_att]#, avg_total_rewards_AGENT_1_ATTENTION]\n",
    "plot_learning_curve(total_rewards_list, 1500, labels, colors, optimal_reward=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    for i, total_rewards in enumerate(total_rewards_list):\n",
    "        mean_rewards_1, mean_rewards_50 = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        max_rewards, min_rewards = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        for t in range(EPISODES):\n",
    "            mean_rewards_50[t] = np.mean(total_rewards[max(0, t-100):(t+1)])\n",
    "            mean_rewards_1[t] = np.mean(total_rewards[max(0, t-100):(t+1)])\n",
    "            max_rewards[t] = np.max(total_rewards[max(0, t-100):(t+1)])  # Adjust the range as needed\n",
    "            min_rewards[t] = np.min(total_rewards[max(0, t-100):(t+1)])  # Adjust the range as needed\n",
    "        ax.plot(mean_rewards_50, label=f'{labels[i]}', alpha=0.9, color=colors[i])\n",
    "        ax.fill_between(range(EPISODES), min_rewards, max_rewards, color=colors[i], alpha=0.15)\n",
    "\n",
    "    ax.axhline(y=optimal_reward, color='black', linestyle='--', label='Optimal Reward', linewidth=0.8)\n",
    "    ax.legend(fontsize=7, loc='best')\n",
    "    ax.grid(True, color=\"white\", linestyle='-', alpha=0.9)\n",
    "    faint_lavender = mcolors.to_rgba('slategrey', alpha=0.2)\n",
    "    ax.set_facecolor(faint_lavender)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.xticks(np.arange(0, EPISODES, step=250))\n",
    "    plt.ylabel(\"Avg. Total Rewards\")\n",
    "    plt.title(\"SAR GridWorld (7x7) - sparse reward\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "EPISODES = 1500\n",
    "labels = [\"Flat\", \"Flat-Attention\", \"HRL\", \"HRL-Attention\"]\n",
    "colors = [\"blue\", \"chocolate\", \"magenta\", \"green\"]\n",
    "total_rewards_list = [avg_total_rewards_AGENT_flat, avg_total_rewards_AGENT_flat_att, avg_total_rewards_AGENT_hier, avg_total_rewards_AGENT_hier_att]#, avg_total_rewards_AGENT_1_ATTENTION]\n",
    "plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward=92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(hier_returns))\n",
    "print(np.mean(hier_att_returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(avg_total_rewards_AGENT_hier))\n",
    "print(np.mean(avg_total_rewards_AGENT_hier_att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(avg_total_rewards_AGENT_flat))\n",
    "print(np.mean(avg_total_rewards_AGENT_flat_att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    for i, total_rewards in enumerate(total_rewards_list):\n",
    "        mean_rewards_1, mean_rewards_50 = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        max_rewards, min_rewards = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        for t in range(EPISODES):\n",
    "            mean_rewards_50[t] = np.mean(total_rewards[max(0, t-50):(t+1)])\n",
    "            mean_rewards_1[t] = np.mean(total_rewards[max(0, t-50):(t+1)])\n",
    "            max_rewards[t] = np.max(total_rewards[max(0, t-10):(t+1)])  # Adjust the range as needed\n",
    "            min_rewards[t] = np.min(total_rewards[max(0, t-10):(t+1)])  # Adjust the range as needed\n",
    "        ax.plot(mean_rewards_50, label=f'{labels[i]}', alpha=0.9, color=colors[i])\n",
    "        ax.fill_between(range(EPISODES), min_rewards, max_rewards, color=colors[i], alpha=0.15)\n",
    "\n",
    "    ax.axhline(y=optimal_reward, color='black', linestyle='--', label='Optimal Reward', linewidth=0.8)\n",
    "    ax.legend(fontsize=7, loc='best')\n",
    "    ax.grid(True, color=\"white\", linestyle='-', alpha=0.9)\n",
    "    faint_lavender = mcolors.to_rgba('slategrey', alpha=0.2)\n",
    "    ax.set_facecolor(faint_lavender)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.xticks(np.arange(0, EPISODES, step=250))\n",
    "    plt.ylabel(\"Avg. Total Rewards\")\n",
    "    plt.title(\"SAR GridWorld (7x7) - sparse reward\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "EPISODES = 1500\n",
    "labels = [\"HRL\"]\n",
    "colors = [\"magenta\"]\n",
    "total_rewards_list = [avg_total_rewards_AGENT_hier]#, avg_total_rewards_AGENT_1_ATTENTION]\n",
    "plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward=76)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, manager, workers, num_trials=1):\n",
    "    for trial in range(num_trials):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        collisions = []\n",
    "        total_reward, cnt, steps = 0, 0, 0\n",
    "        print(f\"Trial {trial + 1} starting:\")\n",
    "        \n",
    "        while not done:\n",
    "            # env.render()\n",
    "            # Manager decides the option based on the current state\n",
    "            option = env.current_option #= manager.choose_action(state, evaluation=True)\n",
    "            # Select the appropriate worker\n",
    "            if option == 0:\n",
    "                worker = explore_worker\n",
    "            elif option == 1:\n",
    "                worker = collect_worker\n",
    "            elif option == 2:\n",
    "                worker = save_worker\n",
    "            else:\n",
    "                raise ValueError(\"Invalid option\")\n",
    "            \n",
    "            # Worker takes the best known action\n",
    "            action = worker.choose_action(state, evaluation=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            print(f\"  State: {state}, Option: {option}, Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if tuple([state[0], state[1]]) in env.fires:\n",
    "                cnt += 1\n",
    "                collisions.append(tuple([state[0], state[1]]))\n",
    "            steps += 1\n",
    "        \n",
    "        print(f\"Trial {trial + 1} finished after {steps} with total reward: {total_reward} and {cnt} collisions at {collisions}\")\n",
    "\n",
    "# Evaluate the learned policy\n",
    "evaluate(env, manager, {\n",
    "    'explore': explore_worker,\n",
    "    'collect': collect_worker,\n",
    "    'save': save_worker\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, workers, num_episodes=1):\n",
    "    total_rewards = 0  # To accumulate rewards across all episodes\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0  # To accumulate rewards for the current episode\n",
    "\n",
    "        while not done:\n",
    "            current_option = env.current_option  # Get current option from the environment\n",
    "            \n",
    "            # Choose the best action from the current worker's Q-table\n",
    "            worker = workers[current_option]\n",
    "            action = worker.choose_action(state, evaluation=True)\n",
    "            \n",
    "            # Execute the action in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Log the details of the current step\n",
    "            print(f\"Episode {episode + 1}, Step: State {state}, Option {current_option}, Action {action}, Reward {reward}, Next State {next_state}\")\n",
    "            \n",
    "            # Update state and accumulate reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        print(f\"Episode {episode + 1} finished with Total Reward: {episode_reward}\")\n",
    "        total_rewards += episode_reward\n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    print(f\"Average Reward per Episode: {average_reward}\")\n",
    "    return average_reward\n",
    "evaluate_policy(env, workers, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "# from utils_functions import get_file_type\n",
    "\n",
    "### class that processes verbal inputs handle disaster-related verbal inputs, analyze them using RAG architecture, and generate a \n",
    "# response in a specified format. It leverages models like ChatOllama and techniques like vector storage and retrieval for its operations.\n",
    "class DisasterResponseAssistant:\n",
    "    def __init__(self, data_path, data_type, model_name=\"mistral\", embedding_model='nomic-embed-text', collection_name=\"rag-chroma\"):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = embedding_model\n",
    "        self.collection_name = collection_name\n",
    "        self.data_path = data_path\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        self.llm = None\n",
    "        self.loader = None\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "        self._load_model()            # Initializes an instance of the ChatOllama model    \n",
    "        self._load_documents()        # Loads and splits the PDF document into chunks\n",
    "        self._create_vectorstore()    # Creates a vector store using Chroma from the document splits\n",
    "        self._create_retriever()      # Creates a retriever from the vector store\n",
    "        \n",
    "        self.hazard_coordinates = []  # To store hazard coordinates\n",
    "        self.poi_coordinates = []     # To store points of interest coordinates\n",
    "    \n",
    "    def _load_model(self):\n",
    "        self.llm = ChatOllama(model=self.model_name)\n",
    "        \n",
    "\n",
    "    def _load_documents(self): ## for json documents\n",
    "        print(f\"document {self.data_type} will be infused\")\n",
    "        if self.data_type == 'pdf':\n",
    "            self.loader = PyPDFLoader(self.data_path)\n",
    "            self.data = self.loader.load_and_split()\n",
    "        elif self.data_type == 'json':\n",
    "            self.loader = JSONLoader(\n",
    "                file_path=self.data_path,\n",
    "                jq_schema='.',\n",
    "                text_content=False)\n",
    "            self.data = self.loader.load()\n",
    "            #pprint(self.data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported document type. Please choose either 'pdf' or 'json'.\")\n",
    "\n",
    "\n",
    "    def _create_vectorstore(self): ## for json documents\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.data,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=embeddings.OllamaEmbeddings(model=self.embedding_model),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def _create_retriever(self):\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "\n",
    "    ### generate a response based on a verbal input\n",
    "    ### construct a template for the response using RAG architecture\n",
    "    def generate_response(self, verbal_input):\n",
    "        prompt_template = \"\"\"You are an assistant, who carefully listens to verbal inputs: {verbal_input} and specialized in analyzing disaster-related inputs. Your task is \n",
    "to identify physical locations mentioned in the text and classify them as either points of interest (POI) or as hazards/dangers (HAZARD) for rescue operations. Use the\n",
    "information provided in the documents: {context}, such as KEYWORDS, descriptions and context when locations are mentioned, to make your classification.\n",
    "Output the classification in the form of a JSON array dictionary with keys 'location', 'coordinates', and 'category'. Here are some rules you always follow:\n",
    "- Focus strictly on physical locations. Avoid including entities that do not represent physical, geographical places (such as individuals, conditions, or \n",
    "  abstract concepts).\n",
    "- Generate human-readable output in the specified dictionary format.\n",
    "- Generate only the requested output, strictly following the dictionary structure.\n",
    "- Within the dictionary, the value of the `category` key must be either 'POI' or 'HAZARD'. \n",
    "- Never generate offensive or foul language.\n",
    "- Never give explanations over your output.\n",
    "Input: {verbal_input}\n",
    "\"\"\"\n",
    "        system_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "        output_parser = StrOutputParser()\n",
    "        after_rag_chain = (\n",
    "            {\"context\": self.retriever, \"verbal_input\": RunnablePassthrough()}\n",
    "            | system_template\n",
    "            | self.llm  # Assuming model_local is defined elsewhere and accessible\n",
    "            | output_parser\n",
    "        )\n",
    "        response = after_rag_chain.invoke(verbal_input)\n",
    "        return response\n",
    "    \n",
    "    def refine_response(self, output):\n",
    "        cleaned_output_str = output.strip().replace('\\n', '').replace('(', '[').replace(')', ']')\n",
    "        output_dict = json.loads(cleaned_output_str)\n",
    "\n",
    "        for item in output_dict:\n",
    "            coord = tuple(item['coordinates'])\n",
    "            if item['category'] == 'HAZARD':\n",
    "                self.hazard_coordinates.append(coord)\n",
    "            else:\n",
    "                self.poi_coordinates.append(coord)\n",
    "                    \n",
    "        print(\"Hazardous Coordinates:\", self.hazard_coordinates)\n",
    "        print(\"Point of Interest Coordinates:\", self.poi_coordinates)\n",
    "        return self.hazard_coordinates, self.poi_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARenvHRL:\n",
    "    def __init__(self, gridsize, startState, finalState, ditches, fires, POIs, infoLocation, image_path, document_path, mode='prod'):\n",
    "        self.gridsize = gridsize\n",
    "        self.startState = startState\n",
    "        self.finalState = finalState\n",
    "        self.ditches = ditches\n",
    "        self.infoLocation = infoLocation\n",
    "        self.fires = fires\n",
    "        self.POIs = POIs\n",
    "        \n",
    "        self.maxSteps = 2 * (self.gridsize[0] * self.gridsize[1])  # max steps needed for NAVIGATION -- EXPLORE\n",
    "        self.ditchPenalty = -20\n",
    "        self.turnPenalty = -1\n",
    "        self.savePenalty = -10\n",
    "        self.exceedStepPenalty = -1\n",
    "        self.wrongCollectPenalty = -4\n",
    "        self.askingReward = 10  ##4\n",
    "        self.winReward = 100   ## 108\n",
    "    \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.desired_cell_size = 50  # The size of cells in the grid for visualization\n",
    "        self.generate_annotated_image(image_path)\n",
    "        \n",
    "        self.actionspace = {'EXPLORE': ['up', 'down', 'left', 'right'], \n",
    "                     'COLLECT': ['get X', 'get Y', 'get Z', 'get A', 'get B', 'get C'],\n",
    "                     'SAVE': ['save', 'use', 'remove', 'carry']}\n",
    "        self.optionspace = ['EXPLORE', 'COLLECT', 'SAVE']\n",
    "\n",
    "        \n",
    "        self.visited_information_state = False\n",
    "        self.isGameEnd = False\n",
    "        self.info_collectedX = False\n",
    "        self.info_collectedY = False\n",
    "        self.info_collectedZ = False\n",
    "        self.option = self.optionspace[0]\n",
    "        self.totalTurns = 0\n",
    "        self.totalAsks = 0\n",
    "        self.totalSaves = 0\n",
    "        self.victim_saved = False\n",
    "        self.correct_sequence = ['X', 'Y']\n",
    "        \n",
    "        self.collected_info = []\n",
    "        self.ask_action_counter = 0\n",
    "        \n",
    "        # self.document_type = self.get_file_type(document_path)\n",
    "        # self.assistant = DisasterResponseAssistant(document_path, self.document_type)\n",
    "        self.ask_action_counter = 0\n",
    "        self.hazards = []\n",
    "        self.pois = []\n",
    "        self.sensor_readings = {}\n",
    "        \n",
    "    \n",
    "        self.current_state = (self.startState[0], self.info_collectedX, self.info_collectedY, self.info_collectedZ, self.victim_saved)\n",
    "        if self.mode == 'debug':\n",
    "            print(\"Initialization complete.\")\n",
    "        \n",
    "        \n",
    "        self.keywords_for_POIs = [\"victim\", \"trail\", \"potential sighting\", \"Screams\", \"shelter\", \"high ground\", \"water source\",\n",
    "                                  \"access route\", \"last known position\", \"high probability area\", \"safe\"]\n",
    "        self.keywords_for_danger = [\"fire\", \"heat\", \"smoke\", \"restricted\", \"no access allowed\", \"flames\", \"dangerous\", \"steep terrain\",\n",
    "                                    \"dense vegetation\", \"unstable structures\", \"unstable buildings\", \"hazardous material\", \"unsafe\"]\n",
    "\n",
    "        self.locationsDict = {\n",
    "            'hospital': (5, 0),\n",
    "            'train station': (5, 7), ## (6, 5)\n",
    "            'school': (3, 0),\n",
    "            'mall': (5, 2),  # (1, 1)\n",
    "            'bank': (10, 0),\n",
    "            'restaurant': (8, 6),\n",
    "            'shop': (6, 5)\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        self.already_asked_for_X = False\n",
    "        self.already_asked_for_Y = False\n",
    "        self.correct_sequence = ['X', 'Y']\n",
    "        self.visited_information_state = False\n",
    "        self.collected_info = []\n",
    "        self.info_collectedX, self.info_collectedY, self.info_collectedZ = False, False, False\n",
    "        self.victim_saved = False\n",
    "        self.isGameEnd = False\n",
    "        self.totalTurns = 0\n",
    "        self.totalAsks = 0\n",
    "        self.totalSaves = 0\n",
    "        self.position = self.startState[0]\n",
    "        self.option = self.optionspace[0]\n",
    "        self.current_state = (self.position, self.info_collectedX, self.info_collectedY, self.info_collectedZ, self.victim_saved)\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_file_type(self, docs_path):\n",
    "        # Split the path and get the extension\n",
    "        _, file_extension = os.path.splitext(docs_path)\n",
    "        # Return the file extension without the period\n",
    "        return file_extension[1:] if file_extension else None\n",
    "\n",
    "    ##### ----------------------------\n",
    "    \n",
    "    def next_state_vision(self, target_state, action):  ### here just movement, thus just EXPLORE option\n",
    "        position = target_state\n",
    "        next_position = position\n",
    "        # Movement actions update\n",
    "        if action == self.actionspace[self.optionspace[0]][0] and position[0] > 0:  # up\n",
    "            next_position = (position[0] - 1, position[1])\n",
    "        elif action == self.actionspace[self.optionspace[0]][1] and position[0] < self.gridsize[0] - 1:  # down\n",
    "            next_position = (position[0] + 1, position[1])\n",
    "        elif action == self.actionspace[self.optionspace[0]][2] and position[1] > 0:  # left\n",
    "            next_position = (position[0], position[1] - 1)\n",
    "        elif action == self.actionspace[self.optionspace[0]][3] and position[1] < self.gridsize[1] - 1:  # right\n",
    "            next_position = (position[0], position[1] + 1)\n",
    "        return (next_position)\n",
    "    \n",
    "    \n",
    "    def ask_action(self, state):\n",
    "        self.ask_action_counter += 1\n",
    "        position, info_collectedX, info_collectedY, info_collectedZ, victim_saved = state\n",
    "        verbal_inputs = []\n",
    "        if not info_collectedY:\n",
    "            \n",
    "            # VERBAL_INPUT1 = \"Hey, there's a victim at the hospital.\"\n",
    "            # VERBAL_INPUT2 = \"Watch out, fire was reported next to the mall.\"#. There's a shelter through the bank and the train station.\"\n",
    "            # #VERBAL_INPUT3 = \"Someone told me about Screams heard at the shop and close to restaurant.\"\n",
    "            # verbal_inputs.append(VERBAL_INPUT1)\n",
    "            # verbal_inputs.append(VERBAL_INPUT2)\n",
    "            # #verbal_inputs.append(VERBAL_INPUT3)\n",
    "            \n",
    "            VERBAL_INPUT1 = \"Hey, there's a victim at the hospital.\"\n",
    "            VERBAL_INPUT2 = \"Also, fire was reported at the train station.\"\n",
    "            VERBAL_INPUT3 = \"There is a fire at the bank.\"\n",
    "            VERBAL_INPUT4 = \"A safe area is the mall.\"\n",
    "            VERBAL_INPUT5 = \"Keep an eye on the access route in the school.\"\n",
    "            VERBAL_INPUT6 = \"Keep an eye on the access route in the restaurant.\"\n",
    "            VERBAL_INPUT7 = \"Keep an eye on the access route in the shop.\"\n",
    "            verbal_inputs.append(VERBAL_INPUT1)\n",
    "            verbal_inputs.append(VERBAL_INPUT2)\n",
    "            verbal_inputs.append(VERBAL_INPUT3)\n",
    "            verbal_inputs.append(VERBAL_INPUT4)\n",
    "            verbal_inputs.append(VERBAL_INPUT5)\n",
    "            verbal_inputs.append(VERBAL_INPUT6)\n",
    "            verbal_inputs.append(VERBAL_INPUT7)\n",
    "            for input_text in verbal_inputs:\n",
    "                    self.simulate_LLM_process_alternative(input_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "    #         VERBAL_INPUT1 = \"Hey, there's a victim at the hospital and I think I also saw fire in the train station and the bank. Hey, you wait! Someone told me about screams heard at the school and close to the mall. Hurry!\"\n",
    "    #         # VERBAL_INPUT1 = \"Hey, there's a victim at the hospital and I think I also saw fire in the train station.\" #and the bank. Hey, you wait! Someone told me about screams heard at the school and close to the mall. Hurry!\"\n",
    "    #         verbal_inputs.append(VERBAL_INPUT1)\n",
    "    #         if self.ask_action_counter <= 1:\n",
    "    #             print(f\"real LLM is about to start handling the input {VERBAL_INPUT1}\")\n",
    "    #             for input_text in verbal_inputs:\n",
    "    #                 response = self.assistant.generate_response(input_text)\n",
    "    #                 if response:\n",
    "    #                     self.visited_information_state = True\n",
    "    #                 self.hazards, self.pois = self.assistant.refine_response(response)\n",
    "    #                 print(f\"real LLM is about to end handling the input {VERBAL_INPUT1}\")\n",
    "    #                 self.update_environment_REAL(self.hazards, self.pois)\n",
    "    #         else:\n",
    "    #             #print(f\"input will be handled hereby by pseudoLLM\")\n",
    "    #             #print(self.hazards, self.pois)\n",
    "    #             self.visited_information_state = True\n",
    "    #             self.update_environment_REAL(self.hazards, self.pois)\n",
    "    #             # for input_text in verbal_inputs:\n",
    "    #             #     self.simulate_LLM_process_alternative(input_text)\n",
    "    \n",
    "    # def update_environment_REAL(self, haz, poi):\n",
    "    #     for hazardous_location in haz:\n",
    "    #         self.sensor_readings[(hazardous_location, True, False, False, False)] = -10.0\n",
    "    #         #self.sensor_readings[(hazardous_location, True)] = -10.0\n",
    "    #         self.fires.append(hazardous_location)\n",
    "    #     for safe_location in poi:\n",
    "    #         self.sensor_readings[(safe_location, True, False, False, False)] = 10.0\n",
    "    #         #self.sensor_readings[(safe_location, True)] = 10.0\n",
    "    #         self.POIs.append(safe_location)\n",
    "        \n",
    "    \n",
    "    ### Simulates the process of obtaining information from a language model (multiple locations)\n",
    "    ### problem when two locations associated to different class are present in the smae sentence \n",
    "    def simulate_LLM_process_alternative(self, input):\n",
    "        sum_embedding = torch.tensor([0, 0], dtype=torch.float32)\n",
    "        locations_in_input = []\n",
    "        for location in self.locationsDict:\n",
    "            # Check if the location keyword is in the information string\n",
    "            if location in input:\n",
    "                location_embedding = torch.tensor(self.locationsDict[location], dtype=torch.float32)\n",
    "                sum_embedding += location_embedding\n",
    "                locations_in_input.append(tuple(int(x) for x in location_embedding.tolist()))\n",
    "        #return locations_in_input\n",
    "        if locations_in_input:\n",
    "            #print(f\"response is {locations_in_input} -- when input is: {input}\")\n",
    "            self.visited_information_state = True\n",
    "            #print(f\"response is {response}\")\n",
    "            sentences = input.split(\". \") if \". \" in input else [input]\n",
    "            #print(f\"the sentences are: {sentences}\")\n",
    "            for sentence in sentences:\n",
    "                is_poi = any(keyword in sentence for keyword in self.keywords_for_POIs)\n",
    "                is_fire = any(keyword in sentence for keyword in self.keywords_for_danger)\n",
    "                #print(f\"In sentence '{sentence}' we have POI: {is_poi} and fire: {is_fire}\")\n",
    "                for location, location_coords in self.locationsDict.items():\n",
    "                    if location in sentence:\n",
    "                        info = tuple(int(x) for x in torch.tensor(location_coords, dtype=torch.float32).tolist())\n",
    "                        #print(f\"info now is {info} and poi {is_poi} and fire {is_fire}\")\n",
    "                        # If the location is already categorized, skip it\n",
    "                        if info in self.POIs or info in self.fires:\n",
    "                            continue\n",
    "                        # Add location to POIs or fires based on the context of the sentence\n",
    "                        self.update_environment(info, is_poi, is_fire)\n",
    "    \n",
    "    def update_environment(self, info, is_poi, is_fire):\n",
    "        if is_poi and not is_fire:\n",
    "            self.sensor_readings[(info, True, True, False, False)] = 10.0\n",
    "            self.POIs.append(info)\n",
    "        elif is_fire and not is_poi:\n",
    "            self.sensor_readings[(info, True, True, False, False)] = -10.0\n",
    "            self.fires.append(info)                   \n",
    "    \n",
    "    ##### does not take into account the order of the info collection (WORKS)\n",
    "    def step(self, action, option):\n",
    "        position, info_collectedX, info_collectedY, info_collectedZ, victim_saved = self.current_state  # Unpack the current state        \n",
    "        option_terminated = False\n",
    "        fell_into_ditch = False\n",
    "        next_position = position\n",
    "        \n",
    "    \n",
    "        if option == self.optionspace[0]:  ### EXPLORE\n",
    "            if action in self.actionspace[self.optionspace[0]]:   ### ['up', 'down', 'left', 'right']\n",
    "                self.totalTurns += 1\n",
    "                # Movement actions update\n",
    "                if action == self.actionspace[self.optionspace[0]][0] and position[0] > 0:  # up\n",
    "                    next_position = (position[0] - 1, position[1])\n",
    "                elif action == self.actionspace[self.optionspace[0]][1] and position[0] < self.gridsize[0] - 1:  # down\n",
    "                    next_position = (position[0] + 1, position[1])\n",
    "                elif action == self.actionspace[self.optionspace[0]][2] and position[1] > 0:  # left\n",
    "                    next_position = (position[0], position[1] - 1)\n",
    "                elif action == self.actionspace[self.optionspace[0]][3] and position[1] < self.gridsize[1] - 1:  # right\n",
    "                    next_position = (position[0], position[1] + 1)\n",
    "            # Check if information is collected or if the agent fell into a ditch\n",
    "            if (next_position == self.infoLocation[0] and not info_collectedX) or \\\n",
    "                (next_position == self.infoLocation[1] and not info_collectedY and info_collectedX) or \\\n",
    "                (next_position == self.infoLocation[2] and not info_collectedZ and info_collectedX and info_collectedY):\n",
    "                    option_terminated = True\n",
    "                    \n",
    "            \n",
    "            if next_position == self.finalState[0] and info_collectedX and info_collectedY and not victim_saved:\n",
    "                option_terminated = True\n",
    "                \n",
    "            \n",
    "            if self.totalTurns > self.maxSteps or next_position in self.ditches:\n",
    "                option_terminated = True\n",
    "                fell_into_ditch = next_position in self.ditches  # Agent fell into a ditch if true\n",
    "                \n",
    "\n",
    "        \n",
    "        elif option == self.optionspace[1]:  ### COLLECT\n",
    "            # Ask actions update\n",
    "            if action in self.actionspace[self.optionspace[1]]:  ### ['get X', 'get Y', 'get Z']\n",
    "                # Ask actions update\n",
    "                self.totalAsks += 1\n",
    "                info_type = action.split(' ')[1]  # Extract the type of information being collected ('X', 'Y', 'Z')\n",
    "                if position == self.infoLocation[0] and info_type == 'X':\n",
    "                    # self.ask_action(self.current_state)\n",
    "                    info_collectedX = True\n",
    "                    self.collected_info.append(info_type)\n",
    "                    option_terminated = True\n",
    "                elif position == self.infoLocation[1] and info_type == 'Y':\n",
    "                    self.ask_action(self.current_state)\n",
    "                    info_collectedY = True\n",
    "                    self.collected_info.append(info_type)\n",
    "                    option_terminated = True\n",
    "                elif position == self.infoLocation[2] and info_type == 'Z':\n",
    "                    info_collectedZ = True\n",
    "                    self.collected_info.append(info_type)\n",
    "                    option_terminated = True\n",
    "                else:\n",
    "                    # If the agent is not at the correct location or the info is out of sequence,\n",
    "                    # the option is terminated.\n",
    "                    option_terminated = True\n",
    "                    \n",
    "        elif option == self.optionspace[2]:   ### SAVE \n",
    "            if action in self.actionspace[self.optionspace[2]]: ### ['save']\n",
    "                if action == 'save' and position == self.finalState[0] and info_collectedX and info_collectedY:\n",
    "                    # Assuming saving is only contingent on collecting information X for simplicity\n",
    "                    victim_saved = True\n",
    "                    option_terminated = True\n",
    "                else:\n",
    "                    #reward = self.wrongCollectPenalty  # Or another penalty as appropriate\n",
    "                    option_terminated = True\n",
    "        \n",
    "        \n",
    "        # Check if all information has been collected in the correct sequence\n",
    "        all_info_collected_in_correct_order = self.collected_info == self.correct_sequence\n",
    "        #done = next_position == self.finalState[0] and all_info_collected_in_correct_order\n",
    "        done = next_position == self.finalState[0] and victim_saved and all_info_collected_in_correct_order\n",
    "        reward = self.calculate_reward(position, action, next_position, done, option, fell_into_ditch, info_collectedX, info_collectedY, info_collectedZ, victim_saved)\n",
    "        ditch_event = fell_into_ditch  # Set ditch_event based on whether the agent fell into a ditch\n",
    "        # Reset the agent's position to the start state if it fell into a ditch\n",
    "        if fell_into_ditch:\n",
    "            next_position = self.startState[0]  ## an xtupisw empodio to option kanei terminate alla oxi to epeisodio\n",
    "        \n",
    "        self.current_state = (next_position, info_collectedX, info_collectedY, info_collectedZ, victim_saved)\n",
    "        return self.current_state, reward, done, option_terminated, ditch_event\n",
    "    \n",
    "\n",
    "    def calculate_reward(self, position, action, next_position, done, option, fell_into_ditch, info_collectedX, info_collectedY, info_collectedZ, victim_saved):\n",
    "        reward = 0  # Base reward\n",
    "        # Penalize for moving, encouraging efficient pathfinding\n",
    "        reward += self.turnPenalty\n",
    "        # Penalize for falling into a ditch\n",
    "        if fell_into_ditch:\n",
    "            reward += self.ditchPenalty\n",
    "            return reward  # Early return since the episode restarts after falling into a ditch\n",
    "        # Reward for collecting information in correct sequence\n",
    "        if option == 'COLLECT':\n",
    "            if info_collectedX and self.is_sequence_correct() and not self.already_asked_for_X:\n",
    "                reward += self.askingReward  # Reward for collecting X\n",
    "                self.already_asked_for_X = True\n",
    "            # elif next_position == self.infoLocation[0] and info_collectedX:\n",
    "            #     reward += self.wrongCollectPenalty\n",
    "            # --------\n",
    "            if info_collectedY and self.is_sequence_correct() and not self.already_asked_for_Y:\n",
    "                reward += self.askingReward  # Reward for collecting Y in sequence after X\n",
    "                self.already_asked_for_Y = True\n",
    "            # --------\n",
    "\n",
    "            # if self.already_asked_for_X or se\n",
    "        \n",
    "        # elif next_position == self.infoLocation[1] and info_collectedY:\n",
    "        #     reward += self.wrongCollectPenalty  # Penalize for collecting out of order or at the wrong location\n",
    "        # Reward for completing the episode successfully\n",
    "        if done:\n",
    "            reward += self.winReward  # Big reward for saving the victim after collecting all required info\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    # ## this calculate reward function works with the deterministically select option method in the agents' classes\n",
    "    # def calculate_reward(self, position, action, next_position, done, current_option, fell_into_ditch, info_collectedX, info_collectedY, info_collectedZ, victim_saved):\n",
    "    #     if fell_into_ditch:\n",
    "    #         return self.ditchPenalty\n",
    "    #     elif done:\n",
    "    #         return self.winReward\n",
    "    #     elif self.totalTurns > self.maxSteps:\n",
    "    #         return self.exceedStepPenalty\n",
    "    #     elif current_option == self.optionspace[1] and not self.is_sequence_correct():\n",
    "    #         return self.wrongCollectPenalty  # Assume you have defined this penalty value in the initializer\n",
    "        \n",
    "\n",
    "    #     # elif current_option == self.optionspace[1]:\n",
    "    #     #     if (position == self.infoLocation[0] and info_collectedX):\n",
    "    #     #         return self.askingReward  # Reward for correctly collecting X at location 0\n",
    "    #     #     elif (position == self.infoLocation[1] and info_collectedY):\n",
    "    #     #         return self.askingReward  # Reward for correctly collecting Y at location 1\n",
    "    #     #     else:\n",
    "    #     #         return self.wrongCollectPenalty\n",
    "\n",
    "    #     else:\n",
    "    #         return self.turnPenalty\n",
    "    \n",
    "    def is_sequence_correct(self):\n",
    "        return self.collected_info == self.correct_sequence[:len(self.collected_info)]\n",
    "\n",
    "    \n",
    "    # def calculate_discrepancy(self, collected_info):\n",
    "    #     # Initialize the discrepancy score\n",
    "    #     discrepancy_score = 0\n",
    "    #     # Create a mapping of element to its index in the optimal_sequence for order comparison\n",
    "    #     optimal_indexes = {element: index for index, element in enumerate(list(self.ListB.keys()))}\n",
    "    #     # Check for missing elements and add their weights to the discrepancy score\n",
    "    #     for element in list(self.ListB.keys()):\n",
    "    #         if element not in collected_info:\n",
    "    #             discrepancy_score += self.ListB[element]\n",
    "    #     # Check the order for elements in the estimated_sequence\n",
    "    #     last_index = -1  # Initialize with an index that's before the first element's index\n",
    "    #     for element in collected_info:\n",
    "    #         if element in optimal_indexes:\n",
    "    #             current_index = optimal_indexes[element]\n",
    "    #             # If the current element is out of order, add its weight to the discrepancy score\n",
    "    #             if current_index < last_index:\n",
    "    #                 discrepancy_score += self.ListB[element]\n",
    "    #             else:\n",
    "    #                 # Update last_index to the current element's index if it's in correct order\n",
    "    #                 last_index = current_index\n",
    "    #     #print(discrepancy_score)\n",
    "    #     return discrepancy_score\n",
    "    \n",
    "    \n",
    "\n",
    "    ### Methods for annotating and generating images of the environment grid\n",
    "    def annotate_cell(self, ax, row, col, text, color='white'):\n",
    "        \"\"\" Annotates a specific cell in the grid with given text and color. \"\"\"\n",
    "        ax.text(col * self.desired_cell_size + self.desired_cell_size/2,\n",
    "                row * self.desired_cell_size + self.desired_cell_size/2,\n",
    "                text,\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                color=color,\n",
    "                fontsize=15,\n",
    "                weight='bold')\n",
    "    \n",
    "    \n",
    "    def generate_annotated_image(self, image_path):\n",
    "        # Load and process the image\n",
    "        img = Image.open(image_path)\n",
    "        # Resize and crop image to fit the grid\n",
    "        aspect_ratio = img.width / img.height\n",
    "        if aspect_ratio > 1:\n",
    "            new_size = (self.gridsize[0] * img.height // self.gridsize[1], img.height)\n",
    "        else:\n",
    "            new_size = (img.width, self.gridsize[1] * img.width // self.gridsize[0])\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        crop_x = (img.width - self.gridsize[0] * (img.height // self.gridsize[1])) // 2\n",
    "        crop_y = (img.height - self.gridsize[1] * (img.width // self.gridsize[0])) // 2\n",
    "        img = img.crop((crop_x, crop_y, img.width - crop_x, img.height - crop_y))\n",
    "        # Resize image to have equal cells\n",
    "        new_image_width = self.desired_cell_size * self.gridsize[0]\n",
    "        new_image_height = self.desired_cell_size * self.gridsize[1]\n",
    "        img = img.resize((new_image_width, new_image_height))\n",
    "        # Create figure with annotations\n",
    "        if not self.gridsize == [17, 17]:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(15, 15))\n",
    "        ax.imshow(img)\n",
    "        # Draw grid lines in white\n",
    "        ax.set_xticks([i * self.desired_cell_size for i in range(self.gridsize[0] + 1)], minor=False)\n",
    "        ax.set_yticks([i * self.desired_cell_size for i in range(self.gridsize[1] + 1)], minor=False)\n",
    "        ax.grid(which=\"both\", color=\"white\", linestyle='-', linewidth=2)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Add annotations for each cell\n",
    "        for row in range(self.gridsize[1]):\n",
    "            for col in range(self.gridsize[0]):\n",
    "                cell_coord = (row, col)\n",
    "                # Check for special states and annotate accordingly\n",
    "                if cell_coord in self.startState:\n",
    "                    self.annotate_cell(ax, row, col, 'START', color='coral')  # Start\n",
    "                elif cell_coord in self.finalState:\n",
    "                    self.annotate_cell(ax, row, col, 'VIC', color='cyan')  # Terminal\n",
    "                elif cell_coord in self.ditches:\n",
    "                    self.annotate_cell(ax, row, col, 'D', color='red')  # Ditch\n",
    "                elif cell_coord in self.fires:\n",
    "                    self.annotate_cell(ax, row, col, 'F', color='orange')  # Fire\n",
    "                elif cell_coord in self.POIs:\n",
    "                    self.annotate_cell(ax, row, col, 'P', color='purple')  # POI\n",
    "                elif cell_coord in self.infoLocation:\n",
    "                    self.annotate_cell(ax, row, col, 'INFO', color='yellow')  # Info\n",
    "                else:\n",
    "                    self.annotate_cell(ax, row, col, str(cell_coord))  # Regular cell coordinate\n",
    "        # Save figure to buffer and load as PIL image\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        self.annotated_image = Image.open(buf)\n",
    "    \n",
    "    ### Visualizes the current state of the environment.\n",
    "    ### Visualizes the current state of the environment.\n",
    "    def render(self):\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))#plt.subplots()\n",
    "        ax.set_xlim(0, self.gridsize[0])\n",
    "        ax.set_ylim(0, self.gridsize[1])\n",
    "        ax.set_xticks(range(self.gridsize[0]))\n",
    "        ax.set_yticks(range(self.gridsize[1]))\n",
    "        ax.grid(which='both')\n",
    "        # Plotting the agent\n",
    "        #agent_pos, _, _ = self.currentState\n",
    "        agent_pos = self.position\n",
    "        ax.plot(agent_pos[1] + 0.5, agent_pos[0] + 0.5, 'o', color='blue', markersize=10)  # Agent as a blue dot\n",
    "        # Plotting the ditches\n",
    "        for ditch in self.ditches:\n",
    "            ax.plot(ditch[1] + 0.5, ditch[0] + 0.5, 'x', color='red', markersize=10)\n",
    "        # Plotting the terminal state (victim's location)\n",
    "        for terminal in self.finalState:\n",
    "            ax.plot(terminal[1] + 0.5, terminal[0] + 0.5, 'P', color='green', markersize=10)\n",
    "        for poi in self.POIs:\n",
    "            ax.plot(poi[1] + 0.5, poi[0] + 0.5, 'P', color='pink', markersize=10)\n",
    "        for hazard in self.fires:\n",
    "            ax.plot(hazard[1] + 0.5, hazard[0] + 0.5, 'x', color='orange', markersize=10)\n",
    "        # Plotting the info location\n",
    "        for info in self.infoLocation:\n",
    "            ax.plot(info[1] + 0.5, info[0] + 0.5, '*', color='yellow', markersize=10)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initialization and training\n",
    "gridsize = [17, 17] #[14, 14]\n",
    "startState = [(0, 16)]\n",
    "victimStates = [(5, 0)]\n",
    "ditches = [(2, 7), (3, 7), (4, 7), (6, 7), (7, 7), (9, 7), (11, 7), (2, 11), (3, 11), (4, 11), (7, 15), (6 ,16), (7, 16), (8, 4), (7, 4),\n",
    "           (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (13, 11), (12, 11), (12, 13), (13, 13), (13, 1), (13 ,2), (14, 1), (15, 1), (4, 3),\n",
    "           (13, 7), (14, 8), (14, 9)]\n",
    "fires = []\n",
    "POIs = []  # Victim locations\n",
    "infoLocation = [(10, 15), (7, 10), (16, 0)]  # Location to ask for information\n",
    "image_path = \"/Users/dimipan/Documents/HRL-LLM/images/Cyclone_Seroj.jpg\"\n",
    "document_path = '/Users/dimipan/Documents/HRL-LLM/data/sar_data_COMPLEX.json'\n",
    "\n",
    "env = SARenvHRL(gridsize, startState, victimStates, ditches, fires, POIs, infoLocation, image_path, document_path, mode='debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initialization and training\n",
    "gridsize = [7, 7]\n",
    "startState = [(3, 0)]\n",
    "victimStates = [(0, 3)]\n",
    "ditches = [(1, 6), (2, 1), (2, 2), (2, 4), (3, 2), (3, 3), (3, 4), (4, 5),\n",
    "         (5, 0), (5, 1), (5, 2), (6, 0), (0, 2), (0, 4), (5, 5)]\n",
    "\n",
    "#ditches = []\n",
    "fires = []\n",
    "POIs = []  # Victim locations\n",
    "#infoLocation = [(6, 1), (4, 4), (2, 6)]  # Location to ask for information was (6, 1)\n",
    "#infoLocation = {(6, 1): 'X', (4, 4): 'Y', (2, 6): 'Z'}\n",
    "infoLocation = [(4, 4), (4, 1), (2, 6)]\n",
    "fires = []\n",
    "POIs = []\n",
    "\n",
    "image_path = \"/Users/dimipan/Documents/HRL-LLM/images/disaster_area.jpg\"\n",
    "document_path = \"/Users/dimipan/Documents/HRL-LLM/data/sar_data.json\"\n",
    "# image_path = \"/home/dimiubuntu/Desktop/local_code_scripts/enhanced_RL/enhanced_RL/images/disaster_area.jpg\"\n",
    "# document_path = \"/home/dimiubuntu/Desktop/local_code_scripts/enhanced_RL/enhanced_RL/data/sar_data.json\"\n",
    "\n",
    "\n",
    "env = SARenvHRL(gridsize, startState, victimStates, ditches, fires, POIs, infoLocation, image_path, document_path, mode='debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalQLearningAgent:\n",
    "    def __init__(self, environment, actions, alpha, gamma):\n",
    "        self.actions = actions\n",
    "        self.env = environment\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.high_level_q_table = {}  # Maps state -> option -> Q-value\n",
    "        self.low_level_q_table = {}  # Maps (state, option, action) -> Q-value\n",
    "        \n",
    "    def _get_high_q_value(self, state, option):\n",
    "        return self.high_level_q_table.get((state, option), 0)\n",
    "\n",
    "    def _get_low_q_value(self, state, action):\n",
    "        return self.low_level_q_table.get((state, action), 0)\n",
    "\n",
    "\n",
    "    # def select_option(self, state, exploration_rate):\n",
    "    #     position, info1, info2, info_3, victim = state\n",
    "    #     if not info1:\n",
    "    #         if position == self.env.infoLocation[0]:\n",
    "    #             return 'COLLECT'\n",
    "    #         else:\n",
    "    #             return 'EXPLORE'\n",
    "    #     elif not info2 and info1:\n",
    "    #         if position == self.env.infoLocation[1]:\n",
    "    #             return 'COLLECT'\n",
    "    #         else:\n",
    "    #             return 'EXPLORE'\n",
    "    #     elif info1 and info2 and not victim:\n",
    "    #         if position == self.env.finalState[0]:\n",
    "    #             return 'SAVE'\n",
    "    #         else:\n",
    "    #             return 'EXPLORE'\n",
    "    #     # else:\n",
    "    #     #     return 'EXPLORE'\n",
    "        \n",
    "        \n",
    "    def select_option(self, state, exploration_rate):\n",
    "        if random.random() < exploration_rate:\n",
    "            return random.choice(list(self.actions.keys()))\n",
    "        else:\n",
    "            q_values = {option: self._get_high_q_value(state, option) for option in self.actions.keys()}\n",
    "            return max(q_values, key=q_values.get)\n",
    "\n",
    "    def select_action(self, state, option, exploration_rate):\n",
    "        if random.random() < exploration_rate:\n",
    "            return random.choice(self.actions[option])\n",
    "        else:\n",
    "            q_values = {(state, option, action): self._get_low_q_value((state, option), action) for action in self.actions[option]}\n",
    "            return max(q_values, key=q_values.get)[2]\n",
    "\n",
    "    def update(self, state, option, action, reward, next_state, done):\n",
    "        # Update low-level Q-value\n",
    "        current_q = self._get_low_q_value((state, option), action)\n",
    "        next_max_q = max(self._get_low_q_value((next_state, option), a) for a in self.actions[option]) if not done else 0\n",
    "        self.low_level_q_table[((state, option), action)] = current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)\n",
    "\n",
    "        # Update high-level Q-value\n",
    "        current_option_q = self._get_high_q_value(state, option)\n",
    "        next_max_option_q = max(self._get_high_q_value(next_state, o) for o in self.actions.keys()) if not done else 0\n",
    "        self.high_level_q_table[(state, option)] = current_option_q + self.alpha * (reward + self.gamma * next_max_option_q - current_option_q)\n",
    "\n",
    "    def execute_option(self, initial_state, option, exploration_rate, visit_counts):\n",
    "        state = initial_state\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        option_terminated = False ##\n",
    "        self.env.totalTurns = 0\n",
    "        self.env.totalAsks = 0\n",
    "        self.env.totalSaves = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = self.select_action(state, option, exploration_rate)\n",
    "            next_state, reward, done, option_terminated, ditch_event = self.env.step(action, option)\n",
    "            if ditch_event:\n",
    "                # Apply the penalty for the ditch event but reset the state for the next iteration\n",
    "                self.update(state, option, action, reward, next_state, done)\n",
    "                #print(f\"O: {option} | {state} | a: {action} | s_: {next_state} | r: {reward}\")\n",
    "                state = self.env.reset()  # Reset to start state after applying penalty\n",
    "            else:\n",
    "                # Regular update when no ditch event\n",
    "                self.update(state, option, action, reward, next_state, done)\n",
    "                #print(f\"O: {option} | {state} | a: {action} | s_: {next_state} | r: {reward}\")\n",
    "                state = next_state\n",
    "            \n",
    "            total_reward += reward\n",
    "            if state[0] in visit_counts:\n",
    "                visit_counts[state[0]] += 1\n",
    "            else:\n",
    "                visit_counts[state[0]] = 1\n",
    "                \n",
    "            \n",
    "            if option_terminated or done:\n",
    "                break\n",
    "        \n",
    "        return state, total_reward, done, visit_counts\n",
    "    \n",
    "    \n",
    "    # Training Loop\n",
    "    def train_agent(self, episodes):\n",
    "        exploration_rate = 1.0\n",
    "        min_exploration_rate = 0.01\n",
    "        exploration_decay = 2\n",
    "        rewards = []\n",
    "        total_reward, Rewards, mean_reward = 0, 0, 0\n",
    "        total = np.zeros(episodes)\n",
    "        max_epochs_per_option = 20\n",
    "        option_epoch = 0\n",
    "        visit_counts = {}\n",
    "\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            if episode % 250 == 0:\n",
    "                print(f\"Episode: {episode}, Total Reward: {total_reward}, Exploration Rate: {exploration_rate}\")\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            Rewards = 0\n",
    "            mean_reward = 0\n",
    "            done = False\n",
    "            option_terminated = False\n",
    "            option_epoch = 0\n",
    "\n",
    "            while not done and option_epoch < max_epochs_per_option:\n",
    "            \n",
    "                option = self.select_option(state, exploration_rate)\n",
    "                #print(f\"\\n----- option selected: {option} at episode: {episode} | ep_step: {option_epoch}\")\n",
    "                state, reward, done, visit_counts = self.execute_option(state, option, exploration_rate, visit_counts)\n",
    "                total_reward += reward\n",
    "                Rewards += reward\n",
    "                option_epoch += 1\n",
    "\n",
    "            # Decay exploration rate\n",
    "            #exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "            \n",
    "            if exploration_rate > 0.1:\n",
    "                exploration_rate -= exploration_decay/episodes\n",
    "            else:\n",
    "                exploration_rate = min_exploration_rate\n",
    "            \n",
    "            rewards.append(total_reward)  ## total_reward\n",
    "            total[episode] = Rewards  ## Rewards\n",
    "\n",
    "        return rewards, total, visit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalQLearningAgent_ATTENTION:\n",
    "    def __init__(self, environment, actions, alpha, gamma, image_path):\n",
    "        self.actions = actions\n",
    "        self.env = environment\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.image_path = image_path\n",
    "        self.high_level_q_table = {}  # Maps state -> option -> Q-value\n",
    "        self.low_level_q_table = {}  # Maps (state, option, action) -> Q-value\n",
    "        self.exploit_start_episode = None\n",
    "        self.exploit_end_episode = None\n",
    "        self.exploit_episodes = 500  # 30% of the episodes\n",
    "        self.exploit_initiated = False\n",
    "        self.exploit_mode = False\n",
    "        \n",
    "        self.attention_space_HIGH = {}\n",
    "        self.attention_space_LOW = {}\n",
    "    \n",
    "    def identify_changes_states(self, readings):\n",
    "        updated_locations = [updated_loc for updated_loc, sensor_value in readings.items() if readings]\n",
    "        return updated_locations\n",
    "            \n",
    "    ### Finds states connected to a given target state, considering possible actions and their inverse\n",
    "    def get_connected_states(self, target_state):\n",
    "        ### {'up': 'down', 'down': 'up', 'left': 'right', 'right': 'left'} -- Inverse action mapping for movement actions\n",
    "        inverse_actions = {env.actionspace[env.optionspace[0]][0]: env.actionspace[env.optionspace[0]][1],\n",
    "                        env.actionspace[env.optionspace[0]][1]: env.actionspace[env.optionspace[0]][0],\n",
    "                        env.actionspace[env.optionspace[0]][2]: env.actionspace[env.optionspace[0]][3],\n",
    "                        env.actionspace[env.optionspace[0]][3]: env.actionspace[env.optionspace[0]][2]}\n",
    "        connected_states_pairs = []   # Generate state-action pairs\n",
    "        for action in self.actions[self.env.optionspace[0]]:   ## we just care about the navigation connections\n",
    "            inverse_action = inverse_actions[action] # Apply the inverse action to the target state\n",
    "            possible_prev_state = self.env.next_state_vision(target_state[0], inverse_action)\n",
    "            #print(f\"poss prev: {possible_prev_state}\")\n",
    "            # Check if the resulting state is valid and leads to the target state\n",
    "            if possible_prev_state != target_state[0] and possible_prev_state not in self.env.ditches:\n",
    "                connected_states_pairs.append((possible_prev_state, action))\n",
    "        #print(f\"out from get_connected: {connected_states_pairs}\")\n",
    "        return connected_states_pairs\n",
    "\n",
    "    ### Updates the attention space, which influences the agent's decision-making based on environmental changes. ### more like policy-shaping\n",
    "    def update_attention_space(self, connection, sensor_readings):\n",
    "        connected_states = self.get_connected_states(connection)\n",
    "        print(connected_states)\n",
    "        # Determine the value to add based on sensor reading\n",
    "        value_to_add = 10.0 if sensor_readings[connection] > 0 else -10.0\n",
    "        for connected_state, action in connected_states:    # Update the attention space value for the state-action pair\n",
    "            ## here we update the attention space low that directly affects the low-level Q table (low level action selection)\n",
    "            self.attention_space_LOW[(((connected_state, True, True, False, False), self.env.optionspace[0]), action)] = value_to_add  ### (((connected_state), True, False, False), 'EXPLORE'), 'action'): value_to_add\n",
    "            ## here we update the attention space high that directly affects the high-level Q table (high level action selection -- option selection)\n",
    "            if value_to_add > 0: # For positive sensor readings, enhance the value for 'EXPLORE' option\n",
    "                self.attention_space_HIGH[(((connected_state), True, True, False, False), self.env.optionspace[0])] = value_to_add ### (((connected_state), False, False, False), 'option'): value\n",
    "            else: # For negative sensor readings, decrease the value for options other than 'EXPLORE'# For negative sensor readings, decrease the value for options other than 'EXPLORE'\n",
    "                self.attention_space_HIGH[(((connected_state), True, True, False, False), self.env.optionspace[1])] = value_to_add\n",
    "                self.attention_space_HIGH[(((connected_state), True, True, False, False), self.env.optionspace[2])] = value_to_add\n",
    "        # Check if the new information refers to the victim state and update exclusively for SAVE option and 'save' action under SAVE option\n",
    "        if connection[0] in self.env.finalState:\n",
    "            victim_state_option_pair = (((connection[0], True, True, False, False), self.env.optionspace[2]))\n",
    "            self.attention_space_HIGH[victim_state_option_pair] = 100\n",
    "\n",
    "            victim_state_action_pair = (((connection[0], True, True, False, False), \\\n",
    "                                        self.env.optionspace[2], self.env.actionspace[self.env.optionspace[2]][0]))\n",
    "            self.attention_space_LOW[victim_state_action_pair] = 100\n",
    "            \n",
    "            ## # Positive value adds to 'EXPLORE', negative to 'ASK' (and could be expanded to others)\n",
    "            #self.attention_space_HIGH[((connected_state, True, False, False), 'EXPLORE' if value_to_add > 0 else 'ASK')] = value_to_add\n",
    "        return self.attention_space_HIGH, self.attention_space_LOW\n",
    "\n",
    "\n",
    "    def update_q_tables_from_attention(self):\n",
    "        if self.attention_space_HIGH:\n",
    "            for key, value in self.attention_space_HIGH.items():\n",
    "                if key in self.high_level_q_table:\n",
    "                    self.high_level_q_table[key] = self.high_level_q_table[key] + value\n",
    "                else:\n",
    "                    self.high_level_q_table[key] = value\n",
    "                ###here it works\n",
    "        if self.attention_space_LOW:\n",
    "            for key, value in self.attention_space_LOW.items():\n",
    "                if key in self.low_level_q_table:\n",
    "                    self.low_level_q_table[key] = self.low_level_q_table[key] + value\n",
    "                else:\n",
    "                    self.low_level_q_table[key] = value\n",
    "        \n",
    "        \n",
    "    def _get_high_q_value(self, state, option):\n",
    "        return self.high_level_q_table.get((state, option), 0)\n",
    "\n",
    "    def _get_low_q_value(self, state, action):\n",
    "        return self.low_level_q_table.get((state, action), 0)\n",
    "    \n",
    "\n",
    "    # def select_option(self, state, exploration_rate):\n",
    "    #     position, info1, info2, info_3, victim = state\n",
    "    #     if not info1:\n",
    "    #         if position == self.env.infoLocation[0]:\n",
    "    #             return 'COLLECT'\n",
    "    #         else:\n",
    "    #             return 'EXPLORE'\n",
    "    #     elif not info2 and info1:\n",
    "    #         if position == self.env.infoLocation[1]:\n",
    "    #             return 'COLLECT'\n",
    "    #         else:\n",
    "    #             return 'EXPLORE'\n",
    "    #     elif info1 and info2 and not victim:\n",
    "    #         if position == self.env.finalState[0]:\n",
    "    #             return 'SAVE'\n",
    "    #         else:\n",
    "    #             return 'EXPLORE'\n",
    "\n",
    "    def select_option(self, state, exploration_rate):\n",
    "        if not self.env.visited_information_state:\n",
    "            if random.random() < exploration_rate:\n",
    "                return random.choice(list(self.actions.keys()))\n",
    "            else:\n",
    "                q_values = {option: self._get_high_q_value(state, option) for option in self.actions.keys()}\n",
    "                return max(q_values, key=q_values.get)\n",
    "        else:\n",
    "            # if random.random() < 0.01:\n",
    "            #     return random.choice(list(self.actions.keys()))\n",
    "            # else:\n",
    "                q_values = {option: self._get_high_q_value(state, option) for option in self.actions.keys()}\n",
    "                return max(q_values, key=q_values.get)\n",
    "\n",
    "    def select_action(self, state, option, exploration_rate):\n",
    "        if not self.env.visited_information_state:\n",
    "            if random.random() < exploration_rate:\n",
    "                return random.choice(self.actions[option])\n",
    "            else:\n",
    "                q_values = {(state, option, action): self._get_low_q_value((state, option), action) for action in self.actions[option]}\n",
    "                return max(q_values, key=q_values.get)[2]\n",
    "        else:\n",
    "            # if random.random() < 0.001:\n",
    "            #     return random.choice(self.actions[option])\n",
    "            # else:\n",
    "                q_values = {(state, option, action): self._get_low_q_value((state, option), action) for action in self.actions[option]}\n",
    "                return max(q_values, key=q_values.get)[2]\n",
    "\n",
    "    def update(self, state, option, action, reward, next_state, done):\n",
    "        # Update low-level Q-value\n",
    "        current_q = self._get_low_q_value((state, option), action)\n",
    "        next_max_q = max(self._get_low_q_value((next_state, option), a) for a in self.actions[option]) if not done else 0\n",
    "        self.low_level_q_table[((state, option), action)] = current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)\n",
    "\n",
    "        # Update high-level Q-value\n",
    "        current_option_q = self._get_high_q_value(state, option)\n",
    "        next_max_option_q = max(self._get_high_q_value(next_state, o) for o in self.actions.keys()) if not done else 0\n",
    "        self.high_level_q_table[(state, option)] = current_option_q + self.alpha * (reward + self.gamma * next_max_option_q - current_option_q)\n",
    "\n",
    "    def execute_option(self, initial_state, option, exploration_rate, visit_counts):\n",
    "        state = initial_state\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        option_terminated = False ##\n",
    "        self.env.totalTurns = 0\n",
    "        self.env.totalAsks = 0\n",
    "        self.env.totalSaves = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = self.select_action(state, option, exploration_rate)\n",
    "            next_state, reward, done, option_terminated, ditch_event = self.env.step(action, option)\n",
    "            if ditch_event:\n",
    "                # Apply the penalty for the ditch event but reset the state for the next iteration\n",
    "                self.update(state, option, action, reward, next_state, done)\n",
    "                # print(f\"O: {option} | {state} | a: {action} | s_: {next_state} | r: {reward}\")\n",
    "                state = self.env.reset()  # Reset to start state after applying penalty\n",
    "            else:\n",
    "                # Regular update when no ditch event\n",
    "                self.update(state, option, action, reward, next_state, done)\n",
    "                # print(f\"O: {option} | {state} | a: {action} | s_: {next_state} | r: {reward}\")\n",
    "                state = next_state\n",
    "            \n",
    "            total_reward += reward\n",
    "            if state[0] in visit_counts:\n",
    "                visit_counts[state[0]] += 1\n",
    "            else:\n",
    "                visit_counts[state[0]] = 1\n",
    "                \n",
    "            \n",
    "            if option_terminated or done:\n",
    "                break\n",
    "\n",
    "        return state, total_reward, done, visit_counts\n",
    "    \n",
    "    ### Manages the transition between exploration and exploitation modes based on information received\n",
    "    def exploitation_strategy(self, received, current_episode):\n",
    "        if received and not self.exploit_mode and not self.exploit_initiated:\n",
    "            self.exploit_start_episode = current_episode\n",
    "            self.exploit_end_episode = self.exploit_start_episode + self.exploit_episodes\n",
    "            self.exploit_mode = True\n",
    "            self.exploit_initiated = True\n",
    "            print(f\"Exploitation mode started at episode {self.exploit_start_episode}\")\n",
    "    \n",
    "    \n",
    "    # Training Loop\n",
    "    def train_agent(self, episodes):\n",
    "        exploration_rate = 1.0\n",
    "        min_exploration_rate = 0.01\n",
    "        exploration_decay = 2\n",
    "        rewards = []\n",
    "        total_reward, Rewards = 0, 0\n",
    "        total = np.zeros(episodes)\n",
    "        max_epochs_per_option = 20\n",
    "        option_epoch = 0\n",
    "        visit_counts = {}\n",
    "        received_input = False\n",
    "        attention_space_OPTIONS, attention_space_PRIMITIVES = {}, {}\n",
    "\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            if episode % 250 == 0:\n",
    "                print(f\"Episode: {episode}, Total Reward: {total_reward}, Exploration Rate: {exploration_rate}\")\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            Rewards = 0\n",
    "            done = False\n",
    "            option_epoch = 0\n",
    "\n",
    "            while not done and option_epoch < max_epochs_per_option:\n",
    "                \n",
    "                if self.env.visited_information_state and not received_input:\n",
    "                    identified_states = self.identify_changes_states(self.env.sensor_readings)\n",
    "                    received_input = True\n",
    "                    self.env.generate_annotated_image(self.image_path) \n",
    "                    print(f\"got the info needed at ep {episode} at location {state[0]}\")\n",
    "                    if identified_states:\n",
    "                        for informed_state in identified_states:\n",
    "                            attention_space_OPTIONS, attention_space_PRIMITIVES = self.update_attention_space(informed_state, self.env.sensor_readings)\n",
    "                    self.update_q_tables_from_attention()\n",
    "                            \n",
    "                \n",
    "                option = self.select_option(state, exploration_rate)\n",
    "                #print(f\"\\n----- option selected: {option} at episode: {episode} | ep_step: {option_steps}\")\n",
    "                state, reward, done, visit_counts = self.execute_option(state, option, exploration_rate, visit_counts)\n",
    "                total_reward += reward\n",
    "                Rewards += reward\n",
    "                option_epoch += 1\n",
    "\n",
    "            # Decay exploration rate\n",
    "            #exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "            if not self.env.visited_information_state:\n",
    "                if exploration_rate > 0.1:\n",
    "                        exploration_rate -= exploration_decay/episodes\n",
    "                else:\n",
    "                    exploration_rate = min_exploration_rate\n",
    "            else:\n",
    "                if exploration_rate > 0.1:\n",
    "                    exploration_rate -= 8*exploration_decay/episodes\n",
    "                else:\n",
    "                    exploration_rate = min_exploration_rate\n",
    "            \n",
    "            # ### for now verbal input is considered truthful --- see (5b) for explanations\n",
    "            # self.exploitation_strategy(received_input, episode)\n",
    "            # if self.exploit_mode and self.exploit_start_episode <= episode < self.exploit_end_episode:\n",
    "            #     if exploration_rate > 0.1:\n",
    "            #         exploration_rate -= exploration_decay/episodes\n",
    "            #     else:\n",
    "            #         exploration_rate = min_exploration_rate\n",
    "            #     if episode == self.exploit_end_episode - 1:\n",
    "            #         self.exploit_mode = False\n",
    "            #         print(f\"Exploitation mode ends at episode {episode}\")\n",
    "            # else:\n",
    "            #     if exploration_rate > 0.1:\n",
    "            #         exploration_rate -= 4*exploration_decay/episodes\n",
    "            #     else:\n",
    "            #         exploration_rate = min_exploration_rate\n",
    "            \n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            total[episode] = Rewards\n",
    "        # print(f\"POIs identified during training {self.env.POIs}\")\n",
    "        # print(f\"fires identified during training {self.env.fires}\")\n",
    "\n",
    "        return rewards, total, visit_counts, attention_space_OPTIONS, attention_space_PRIMITIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "GAMMA = 0.998\n",
    "EPISODES = 5000\n",
    "agent_simple = HierarchicalQLearningAgent(env, env.actionspace, ALPHA, GAMMA)\n",
    "rewards_Q, total_Q, visits_Q = agent_simple.train_agent(EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### agent's hyperparameters\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.998\n",
    "EPISODES = 5000\n",
    "agent_attention = HierarchicalQLearningAgent_ATTENTION(env, env.actionspace, ALPHA, GAMMA, image_path)\n",
    "rewards_ATT, total_ATT, visits_ATT, att_optios_ATT, att_prim_ATT = agent_attention.train_agent(EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards1, rewards2, rewards3, rewards4):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(rewards1, label='Rewards per Episode')\n",
    "    plt.plot(rewards2, label='Rewards per Episode')\n",
    "    plt.plot(rewards3, label='Rewards per Episode')\n",
    "    plt.plot(rewards4, label='Rewards per Episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards_Q, rewards_ATT, total_returns0, total_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    for i, total_rewards in enumerate(total_rewards_list):\n",
    "        mean_rewards_1, mean_rewards_50 = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        for t in range(EPISODES):\n",
    "            mean_rewards_1[t] = np.mean(total_rewards[max(0, t-5):(t+1)])\n",
    "            mean_rewards_50[t] = np.mean(total_rewards[max(0, t-150):(t+1)])\n",
    "        ax.plot(mean_rewards_50, label=f'{labels[i]}', alpha=0.9, color=colors[i])\n",
    "        ax.fill_between(range(EPISODES), mean_rewards_1, mean_rewards_50, color=colors[i], alpha=0.15)\n",
    "        # Check for 20 consecutive iterations with reward >= optimal reward\n",
    "        for t in range(EPISODES - 10):\n",
    "            if all(total_rewards[t:t+10] >= optimal_reward):\n",
    "                ax.axvline(x=t+10, color=colors[i], linestyle='dotted')\n",
    "                print(f\"Line appears at episode: {t+10} for agent {labels[i]}\")\n",
    "                break\n",
    "    ax.axhline(y=optimal_reward, color='green', linestyle='--', label='Optimal Reward')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.4)  # Add this line to enable gridlines\n",
    "    # ax.set_ylim([80, 110])  # Set y-axis limits\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.xticks(np.arange(0, EPISODES, step=250))\n",
    "    # plt.yscale('symlog')  # 'symlog' handles negative values as well as positive values\n",
    "\n",
    "    plt.ylabel(\"Total Rewards\")\n",
    "    plt.title(\"Mean Total Rewards Comparison\")\n",
    "    plt.show()\n",
    "labels = [\"hierQ\", \"hierQ+\", \"Q\", \"Q+\"]\n",
    "colors = [\"blue\", \"magenta\", \"orange\", \"red\"]\n",
    "total_rewards_list = [total_Q, total_ATT, total_returns0, total_returns]#, avg_total_rewards_AGENT_1_ATTENTION]\n",
    "plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializes the SAR environment. It sets up the grid, start state, victim locations, hazards (ditches and fires),\n",
    "points of interest (POIs), information location, and various rewards and penalties. It also prepares visualization tools.\n",
    "\"\"\"\n",
    "\n",
    "class SARenv:\n",
    "    def __init__(self, gridsize, startState, victimStates, ditches, fires, POIs, infoLocation, image_path, mode='prod'):\n",
    "        self.gridsize = gridsize              # size of the grid environment\n",
    "        self.startState = startState          # The starting state of the agent\n",
    "        self.victimStates = victimStates      # The locations where victims are present.\n",
    "        self.ditches = ditches                # The locations representing obstacles (ditches) -- can't go through\n",
    "        self.fires = fires                    # The locations representing fires.\n",
    "        self.POIs = POIs                      #  Points of interest in the environment.\n",
    "        self.infoLocation = infoLocation  # Location where the agent needs to ask for information\n",
    "        self.maxSteps = 1 * (self.gridsize[0] * self.gridsize[1])  # Example step limit\n",
    "        self.ditchPenalty = -30\n",
    "        self.savePenalty = -5\n",
    "        self.turnPenalty = -1\n",
    "        self.askingReward = 6\n",
    "        self.wrongAskPenalty = -5\n",
    "        self.exceedStepPenalty = -5\n",
    "        self.winReward = 100\n",
    "        self.mode = mode\n",
    "        \n",
    "        # self.docs_path = \"/home/research100/Documents/sample/enhanced_RL/enhanced_RL/data/rag_pseudo.pdf\"\n",
    "        #self.assistant = DisasterResponseAssistant(self.docs_path)\n",
    "        self.ask_action_counter = 0\n",
    "        self.hazards = []\n",
    "        self.pois = []\n",
    "        # Define the correct order of information collection\n",
    "        self.referenceSequence = ['X', 'Y', 'Z']\n",
    "        # Extend the initialization method to include the new info locations\n",
    "        self.infoLocations = {\n",
    "            (6, 1): 'X',\n",
    "            (4, 4): 'Y',\n",
    "            (2, 6): 'Z'\n",
    "        }\n",
    "        # Initialize a list to keep track of the types of information collected\n",
    "        self.infoCollected = []\n",
    "\n",
    "\n",
    "\n",
    "        self.desired_cell_size = 50  # The size of cells in the grid for visualization\n",
    "        self.generate_annotated_image(image_path)\n",
    "\n",
    "        self.create_statespace()\n",
    "        self.create_statespace_VISUALISATION()\n",
    "        self.stateCount = self.get_statespace_len()\n",
    "        self.stateDict = {k: v for k, v in zip(self.statespace, range(self.stateCount))}\n",
    "        self.currentState = (self.startState[0], False, False, False)  # State includes position and info statusX, and info statusY\n",
    "        self.actionspace = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]  # Actions: 'UP', 'DOWN', 'LEFT', 'RIGHT', 'ASK', 'SAVE'\n",
    "        self.actionDict = {0: 'UP', 1: 'DOWN', 2: 'LEFT', 3: 'RIGHT', 4: 'ASK', 5: 'SAVE', 6: 'ASK_Y', 7: 'ASK_Z', \n",
    "                           8: 'ASK_A', 9: 'ASK_B', 10: 'ASK_C', 11: 'use', 12: 'remove', 13: 'carry'}\n",
    "        self.actionCount = self.get_actionspace_len()\n",
    "        \n",
    "\n",
    "        self.last_reward_state = None    # Attribute to remember the last reward state\n",
    "        \n",
    "        self.victim_saved = False\n",
    "        self.isGameEnd = False\n",
    "        self.visited_information_state = False\n",
    "        self.response = None\n",
    "        self.totalTurns = 0\n",
    "        self.sensor_readings = {state: 1.0 for state in self.get_statespace()}\n",
    "\n",
    "        self.keywords_for_POIs = [\"victim\", \"trail\", \"potential sighting\", \"Screams\", \"shelter\", \"high ground\", \"water source\",\n",
    "                                  \"access route\", \"last known position\", \"high probability area\", \"safe\"]\n",
    "        self.keywords_for_danger = [\"fire\", \"heat\", \"smoke\", \"restricted\", \"no access allowed\", \"flames\", \"dangerous\", \"steep terrain\",\n",
    "                                    \"dense vegetation\", \"unstable structures\", \"unstable buildings\", \"hazardous material\", \"unsafe\"]\n",
    "\n",
    "        self.locationsDict = {\n",
    "            'hospital': (0, 3),\n",
    "            'train station': (5, 6), ## (6, 5)\n",
    "            'school': (3, 0),\n",
    "            'mall': (4, 1),  # (1, 1)\n",
    "            'bank': (6, 5),\n",
    "            'restaurant': (2, 0),\n",
    "            'shop': (1, 2),\n",
    "            'bakery': (3, 6),\n",
    "            'petrol station': (2, 5)\n",
    "        }\n",
    "\n",
    "        if self.mode == 'debug':\n",
    "            print(\"State Space\", self.statespace)\n",
    "            print(\"State Dict\", self.stateDict)\n",
    "            print(\"Action Space\", self.actionspace)\n",
    "            print(\"Action Dict\", self.actionDict)\n",
    "            print(\"Start State\", self.startState)\n",
    "            print(\"Terminal States\", self.victimStates)\n",
    "            print(\"Ditches\", self.ditches)\n",
    "            print(\"WinReward:{}, TurnPenalty:{}, DitchPenalty:{}, savePenalty:{}, AskingReward:{}, \\\n",
    "                WrongAskPenalty:{}\".format(self.winReward, self.turnPenalty, self.ditchPenalty, \\\n",
    "                    self.savePenalty, self.askingReward, self.wrongAskPenalty))\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "    def create_statespace(self):\n",
    "        self.statespace = [((row, col), info_statusX, info_statusY, info_statusZ)\n",
    "                           for row in range(self.gridsize[0])\n",
    "                           for col in range(self.gridsize[1])\n",
    "                           for info_statusX in [False, True]\n",
    "                           for info_statusY in [False, True]\n",
    "                           for info_statusZ in [False, True]]\n",
    "    def get_statespace(self): return self.statespace\n",
    "    def get_actionspace(self): return self.actionspace\n",
    "    def get_statespace_len(self): return len(self.statespace)\n",
    "    def get_actionspace_len(self): return len(self.actionspace)\n",
    "    def get_actiondict(self): return self.actionDict\n",
    "    def create_statespace_VISUALISATION(self):\n",
    "        self.statespace_vis = [((row, col)) for row in range(self.gridsize[0]) for col in range(self.gridsize[1])]\n",
    "    def get_statespace_VISUALISATION(self): return self.statespace_vis\n",
    "\n",
    "\n",
    "    ### Handles the 'ASK' action, providing the agent with environmental information\n",
    "    def ask_action(self, state):\n",
    "        self.ask_action_counter += 1\n",
    "        position, info_statusX, info_statusY, info_statusZ = state\n",
    "        verbal_inputs = []\n",
    "        if not info_statusZ:\n",
    "            # self.last_reward_state = (position, True) # Update the last_reward_state here upon successful information retrieval\n",
    "            #print(f\"last reward state {self.last_reward_state}\")\n",
    "            VERBAL_INPUT1 = \"Hey, there's a victim at the hospital.\"  ###\n",
    "            VERBAL_INPUT2 = \"Also, fire was reported at the train station.\"\n",
    "            VERBAL_INPUT3 = \"There is a fire at the bank.\"\n",
    "            VERBAL_INPUT4 = \"A safe area is the mall.\"\n",
    "            VERBAL_INPUT5 = \"Keep an eye on the access route in the school.\"\n",
    "            VERBAL_INPUT6 = \"Keep an eye on the access route in the restaurant.\"\n",
    "            VERBAL_INPUT7 = \"Keep an eye on the access route in the shop.\"\n",
    "            VERBAL_INPUT8 = \"Significant instances of heat at bakery.\"\n",
    "            VERBAL_INPUT9 = \"Police told us that no access allowed around the petrol station.\"\n",
    "            verbal_inputs.append(VERBAL_INPUT1)\n",
    "            verbal_inputs.append(VERBAL_INPUT2)\n",
    "            verbal_inputs.append(VERBAL_INPUT3)\n",
    "            verbal_inputs.append(VERBAL_INPUT4)\n",
    "            verbal_inputs.append(VERBAL_INPUT5)\n",
    "            verbal_inputs.append(VERBAL_INPUT6)\n",
    "            verbal_inputs.append(VERBAL_INPUT7)\n",
    "            verbal_inputs.append(VERBAL_INPUT8)\n",
    "            verbal_inputs.append(VERBAL_INPUT9)\n",
    "            for input_text in verbal_inputs:\n",
    "                    self.simulate_LLM_process_alternative(input_text)\n",
    "            \n",
    "            # if self.ask_action_counter < 1:\n",
    "            #     print(f\"real LLM is about to start handling the input {VERBAL_INPUT1}\")\n",
    "            #     for input_text in verbal_inputs:\n",
    "            #         response = self.assistant.generate_response(input_text)\n",
    "            #         if response:\n",
    "            #             self.visited_information_state = True\n",
    "            #         self.hazards, self.pois = self.assistant.refine_response(response)\n",
    "            #         print(f\"real LLM is about to end handling the input {VERBAL_INPUT1}\")\n",
    "            #         self.update_environment_REAL(self.hazards, self.pois)\n",
    "            # else:\n",
    "            #     # #print(f\"input will be handled hereby by pseudoLLM\")\n",
    "            #     # print(self.hazards, self.pois)\n",
    "            #     # self.visited_information_state = True\n",
    "            #     # self.update_environment_REAL(self.hazards, self.pois)\n",
    "            #     for input_text in verbal_inputs:\n",
    "            #         self.simulate_LLM_process_alternative(input_text)\n",
    "                \n",
    "    \n",
    "    def update_environment_REAL(self, haz, poi):\n",
    "        for hazardous_location in haz:\n",
    "            self.sensor_readings[(hazardous_location, True, True, True)] = -10.0\n",
    "            self.fires.append(hazardous_location)\n",
    "        for safe_location in poi:\n",
    "            self.sensor_readings[(safe_location, True, True, True)] = 10.0\n",
    "            self.POIs.append(safe_location)\n",
    "            \n",
    "            \n",
    "       \n",
    "    \n",
    "    ### Simulates the process of obtaining information from a language model (multiple locations)\n",
    "    ### problem when two locations associated to different class are present in the smae sentence \n",
    "    def simulate_LLM_process_alternative(self, input):\n",
    "        sum_embedding = torch.tensor([0, 0], dtype=torch.float32)\n",
    "        locations_in_input = []\n",
    "        for location in self.locationsDict:\n",
    "            # Check if the location keyword is in the information string\n",
    "            if location in input:\n",
    "                location_embedding = torch.tensor(self.locationsDict[location], dtype=torch.float32)\n",
    "                sum_embedding += location_embedding\n",
    "                locations_in_input.append(tuple(int(x) for x in location_embedding.tolist()))\n",
    "        #return locations_in_input\n",
    "        if locations_in_input:\n",
    "            #print(f\"response is {locations_in_input} -- when input is: {input}\")\n",
    "            self.visited_information_state = True\n",
    "            #print(f\"response is {response}\")\n",
    "            sentences = input.split(\". \") if \". \" in input else [input]\n",
    "            #print(f\"the sentences are: {sentences}\")\n",
    "            for sentence in sentences:\n",
    "                is_poi = any(keyword in sentence for keyword in self.keywords_for_POIs)\n",
    "                is_fire = any(keyword in sentence for keyword in self.keywords_for_danger)\n",
    "                #print(f\"In sentence '{sentence}' we have POI: {is_poi} and fire: {is_fire}\")\n",
    "                for location, location_coords in self.locationsDict.items():\n",
    "                    if location in sentence:\n",
    "                        info = tuple(int(x) for x in torch.tensor(location_coords, dtype=torch.float32).tolist())\n",
    "                        #print(f\"info now is {info} and poi {is_poi} and fire {is_fire}\")\n",
    "                        # If the location is already categorized, skip it\n",
    "                        if info in self.POIs or info in self.fires:\n",
    "                            continue\n",
    "                        # Add location to POIs or fires based on the context of the sentence\n",
    "                        self.update_environment(info, is_poi, is_fire)\n",
    "                        \n",
    "    \n",
    "    def update_environment(self, info, is_poi, is_fire):\n",
    "        if is_poi and not is_fire:\n",
    "            self.sensor_readings[(info, True, True, True)] = 10.0\n",
    "            self.POIs.append(info)\n",
    "        elif is_fire and not is_poi:\n",
    "            self.sensor_readings[(info, True, True, True)] = -10.0\n",
    "            self.fires.append(info)\n",
    "    \n",
    "\n",
    "    ### Determines the next state of the agent based on the current state and action. It handles different actions\n",
    "    ### like movement, asking for information, and saving victims\n",
    "    def next_state_vision(self, current_state, action):\n",
    "        position, info_statusX, info_statusY, info_statusZ = current_state\n",
    "        s_row, s_col = position\n",
    "        # Handle movement actions\n",
    "        if action == 0:  # Move Up\n",
    "            next_row = max(0, s_row - 1)\n",
    "        elif action == 1:  # Move Down\n",
    "            next_row = min(self.gridsize[0] - 1, s_row + 1)\n",
    "        else:\n",
    "            next_row = s_row\n",
    "        if action == 2:  # Move Left\n",
    "            next_col = max(0, s_col - 1)\n",
    "        elif action == 3:  # Move Right\n",
    "            next_col = min(self.gridsize[1] - 1, s_col + 1)\n",
    "        else:\n",
    "            next_col = s_col\n",
    "        # Stay in the cell for ASK and SAVE actions\n",
    "        if action in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "            next_row, next_col = position\n",
    "        next_position = (next_row, next_col)\n",
    "        next_info_statusX = info_statusX\n",
    "        next_info_statusY = info_statusY\n",
    "        next_info_statusZ = info_statusZ\n",
    "        # # Update info_status on ASK action\n",
    "        # if action == 4 and position == self.infoLocation[0]:\n",
    "        #     self.ask_action(current_state)\n",
    "        #     info_status = True\n",
    "        return (next_position, next_info_statusX, next_info_statusY, next_info_statusZ)\n",
    "\n",
    "\n",
    "    ### Determines the next state of the agent based on the current state and action. It handles different actions\n",
    "    ### like movement, asking for information, and saving victims\n",
    "    def next_state(self, current_state, action):\n",
    "        position, info_statusX, info_statusY, info_statusZ = current_state\n",
    "        s_row, s_col = position\n",
    "        next_info_statusX = info_statusX\n",
    "        next_info_statusY = info_statusY\n",
    "        next_info_statusZ = info_statusZ\n",
    "        next_row, next_col = s_row, s_col\n",
    "        # Handle movement actions\n",
    "        if action == 0:  # Move Up\n",
    "            next_row = max(0, s_row - 1)\n",
    "        elif action == 1:  # Move Down\n",
    "            next_row = min(self.gridsize[0] - 1, s_row + 1)\n",
    "        elif action == 2:  # Move Left\n",
    "            next_col = max(0, s_col - 1)\n",
    "        elif action == 3:  # Move Right\n",
    "            next_col = min(self.gridsize[1] - 1, s_col + 1)\n",
    "        \n",
    "        next_position = (next_row, next_col)\n",
    "        # Update info_status on ASK action  \n",
    "        if action == 4 and position == self.infoLocation[0] and not info_statusX:\n",
    "            # self.ask_action(current_state)\n",
    "            next_info_statusX = True\n",
    "        \n",
    "        # Update info_status on ASK action  \n",
    "        elif action == 6 and position == self.infoLocation[1] and info_statusX:\n",
    "            # self.ask_action(current_state)\n",
    "            next_info_statusY = True\n",
    "        \n",
    "        elif action == 7 and position == self.infoLocation[2] and info_statusX and info_statusY:\n",
    "            self.ask_action(current_state)\n",
    "            next_info_statusZ = True\n",
    "            \n",
    "        \n",
    "        if action == 5 and position == self.victimStates[0] and next_position == self.victimStates[0] and next_info_statusX and next_info_statusY and info_statusZ:\n",
    "            self.victim_saved = True\n",
    "            self.isGameEnd = True\n",
    "        if next_position in self.ditches:\n",
    "            self.isGameEnd = True\n",
    "        if self.totalTurns >= self.maxSteps:\n",
    "            self.isGameEnd = True\n",
    "        return (next_position, next_info_statusX, next_info_statusY, next_info_statusZ)\n",
    "\n",
    "\n",
    "    ### Calculates the reward based on the agent's actions and the current state.\n",
    "    def compute_reward(self, state, next_state, action):\n",
    "        _, info_statusX, info_statusY, info_statusZ = state\n",
    "        pos_next, next_info_statusX, next_info_statusY, next_info_statusZ = next_state\n",
    "        reward = self.turnPenalty\n",
    "        if action == 5 and pos_next == self.victimStates[0]:\n",
    "            if next_info_statusX and next_info_statusY and next_info_statusZ:\n",
    "                reward += self.winReward  # winReward (+100)\n",
    "            else:\n",
    "                reward += self.savePenalty  # savePenalty (-10)\n",
    "        elif action == 4:\n",
    "            if pos_next == self.infoLocation[0] and not info_statusX:\n",
    "                reward += self.askingReward \n",
    "            else:\n",
    "                reward += self.wrongAskPenalty  # askingReward (+5) or wrongAskPenalty (-1)\n",
    "        elif action == 6:\n",
    "            if pos_next == self.infoLocation[1] and not info_statusY and info_statusX:\n",
    "                reward += self.askingReward\n",
    "            else:\n",
    "                reward += self.wrongAskPenalty\n",
    "        elif action == 7:\n",
    "            if pos_next == self.infoLocation[2] and not info_statusZ and info_statusX and info_statusY:\n",
    "                reward += self.askingReward\n",
    "            else:\n",
    "                reward += self.wrongAskPenalty\n",
    "        elif pos_next in self.ditches:\n",
    "            reward += self.ditchPenalty  # ditchPenalty (-20)\n",
    "        elif self.totalTurns > self.maxSteps:\n",
    "            reward += self.exceedStepPenalty\n",
    "        else:\n",
    "            reward\n",
    "        return reward\n",
    "\n",
    "\n",
    "    ### Advances the environment by one time step based on the agent's action\n",
    "    def step(self, action):\n",
    "        if self.isGameEnd:\n",
    "            #print('game over')\n",
    "            raise Exception('Game is Over')\n",
    "        if action not in self.actionspace:\n",
    "            raise ValueError('Invalid action taken')\n",
    "        next_state = self.next_state(self.currentState, action)\n",
    "        reward = self.compute_reward(self.currentState, next_state, action) #self.currentState\n",
    "        self.currentState = next_state\n",
    "        done = self.isGameEnd\n",
    "        self.totalTurns += 1\n",
    "        return self.currentState, reward, done, self.totalTurns\n",
    "\n",
    "\n",
    "    ### Resets the environment to its initial state.\n",
    "    def reset(self, start_state=None):\n",
    "        self.isGameEnd = False\n",
    "        self.visited_information_state = False\n",
    "        self.victim_saved = False\n",
    "        self.totalTurns = 0\n",
    "        self.infoCollected = []\n",
    "        self.currentState = start_state if start_state else (self.startState[0], False, False, False)\n",
    "        self.POIs = []\n",
    "        self.fires = []\n",
    "        return self.currentState\n",
    "\n",
    "    def reset_for_animation(self, start):\n",
    "        self.isGameEnd = False\n",
    "        self.visited_information_state = False\n",
    "        self.totalTurns = 0\n",
    "        self.currentState = (start, False)\n",
    "        return self.currentState\n",
    "\n",
    "\n",
    "    ### Methods for annotating and generating images of the environment grid\n",
    "    def annotate_cell(self, ax, row, col, text, color='white'):\n",
    "        \"\"\" Annotates a specific cell in the grid with given text and color. \"\"\"\n",
    "        ax.text(col * self.desired_cell_size + self.desired_cell_size/2,\n",
    "                row * self.desired_cell_size + self.desired_cell_size/2,\n",
    "                text,\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                color=color,\n",
    "                fontsize=15,\n",
    "                weight='bold')\n",
    "\n",
    "\n",
    "    def generate_annotated_image(self, image_path):\n",
    "        # Load and process the image\n",
    "        img = Image.open(image_path)\n",
    "        # Resize and crop image to fit the grid\n",
    "        aspect_ratio = img.width / img.height\n",
    "        if aspect_ratio > 1:\n",
    "            new_size = (self.gridsize[0] * img.height // self.gridsize[1], img.height)\n",
    "        else:\n",
    "            new_size = (img.width, self.gridsize[1] * img.width // self.gridsize[0])\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        crop_x = (img.width - self.gridsize[0] * (img.height // self.gridsize[1])) // 2\n",
    "        crop_y = (img.height - self.gridsize[1] * (img.width // self.gridsize[0])) // 2\n",
    "        img = img.crop((crop_x, crop_y, img.width - crop_x, img.height - crop_y))\n",
    "        # Resize image to have equal cells\n",
    "        new_image_width = self.desired_cell_size * self.gridsize[0]\n",
    "        new_image_height = self.desired_cell_size * self.gridsize[1]\n",
    "        img = img.resize((new_image_width, new_image_height))\n",
    "        # Create figure with annotations\n",
    "        if not self.gridsize == [17, 17]:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(15, 15))\n",
    "        ax.imshow(img)\n",
    "        # Draw grid lines in white\n",
    "        ax.set_xticks([i * self.desired_cell_size for i in range(self.gridsize[0] + 1)], minor=False)\n",
    "        ax.set_yticks([i * self.desired_cell_size for i in range(self.gridsize[1] + 1)], minor=False)\n",
    "        ax.grid(which=\"both\", color=\"white\", linestyle='-', linewidth=2)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Add annotations for each cell\n",
    "        for row in range(self.gridsize[1]):\n",
    "            for col in range(self.gridsize[0]):\n",
    "                cell_coord = (row, col)\n",
    "                # Check for special states and annotate accordingly\n",
    "                if cell_coord in self.startState:\n",
    "                    self.annotate_cell(ax, row, col, 'START', color='coral')  # Start\n",
    "                elif cell_coord in self.victimStates:\n",
    "                    self.annotate_cell(ax, row, col, 'VIC', color='cyan')  # Terminal\n",
    "                elif cell_coord in self.ditches:\n",
    "                    self.annotate_cell(ax, row, col, 'D', color='red')  # Ditch\n",
    "                elif cell_coord in self.fires:\n",
    "                    self.annotate_cell(ax, row, col, 'F', color='orange')  # Fire\n",
    "                elif cell_coord in self.POIs:\n",
    "                    self.annotate_cell(ax, row, col, 'P', color='purple')  # POI\n",
    "                elif cell_coord in self.infoLocation:\n",
    "                    self.annotate_cell(ax, row, col, 'INFO', color='yellow')  # Info\n",
    "                else:\n",
    "                    self.annotate_cell(ax, row, col, str(cell_coord))  # Regular cell coordinate\n",
    "        # Save figure to buffer and load as PIL image\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        self.annotated_image = Image.open(buf)\n",
    "\n",
    "\n",
    "    ### Visualizes the current state of the environment.\n",
    "    ### Visualizes the current state of the environment.\n",
    "    def render(self):\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))#plt.subplots()\n",
    "        ax.set_xlim(0, self.gridsize[0])\n",
    "        ax.set_ylim(0, self.gridsize[1])\n",
    "        ax.set_xticks(range(self.gridsize[0]))\n",
    "        ax.set_yticks(range(self.gridsize[1]))\n",
    "        ax.grid(which='both')\n",
    "        # Plotting the agent\n",
    "        agent_pos, _, _, _ = self.currentState\n",
    "        ax.plot(agent_pos[1] + 0.5, agent_pos[0] + 0.5, 'o', color='blue', markersize=10)  # Agent as a blue dot\n",
    "        # Plotting the ditches\n",
    "        for ditch in self.ditches:\n",
    "            ax.plot(ditch[1] + 0.5, ditch[0] + 0.5, 'x', color='red', markersize=10)\n",
    "        # Plotting the terminal state (victim's location)\n",
    "        for terminal in self.victimStates:\n",
    "            ax.plot(terminal[1] + 0.5, terminal[0] + 0.5, 'P', color='green', markersize=10)\n",
    "        for poi in self.POIs:\n",
    "            ax.plot(poi[1] + 0.5, poi[0] + 0.5, 'P', color='pink', markersize=10)\n",
    "        for hazard in self.fires:\n",
    "            ax.plot(hazard[1] + 0.5, hazard[0] + 0.5, 'x', color='orange', markersize=10)\n",
    "        # Plotting the info location\n",
    "        for info in self.infoLocation:\n",
    "            ax.plot(info[1] + 0.5, info[0] + 0.5, '*', color='yellow', markersize=10)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "# Example initialization and training\n",
    "gridsize = [7, 7]\n",
    "startState = [(4, 1)]\n",
    "victimStates = [(0, 3)]\n",
    "ditches = [(1, 6), (2, 2), (2, 4), (3, 2), (3, 3), (3, 4), (4, 5), \\\n",
    "    (5, 0), (5, 1), (5, 2), (6, 0), (0, 2), (0, 4)]\n",
    "fires = []\n",
    "POIs = []  # Victim locations\n",
    "infoLocation = [(4, 4), (6, 2), (5, 5)]  # Location to ask for information was (6, 1)\n",
    "#image_path = \"/home/research100/Desktop/disaster_area.jpg\"\n",
    "#image_path = \"/home/dimiubuntu/Documents/enhanced_RL/disaster_area.jpg\"\n",
    "image_path = \"/home/research100/Documents/sample/enhanced_RL/enhanced_RL/enhanced_RL/images/disaster_area.jpg\"\n",
    "#image_path = \"/content/disaster_area.jpg\"\n",
    "\n",
    "env = SARenv(gridsize, startState, victimStates, ditches, fires, POIs, infoLocation, image_path, mode='debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "  def __init__(self, ALPHA, GAMMA, EPSILON_MAX, EPSILON_MIN, DECAY_RATE, EPISODES):\n",
    "    self.ALPHA = ALPHA\n",
    "    self.GAMMA = GAMMA\n",
    "    self.EPSILON_MAX = EPSILON_MAX\n",
    "    self.EPSILON_MIN = EPSILON_MIN\n",
    "    self.EPISODES = EPISODES\n",
    "    self.DECAY_RATE = DECAY_RATE\n",
    "    self.explored_count = 0\n",
    "    self.exploited_count = 0\n",
    "    self.exploration_counts = []\n",
    "    self.exploitation_counts = []\n",
    "    self.Q = {(s, a): 0.0 for s in env.get_statespace() for a in env.get_actionspace()}\n",
    "    self.previous_Q = {(s, a): 0.0 for s in env.get_statespace() for a in env.get_actionspace()}\n",
    "    self.EPSILON = EPSILON_MAX\n",
    "    self.real_interaction = False\n",
    "\n",
    "  def max_Action(self, state):\n",
    "    #values = np.array([self.Q[state, a] for a in env.get_actionspace()])\n",
    "    values = np.array([self.Q[(state, a)] for a in env.get_actionspace()])\n",
    "    greedy_action = np.argmax(values)\n",
    "    return greedy_action\n",
    "        \n",
    "\n",
    "  def get_action(self, state):\n",
    "    rand = np.random.random()\n",
    "    if rand < self.EPSILON:\n",
    "      self.explored_count += 1  # Increment exploration count\n",
    "      a = np.random.choice(env.get_actionspace())\n",
    "    else:\n",
    "      self.exploited_count += 1  # Increment exploitation count\n",
    "      a = self.max_Action(state)\n",
    "    return a\n",
    "\n",
    "  def decay_epsilon(self):\n",
    "    if self.EPSILON > 0.1:\n",
    "      self.EPSILON -= self.DECAY_RATE/self.EPISODES\n",
    "    else:\n",
    "      self.EPSILON = self.EPSILON_MIN\n",
    "    return self.EPSILON\n",
    "\n",
    "  def update(self, state, action, next_state, next_action, reward):\n",
    "    target = reward + self.GAMMA*self.Q[next_state, next_action]\n",
    "    td_error = self.ALPHA*(target - self.Q[state, action])\n",
    "    self.Q[state, action] = self.Q[state, action] + td_error\n",
    "    # self.Q[state, action] = self.Q[state, action] + self.ALPHA*(reward + self.GAMMA*self.Q[next_state, next_action] - self.Q[state, action])\n",
    "    return self.Q\n",
    "\n",
    "  def train_agent(self):\n",
    "    total_rewards = np.zeros(self.EPISODES)\n",
    "    Rewards = 0\n",
    "    visit_counts = {}\n",
    "    steps_per_episode = []\n",
    "    Q_history = []\n",
    "    epsilon_values = []\n",
    "\n",
    "    for ep in tqdm(range(self.EPISODES)):\n",
    "      if ep % 250 == 0:\n",
    "        print(f\"episode: {ep} | reward: {Rewards} | : {self.EPSILON}\")\n",
    "\n",
    "      s = env.reset()\n",
    "      done = False\n",
    "      Rewards = 0\n",
    "      steps = 0\n",
    "      while not done:\n",
    "                \n",
    "        a = self.get_action(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        #r = self.compute_reward(s, s_, a)\n",
    "        a_ = self.max_Action(s_)\n",
    "        # env.render()\n",
    "        # time.sleep(0.05)\n",
    "        Rewards += r\n",
    "        updated_Q = self.update(s, a, s_, a_, r)\n",
    "        # print(f\"State: {s} | AV_ACT: {env.get_actionspace()} | Action: {a} | next_state: {s_} | NEXT_AV_ACT: {env.get_actionspace()} | Next_Action: {a_} | Reward: {r}\")\n",
    "        s = s_\n",
    "        steps += 1\n",
    "\n",
    "\n",
    "        if s[0] in visit_counts:\n",
    "            visit_counts[s[0]] += 1\n",
    "        else:\n",
    "            visit_counts[s[0]] = 1\n",
    "\n",
    "      self.EPSILON = self.decay_epsilon()\n",
    "      # if ep % 2000 == 0:\n",
    "      #   Q_history.append(updated_Q.copy())\n",
    "      total_rewards[ep] = Rewards\n",
    "      steps_per_episode.append(steps)\n",
    "      self.exploration_counts.append(self.explored_count)\n",
    "      self.exploitation_counts.append(self.exploited_count)\n",
    "      epsilon_values.append(self.EPSILON)\n",
    "    print(f\"Exploration count: {self.explored_count}\")\n",
    "    print(f\"Exploitation count: {self.exploited_count}\")\n",
    "\n",
    "\n",
    "    return updated_Q, total_rewards, visit_counts, self.exploration_counts, self.exploitation_counts, steps_per_episode, epsilon_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AGENT RECEIVES INFORMATION -- UPDATES THE ENVIRONMENT -- ACTS BASED ON THAT\n",
    "class QLearningAgentAttention:\n",
    "  def __init__(self, ALPHA, GAMMA, EPSILON_MAX, EPSILON_MIN, DECAY_RATE, EPISODES):\n",
    "    self.ALPHA = ALPHA\n",
    "    self.GAMMA = GAMMA\n",
    "    self.EPSILON_MAX = EPSILON_MAX\n",
    "    self.EPSILON_MIN = EPSILON_MIN\n",
    "    self.EPISODES = EPISODES\n",
    "    self.DECAY_RATE = DECAY_RATE\n",
    "    self.explored_count = 0\n",
    "    self.exploited_count = 0\n",
    "    self.exploration_counts = []\n",
    "    self.exploitation_counts = []\n",
    "    self.exploit_start_episode = None\n",
    "    self.exploit_end_episode = None\n",
    "    self.exploit_episodes = 1000  # 30% of the episodes\n",
    "    self.exploit_initiated = False\n",
    "    self.exploit_mode = False\n",
    "    self.should_exploit = False\n",
    "    self.received_input = False  # tracks if verbal input has been received\n",
    "    self.similarity_threshold = 0.9\n",
    "    self.Q = {(s, a): 0.0 for s in env.get_statespace() for a in env.get_actionspace()}\n",
    "    self.attention_space = {(s, a): 0.0 for s in env.get_statespace() for a in env.get_actionspace()}\n",
    "    self.previous_Q = {(s, a): 0.0 for s in env.get_statespace() for a in env.get_actionspace()}\n",
    "    self.EPSILON = EPSILON_MAX\n",
    "    self.POIs_identified = []\n",
    "    self.fires_identified = []\n",
    "\n",
    "  ### Identifies states that have changed based on sensor readings.\n",
    "  def identify_changed_states(self, readings):\n",
    "    changed_states = [i for i, value in readings.items() if value != 1]\n",
    "    return changed_states\n",
    "\n",
    "  ### Finds states connected to a given target state, considering possible actions and their inverse\n",
    "  def get_connected_states(self, target_state):\n",
    "    # Inverse action mapping for movement actions\n",
    "    inverse_actions = {0: 1, 1: 0, 2: 3, 3: 2}\n",
    "    # Generate state-action pairs\n",
    "    connected_states_pairs = []\n",
    "    for action in env.actionspace:\n",
    "        # Skip 'ASK' and 'SAVE' actions\n",
    "        if action in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "            continue\n",
    "        # Apply the inverse action to the target state\n",
    "        inverse_action = inverse_actions[action]\n",
    "        possible_prev_state = env.next_state_vision(target_state, inverse_action)\n",
    "        # Check if the resulting state is valid and leads to the target state\n",
    "        if possible_prev_state != target_state and possible_prev_state in env.statespace and possible_prev_state[0] not in env.ditches:\n",
    "            connected_states_pairs.append((possible_prev_state, action))\n",
    "    return connected_states_pairs\n",
    "\n",
    "\n",
    "  ### Updates the attention space, which influences the agent's decision-making based on environmental changes.\n",
    "  def update_attention_space(self, connection, sensor_readings):\n",
    "    connected_states = self.get_connected_states(connection)\n",
    "    # Determine the value to add based on sensor reading\n",
    "    value_to_add = 2.0 if sensor_readings[connection] > 0 else -100.0\n",
    "    for connected_state, action in connected_states:\n",
    "      # Update the attention space value for the state-action pair\n",
    "        self.attention_space[(connected_state, action)] = value_to_add\n",
    "    # # Check if the new information refers to a victim state and update exclusively for SAVE action\n",
    "    if connection[0] in env.victimStates:\n",
    "        victim_state_action_pair = ((connection[0], True, True, True), 5) # Action 5 corresponds to 'SAVE'\n",
    "        self.attention_space[victim_state_action_pair] = 100\n",
    "    return self.attention_space\n",
    "\n",
    "  def max_Action(self, state):\n",
    "    #values = np.array([self.Q[state, a] for a in env.get_actionspace()])\n",
    "    values = np.array([self.Q[(state, a)] for a in env.get_actionspace()])\n",
    "    greedy_action = np.argmax(values)\n",
    "    return greedy_action\n",
    "\n",
    "  # ### Chooses an action based on the current policy (exploitation vs. exploration)\n",
    "  # def get_action(self, state, sensor_readings): # this action is the same to the next state\n",
    "  #   if self.exploit_mode:\n",
    "  #     sensor_influenced_states = [state for state, reading in sensor_readings.items() if reading != 1.0]\n",
    "  #     if sensor_influenced_states:\n",
    "  #         self.exploited_count += 1\n",
    "  #         a = self.max_Action(state)\n",
    "  #     else:\n",
    "  #         self.explored_count += 1\n",
    "  #         a = np.random.choice(env.get_actionspace())\n",
    "  #   else:\n",
    "  #     rand = np.random.random()\n",
    "  #     if rand < self.EPSILON:\n",
    "  #       self.explored_count += 1  # Increment exploration count\n",
    "  #       a = np.random.choice(env.get_actionspace())\n",
    "  #     else:\n",
    "  #       self.exploited_count += 1  # Increment exploitation count\n",
    "  #       a = self.max_Action(state)\n",
    "  #   return a\n",
    "\n",
    "  \n",
    "  def get_action(self, state, sensor_readings):\n",
    "    if not env.visited_information_state:\n",
    "      rand = np.random.random()\n",
    "      if rand < self.EPSILON:\n",
    "        self.explored_count += 1  # Increment exploration count\n",
    "        a = np.random.choice(env.get_actionspace())\n",
    "      else:\n",
    "        self.exploited_count += 1  # Increment exploitation count\n",
    "        a = self.max_Action(state)\n",
    "    else:\n",
    "      # rand = np.random.random()\n",
    "      # if rand < 0.001:\n",
    "      #   self.explored_count += 1  # Increment exploration count\n",
    "      #   a = np.random.choice(env.get_actionspace())\n",
    "      # else:\n",
    "        self.exploited_count += 1  # Increment exploitation count\n",
    "        a = self.max_Action(state)\n",
    "    return a\n",
    "\n",
    "  ### Methods for decaying the exploration rate (epsilon) over time\n",
    "  def decay_epsilon_exploit(self):\n",
    "    if self.EPSILON > 0.1:\n",
    "      self.EPSILON -= (8*self.DECAY_RATE)/self.EPISODES\n",
    "    else:\n",
    "      self.EPSILON = self.EPSILON_MIN\n",
    "    return self.EPSILON\n",
    "\n",
    "  def decay_epsilon(self):\n",
    "    if self.EPSILON > 0.1:\n",
    "      self.EPSILON -= self.DECAY_RATE/self.EPISODES\n",
    "    else:\n",
    "      self.EPSILON = self.EPSILON_MIN\n",
    "    return self.EPSILON\n",
    "\n",
    "  ### Manages the transition between exploration and exploitation modes based on information received\n",
    "  def exploitation_strategy(self, received, current_episode):\n",
    "    if received and not self.exploit_mode and not self.exploit_initiated:\n",
    "        self.exploit_start_episode = current_episode\n",
    "        self.exploit_end_episode = self.exploit_start_episode + self.exploit_episodes\n",
    "        self.exploit_mode = True\n",
    "        self.exploit_initiated = True\n",
    "        print(f\"Exploitation mode started at episode {self.exploit_start_episode}\")\n",
    "\n",
    "  ### Updates the Q-values based on the temporal difference learning formula\n",
    "  def update(self, state, action, next_state, next_action, reward):\n",
    "    target = reward + self.GAMMA*self.Q[next_state, next_action]\n",
    "    td_error = self.ALPHA*(target - self.Q[state, action])\n",
    "    self.Q[state, action] = self.Q[state, action] + td_error\n",
    "    # self.Q[state, action] = self.Q[state, action] + self.ALPHA*(reward + self.GAMMA*self.Q[next_state, next_action] - self.Q[state, action])\n",
    "    return self.Q\n",
    "\n",
    "  ### the training of the agent over a specified number of episodes\n",
    "  def train_agent(self):\n",
    "    total_rewards = np.zeros(self.EPISODES)\n",
    "    Rewards = 0\n",
    "    visit_counts = {}\n",
    "    Q_history = []\n",
    "    steps_per_episode = []\n",
    "    identified_states = []\n",
    "    epsilon_values = []\n",
    "    sensor_readings = env.sensor_readings\n",
    "    attention_space = {}\n",
    "    for ep in tqdm(range(self.EPISODES)):\n",
    "      if ep % 250 == 0:\n",
    "        print(f\"episode: {ep} | reward: {Rewards} | : {self.EPSILON}\")\n",
    "\n",
    "      s = env.reset()\n",
    "      done = False\n",
    "      Rewards = 0\n",
    "      steps = 0\n",
    "      while not done:\n",
    "\n",
    "        if env.visited_information_state and not self.received_input:\n",
    "          identified_states = self.identify_changed_states(sensor_readings)\n",
    "          self.received_input = True\n",
    "          env.generate_annotated_image(image_path)\n",
    "          print(f\"Got the info needed at ep {ep} and location {s[0]}\")\n",
    "          if identified_states:\n",
    "            for next_state in identified_states:\n",
    "              attention_space = self.update_attention_space(next_state, sensor_readings)\n",
    "              ## normally, if we'd like to directly import this update to the Q table, we have to set a metric of\n",
    "              ## confidence .. that this information is concrete\n",
    "          for key in attention_space.keys():\n",
    "              self.Q[key] = self.Q[key] + attention_space[key]\n",
    "\n",
    "\n",
    "        a = self.get_action(s, sensor_readings)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        a_ = self.max_Action(s_)\n",
    "        Rewards += r\n",
    "        updated_Q = self.update(s, a, s_, a_, r)\n",
    "        # print(f\"State: {s} | AV_ACT: {env.get_actionspace()} | Action: {a} | next_state: {s_} | NEXT_AV_ACT: {env.get_actionspace()} | Next_Action: {a_} | Reward: {r}\")\n",
    "        s = s_\n",
    "        steps += 1\n",
    "\n",
    "\n",
    "        if s[0] in visit_counts:\n",
    "            visit_counts[s[0]] += 1\n",
    "        else:\n",
    "            visit_counts[s[0]] = 1\n",
    "\n",
    "      # ### for now verbal input is considered truthful --- see (5b) for explanations\n",
    "      # self.exploitation_strategy(self.received_input, ep)\n",
    "      # if self.exploit_mode and self.exploit_start_episode <= ep < self.exploit_end_episode:\n",
    "      #   self.EPSILON = self.decay_epsilon_exploit()\n",
    "      #   if ep == self.exploit_end_episode - 1:\n",
    "      #       self.exploit_mode = False\n",
    "      #       print(f\"Exploitation mode ends at episode {ep}\")\n",
    "      # else:\n",
    "      #   self.EPSILON = self.decay_epsilon()\n",
    "\n",
    "      if not env.visited_information_state:\n",
    "        self.EPSILON = self.decay_epsilon()\n",
    "      else:\n",
    "        self.EPSILON = self.decay_epsilon_exploit()\n",
    "        \n",
    "      \n",
    "      self.previous_Q = updated_Q.copy()\n",
    "      total_rewards[ep] = Rewards\n",
    "      steps_per_episode.append(steps)\n",
    "      # save the Q table every 500 episodes\n",
    "      # if ep % 250 == 0:\n",
    "      #   Q_history.append(updated_Q.copy())\n",
    "      self.exploration_counts.append(self.explored_count)\n",
    "      self.exploitation_counts.append(self.exploited_count)\n",
    "      epsilon_values.append(self.EPSILON)\n",
    "      self.POIs_identified.append(env.POIs)\n",
    "      self.fires_identified.append(env.fires)\n",
    "    print(f\"Exploration count: {self.explored_count}\")\n",
    "    print(f\"Exploitation count: {self.exploited_count}\")\n",
    "    print(f\"POIs identified during training {env.POIs}\")\n",
    "    print(f\"fires identified during training {env.fires}\")\n",
    "\n",
    "    return updated_Q, total_rewards, visit_counts, self.exploration_counts, self.exploitation_counts, steps_per_episode, epsilon_values, attention_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_total_rewards_AGENT_flat = []  # List to store total rewards from each run\n",
    "for _ in range(20):\n",
    "    EPISODES = 1500\n",
    "    ALPHA = 0.1\n",
    "    GAMMA = 0.998\n",
    "    EPSILON_MAX = 1.0\n",
    "    EPSILON_MIN = 0.01\n",
    "    DECAY_RATE = 2\n",
    "    agent_rescuer0 = QLearningAgent(ALPHA, GAMMA, EPSILON_MAX, EPSILON_MIN, DECAY_RATE, EPISODES)\n",
    "    Q_table0, total_returns0, state_visitations_simple0, exploration_simple0, exploitation_simple0, total_steps0, eps_history0 = agent_rescuer0.train_agent()\n",
    "\n",
    "    all_total_rewards_AGENT_flat.append(total_returns0)\n",
    "    \n",
    "avg_total_rewards_AGENT_flat = np.mean(all_total_rewards_AGENT_flat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1500\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.998\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "DECAY_RATE = 2\n",
    "agent_rescuer0 = QLearningAgent(ALPHA, GAMMA, EPSILON_MAX, EPSILON_MIN, DECAY_RATE, EPISODES)\n",
    "Q_table0, total_returns0, state_visitations_simple0, exploration_simple0, exploitation_simple0, total_steps0, eps_history0 = agent_rescuer0.train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1500\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.998\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "DECAY_RATE = 2\n",
    "agent_rescuer = QLearningAgentAttention(ALPHA, GAMMA, EPSILON_MAX, EPSILON_MIN, DECAY_RATE, EPISODES)\n",
    "Q_table, total_returns, state_visitations_simple, exploration_simple, exploitation_simple, total_steps, eps_history, attention = agent_rescuer.train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_total_rewards_AGENT_flat_att = []  # List to store total rewards from each run\n",
    "for _ in range(1):\n",
    "    EPISODES = 1500\n",
    "    ALPHA = 0.1\n",
    "    GAMMA = 0.998\n",
    "    EPSILON_MAX = 1.0\n",
    "    EPSILON_MIN = 0.01\n",
    "    DECAY_RATE = 2\n",
    "    agent_rescuer = QLearningAgentAttention(ALPHA, GAMMA, EPSILON_MAX, EPSILON_MIN, DECAY_RATE, EPISODES)\n",
    "    Q_table, total_returns, state_visitations_simple, exploration_simple, exploitation_simple, total_steps, eps_history, attention = agent_rescuer.train_agent()\n",
    "\n",
    "\n",
    "    all_total_rewards_AGENT_flat_att.append(total_returns)\n",
    "    \n",
    "avg_total_rewards_AGENT_flat_att = np.mean(all_total_rewards_AGENT_flat_att, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    for i, total_rewards in enumerate(total_rewards_list):\n",
    "        mean_rewards_1, mean_rewards_50 = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        for t in range(EPISODES):\n",
    "            mean_rewards_1[t] = np.mean(total_rewards[max(0, t-5):(t+1)])\n",
    "            mean_rewards_50[t] = np.mean(total_rewards[max(0, t-500):(t+1)])\n",
    "        ax.plot(mean_rewards_50, label=f'{labels[i]}', alpha=0.9, color=colors[i])\n",
    "        ax.fill_between(range(EPISODES), mean_rewards_1, mean_rewards_50, color=colors[i], alpha=0.15)\n",
    "        # Check for 20 consecutive iterations with reward >= optimal reward\n",
    "        for t in range(EPISODES - 10):\n",
    "            if all(total_rewards[t:t+10] >= optimal_reward):\n",
    "                # ax.axvline(x=t+10, color=colors[i], linestyle='dotted')\n",
    "                print(f\"Line appears at episode: {t+10} for agent {labels[i]}\")\n",
    "                break\n",
    "    # ax.axhline(y=optimal_reward, color='green', linestyle='--', label='Optimal Reward')\n",
    "    ax.axhline(y=optimal_reward, color='black', linestyle='--', label='Optimal Reward', linewidth=0.8)  # Reduced linewidth\n",
    "\n",
    "    ax.legend(fontsize=8, loc='lower right')\n",
    "    ax.grid(True, color=\"white\", linestyle='-', alpha=0.9)\n",
    "    # Define a fainter lavender by adding an alpha value\n",
    "    faint_lavender = mcolors.to_rgba('slategrey', alpha=0.2)  # Adjust alpha as needed\n",
    "    ax.set_facecolor(faint_lavender)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.xticks(np.arange(0, EPISODES, step=250))\n",
    "    # plt.yscale('symlog')  # 'symlog' handles negative values as well as positive values\n",
    "\n",
    "    plt.ylabel(\"Avg. Total Rewards\")\n",
    "    plt.title(\"SAR GridWorld (7x7)\")\n",
    "    # Disable the spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.show()\n",
    "labels = [\"RL\", \"RL-Attention\"]\n",
    "colors = [\"blue\", \"orange\"]\n",
    "total_rewards_list = [avg_total_rewards_AGENT_flat, avg_total_rewards_AGENT_flat_att]#, avg_total_rewards_AGENT_1_ATTENTION]\n",
    "plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    for i, total_rewards in enumerate(total_rewards_list):\n",
    "        mean_rewards_1, mean_rewards_50 = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        max_rewards, min_rewards = np.zeros(EPISODES), np.zeros(EPISODES)\n",
    "        for t in range(EPISODES):\n",
    "            mean_rewards_50[t] = np.mean(total_rewards[max(0, t-100):(t+1)])\n",
    "            mean_rewards_1[t] = np.mean(total_rewards[max(0, t-100):(t+1)])\n",
    "            max_rewards[t] = np.max(total_rewards[max(0, t-100):(t+1)])  # Adjust the range as needed\n",
    "            min_rewards[t] = np.min(total_rewards[max(0, t-100):(t+1)])  # Adjust the range as needed\n",
    "        ax.plot(mean_rewards_50, label=f'{labels[i]}', alpha=0.9, color=colors[i])\n",
    "        ax.fill_between(range(EPISODES), min_rewards, max_rewards, color=colors[i], alpha=0.15)\n",
    "\n",
    "    ax.axhline(y=optimal_reward, color='black', linestyle='--', label='Optimal Reward', linewidth=0.8)\n",
    "    ax.legend(fontsize=7, loc='lower right')\n",
    "    ax.grid(True, color=\"white\", linestyle='-', alpha=0.9)\n",
    "    faint_lavender = mcolors.to_rgba('slategrey', alpha=0.2)\n",
    "    ax.set_facecolor(faint_lavender)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.xticks(np.arange(0, EPISODES, step=250))\n",
    "    plt.ylabel(\"Avg. Total Rewards\")\n",
    "    plt.title(\"SAR GridWorld (7x7)\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "EPISODES = 1500\n",
    "labels = [\"RL\", \"RL-Attention\"]\n",
    "colors = [\"blue\", \"orange\"]\n",
    "total_rewards_list = [avg_total_rewards_AGENT_flat, avg_total_rewards_AGENT_flat_att]  # Define these\n",
    "plot_learning_curve(total_rewards_list, EPISODES, labels, colors, optimal_reward=92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(total_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the agents and their average total rewards\n",
    "agents = ['Flat', 'Flat-Attention', 'HRL', 'HRL-Attention']\n",
    "rewards = [3, 0, 3, 0]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(agents, rewards, s=[abs(r)*20 for r in rewards], alpha=0.5, color=['blue', 'chocolate', 'magenta', 'green'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Agents')\n",
    "plt.ylabel('Average Total Reward')\n",
    "plt.title('Average Total Rewards for Agents')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate each bubble with the corresponding reward\n",
    "for i, txt in enumerate(rewards):\n",
    "    plt.annotate(f'{txt}', (agents[i], rewards[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for simple Q and attention\n",
    "def policy_evaluation(num_eval_episodes, max_eval_steps_per_episode, agent):\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    cnt = 0\n",
    "    for _ in range(num_eval_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for _ in range(max_eval_steps_per_episode):\n",
    "            # env.render()\n",
    "            action = agent.max_Action(state)\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            print(f\"In {state} --> {env.get_actiondict()[action]} --> get {reward} reward | TOTAL REWARD {total_reward}\")\n",
    "            # state = state_\n",
    "            if state_[0] in env.fires:\n",
    "                cnt += 1\n",
    "            steps += 1\n",
    "            state = state_\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode finished after {steps} steps with total reward {total_reward} and {cnt} collisions\")\n",
    "                break\n",
    "\n",
    "    mean_reward = total_reward / num_eval_episodes\n",
    "    print(f\"Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "policy_evaluation(num_eval_episodes = 1, max_eval_steps_per_episode = 100, agent=agent_rescuer0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent, verbose=True):\n",
    "    state = env.reset()\n",
    "    #state = ((4, 0), False)\n",
    "    total_reward = 0\n",
    "    collisions = []\n",
    "    destroy = []\n",
    "    done = False\n",
    "    steps = 0\n",
    "    cnt = 0\n",
    "    cnt_dynamic = 0\n",
    "    while not done:\n",
    "        option = agent.select_option(state, exploration_rate=0)\n",
    "        action = agent.select_action(state, option, exploration_rate=0)\n",
    "        next_state, reward, done, _, _ = env.step(action, option)\n",
    "        total_reward += reward\n",
    "        if verbose:\n",
    "            # Use termcolor for colored text\n",
    "            option_text = colored(f\"Option: {option}\", \"green\")\n",
    "            action_text = colored(f\"Action: {action}\", \"blue\")\n",
    "            print(f\"Step {steps}: || State={state} || {option_text} || {action_text} || Reward={reward} || Next State={next_state} || Done={done}\")\n",
    "        # state = next_state\n",
    "        if next_state[0] in env.ditches:\n",
    "            cnt += 1\n",
    "            collisions.append(next_state[0])\n",
    "        \n",
    "        if next_state[0] in env.fires:\n",
    "            cnt_dynamic += 1\n",
    "            destroy.append(next_state[0])\n",
    "        steps += 1\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Total reward: {total_reward} | Steps Taken: {steps} with {cnt} collisions in {collisions} and {cnt_dynamic} drops in {destroy} | Success: {'Yes' if state[0] == env.finalState[0] and state[1] and state[4] else 'No'}\\n\")\n",
    "\n",
    "evaluate_policy(env, agent_simple, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitation_heatmap(visit_counts, GRID):\n",
    "    # Create a grid of visit counts from the dictionary\n",
    "    grid_counts = np.zeros((GRID[0], GRID[1]))  # Assuming a 7x7 grid\n",
    "    for state, count in visit_counts.items():\n",
    "        grid_counts[state] = count\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 14))\n",
    "    heatmap = ax.imshow(grid_counts, cmap='viridis', interpolation='nearest')\n",
    "    # Add visitation counts to the cells\n",
    "    for i in range(grid_counts.shape[0]):\n",
    "        for j in range(grid_counts.shape[1]):\n",
    "            count = grid_counts[i, j]\n",
    "\n",
    "            ax.text(j, i, str(int(count)), ha='center', va='center', color='w', fontsize=8)\n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(heatmap, ax=ax)\n",
    "    cbar.ax.set_ylabel('Visit Count', rotation=-90, va=\"bottom\")\n",
    "    # Set axis limits and labels\n",
    "    ax.set_xlim([-0.5, grid_counts.shape[1] - 0.5])\n",
    "    ax.set_ylim([grid_counts.shape[0] - 0.5, -0.5])\n",
    "    ax.set_title('Visit Count Heatmap')\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Row')\n",
    "    # Set axis ticks to display numbers from 0 to 14\n",
    "    ax.set_xticks(range(grid_counts.shape[1]))\n",
    "    ax.set_yticks(range(grid_counts.shape[0]))\n",
    "    # Set tick labels to 0-14\n",
    "    ax.set_xticklabels(range(grid_counts.shape[1]))\n",
    "    ax.set_yticklabels(range(grid_counts.shape[0]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitation_heatmap(state_visitations_simple0, env.gridsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitation_heatmap(state_visitations_simple, env.gridsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitation_heatmap(visits_Q, env.gridsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitation_heatmap(visits_ATT, env.gridsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
